{"version":3,"sources":["webpack://WebIDL2/webpack/universalModuleDefinition","webpack://WebIDL2/webpack/bootstrap","webpack://WebIDL2/./lib/error.js","webpack://WebIDL2/./lib/productions/base.js","webpack://WebIDL2/./lib/validators/helpers.js","webpack://WebIDL2/./lib/productions/type.js","webpack://WebIDL2/./lib/productions/default.js","webpack://WebIDL2/./lib/productions/array-base.js","webpack://WebIDL2/./lib/productions/extended-attributes.js","webpack://WebIDL2/./lib/productions/helpers.js","webpack://WebIDL2/./lib/productions/argument.js","webpack://WebIDL2/./lib/productions/token.js","webpack://WebIDL2/./lib/productions/operation.js","webpack://WebIDL2/./lib/productions/attribute.js","webpack://WebIDL2/./lib/tokeniser.js","webpack://WebIDL2/./lib/productions/enum.js","webpack://WebIDL2/./lib/productions/includes.js","webpack://WebIDL2/./lib/productions/typedef.js","webpack://WebIDL2/./lib/productions/callback.js","webpack://WebIDL2/./lib/productions/container.js","webpack://WebIDL2/./lib/productions/constant.js","webpack://WebIDL2/./lib/productions/iterable.js","webpack://WebIDL2/./lib/productions/constructor.js","webpack://WebIDL2/./lib/productions/interface.js","webpack://WebIDL2/./lib/validators/interface.js","webpack://WebIDL2/./lib/productions/mixin.js","webpack://WebIDL2/./lib/productions/field.js","webpack://WebIDL2/./lib/productions/dictionary.js","webpack://WebIDL2/./lib/productions/namespace.js","webpack://WebIDL2/./lib/productions/callback-interface.js","webpack://WebIDL2/./lib/webidl2.js","webpack://WebIDL2/./lib/writer.js","webpack://WebIDL2/./lib/validator.js","webpack://WebIDL2/./index.js"],"names":["root","factory","exports","module","define","amd","this","installedModules","__webpack_require__","moduleId","i","l","modules","call","m","c","d","name","getter","o","Object","defineProperty","enumerable","get","r","Symbol","toStringTag","value","t","mode","__esModule","ns","create","key","bind","n","object","property","prototype","hasOwnProperty","p","s","error_error","source","position","current","message","kind","level","autofix","ruleName","sliceTokens","count","slice","Math","max","tokensToText","inputs","precedes","text","map","trivia","join","nextToken","type","length","line","precedingLastLine","splitted","split","lastLine","subsequentTokens","subsequentText","sourceContext","repeat","contextType","context","partial","bareMessage","sourceName","input","tokens","syntaxError","validationError","token","options","index","Base","[object Object]","defineProperties","json","undefined","inheritance","proto","descMap","getOwnPropertyDescriptors","entries","getPrototypeOf","idlTypeIncludesDictionary","idlType","defs","useNullableInner","union","def","unique","typedefIncludesDictionary","cache","has","set","result","nullable","subtype","type_suffix","tokeniser","obj","consume","probe","error","single_type","typeName","ret","base","type_Type","open","return_type","push","type_with_extended_attributes","keyType","stringTypes","keyIdlType","separator","valueType","close","generic_type","primitive_type","generic","typ","or","union_type","super","extAttrs","Boolean","helpers_unescape","prefix","postfix","filter","typedef","target","reference","targetToken","validate","default_Default","assign","const_value","expression","const_data","negative","ArrayBase","Array","extended_attributes_ExtendedAttributeParameters","secondaryName","list","rhsType","ids","parser","token_Token","listName","identifiers","argument_list","hasRhs","extended_attributes_SimpleExtendedAttribute","params","parse","rhs","arguments","arg","extended_attributes_ExtendedAttributes","extAttr","argument_Argument","start_position","optional","variadic","argumentNameKeywords","default","unconsume","tokeniser_Tokeniser","operation_Operation","special","regular","termination","includes","argument","attribute_Attribute","noInherit","readonly","identifier","startsWith","allowDangler","first","items","item","num_type","integer_type","decimal_type","voidToken","stringifier","getLastIndentation","str","lines","match","autofixAddExposedWindow","exposed","existing","test","unshift","tokenRe","decimal","integer","string","whitespace","comment","other","nonRegexTerminals","concat","punctuations","reserved","idl","lastCharIndex","nextChar","charAt","attemptTokenMatch","noFlushTrivia","currentTrivia","pop","lastIndex","WebIDLParseError","punctuation","Error","re","exec","tokenise","candidates","enum_EnumValue","enum_Enum","values","includes_Includes","mixin","typedef_Typedef","callback_CallbackFunction","container_Container","instance","inheritable","allowedMembers","colon","members","ea","mem","args","member","constant_Constant","unescape","iterable_IterableLike","async","secondTypeRequired","secondTypeAllowed","constructor_Constructor","static_member","interface_Interface","every","constructors","constructor","autofixConstructor","opNames","Set","getOperations","op","partials","mixins","mixinMap","ext","additions","forEachExtension","addition","add","existings","checkInterfaceMemberDuplication","interfaceDef","constructorExtAttr","indentation","memberIndent","getFirstToken","data","sort","x","y","parentTrivia","indentCh","getMemberIndentation","constructorOp","existingIndex","findIndex","splice","indexOf","removed","trim","mixin_Mixin","field_Field","required","dictionary_Dictionary","namespace_Namespace","callback_interface_CallbackInterface","callback","parseByTokens","interface_","opts","definition","res","eof","concrete","definitions","noop","templates","wrap","nameless","extendedAttribute","extendedAttributeReference","write","ast","ts","raw","unescaped","wrapper","reference_token","name_token","type_body","it","firstToken","ref","extended_attributes","default_","make_ext_at","id","eats","container","inh","iterate","iterable_like","parent","table","interface","interface mixin","namespace","operation","body","attribute","dictionary","field","const","enum","enum-value","v","iterable","maplike","setlike","callback interface","things","results","thing","dispatch","getMixinMap","all","Map","include","array","validateIterable","duplicates","WeakMap","groupDefinitions","dup","checkDuplicatedNames","flat","__webpack_exports__"],"mappings":"CAAA,SAAAA,EAAAC,GACA,iBAAAC,SAAA,iBAAAC,OACAA,OAAAD,QAAAD,IACA,mBAAAG,eAAAC,IACAD,OAAA,GAAAH,GACA,iBAAAC,QACAA,QAAA,QAAAD,IAEAD,EAAA,QAAAC,IARA,CASCK,KAAA,WACD,mBCTA,IAAAC,EAAA,GAGA,SAAAC,EAAAC,GAGA,GAAAF,EAAAE,GACA,OAAAF,EAAAE,GAAAP,QAGA,IAAAC,EAAAI,EAAAE,GAAA,CACAC,EAAAD,EACAE,GAAA,EACAT,QAAA,IAUA,OANAU,EAAAH,GAAAI,KAAAV,EAAAD,QAAAC,IAAAD,QAAAM,GAGAL,EAAAQ,GAAA,EAGAR,EAAAD,QA0DA,OArDAM,EAAAM,EAAAF,EAGAJ,EAAAO,EAAAR,EAGAC,EAAAQ,EAAA,SAAAd,EAAAe,EAAAC,GACAV,EAAAW,EAAAjB,EAAAe,IACAG,OAAAC,eAAAnB,EAAAe,EAAA,CAA0CK,YAAA,EAAAC,IAAAL,KAK1CV,EAAAgB,EAAA,SAAAtB,GACA,oBAAAuB,eAAAC,aACAN,OAAAC,eAAAnB,EAAAuB,OAAAC,YAAA,CAAwDC,MAAA,WAExDP,OAAAC,eAAAnB,EAAA,cAAiDyB,OAAA,KAQjDnB,EAAAoB,EAAA,SAAAD,EAAAE,GAEA,GADA,EAAAA,IAAAF,EAAAnB,EAAAmB,IACA,EAAAE,EAAA,OAAAF,EACA,KAAAE,GAAA,iBAAAF,QAAAG,WAAA,OAAAH,EACA,IAAAI,EAAAX,OAAAY,OAAA,MAGA,GAFAxB,EAAAgB,EAAAO,GACAX,OAAAC,eAAAU,EAAA,WAAyCT,YAAA,EAAAK,UACzC,EAAAE,GAAA,iBAAAF,EAAA,QAAAM,KAAAN,EAAAnB,EAAAQ,EAAAe,EAAAE,EAAA,SAAAA,GAAgH,OAAAN,EAAAM,IAAqBC,KAAA,KAAAD,IACrI,OAAAF,GAIAvB,EAAA2B,EAAA,SAAAhC,GACA,IAAAe,EAAAf,KAAA2B,WACA,WAA2B,OAAA3B,EAAA,SAC3B,WAAiC,OAAAA,GAEjC,OADAK,EAAAQ,EAAAE,EAAA,IAAAA,GACAA,GAIAV,EAAAW,EAAA,SAAAiB,EAAAC,GAAsD,OAAAjB,OAAAkB,UAAAC,eAAA1B,KAAAuB,EAAAC,IAGtD7B,EAAAgC,EAAA,GAIAhC,IAAAiC,EAAA,kCCjEA,SAASC,EAAKC,EAAAC,EAAAC,EAAAC,EAAAC,GAAAC,MAA4CA,EAAA,QAAAC,UAAAC,YAAqC,IAI/F,SAAAC,EAAAC,GACA,OAAAA,EAAA,EACAT,EAAAU,MAAAT,IAAAQ,GACAT,EAAAU,MAAAC,KAAAC,IAAAX,EAAAQ,EAAA,GAAAR,GAGA,SAAAY,EAAAC,GAAAC,SAAiCA,GAAW,IAC5C,MAAAC,EAAAF,EAAAG,IAAAhC,KAAAiC,OAAAjC,EAAAD,OAAAmC,KAAA,IACAC,EAAApB,EAAAC,GACA,cAAAmB,EAAAC,KACAL,EAEAD,EACAC,EAAAI,EAAAF,OAEAF,EAAAN,MAAAU,EAAAF,OAAAI,QAGA,MACAC,EACA,QAAAvB,EAAAC,GAAAoB,KAAArB,EAAAC,GAAAsB,KACAvB,EAAAsB,OAAA,EAAAtB,EAAAC,EAAA,GAAAsB,KACA,EAEAC,EA1CA,SAAAR,GACA,MAAAS,EAAAT,EAAAU,MAAA,MACA,OAAAD,IAAAH,OAAA,GAwCAK,CACAd,EAAAL,GAPA,GAOA,CAA2CO,UAAA,KAG3Ca,EAAApB,EAVA,GAWAqB,EAAAhB,EAAAe,GAIAE,EAAAN,EAHAK,EAAAH,MAAA,SAGA,MADA,IAAAK,OAAAP,EAAAF,QAAA,KAGAU,EAAA,WAAA5B,EAAA,iBAGA6B,KAAqB7B,mBAAsBmB,IAF3CvB,EAAA1B,YAA4C0B,EAAA1B,OAAY,KACxD4B,KAAA5B,UAA8D0D,OAAiB9B,EAAAgC,QAAA,gBAAoChC,EAAAmB,QAAgBnB,EAAA5B,SAAa,QACvDwD,IACzF,OACA3B,WAAgB8B,KAAW9B,IAC3BgC,YAAAhC,EACA8B,UACAV,OACAa,WAAApC,EAAA1B,KACA+B,QACAE,WACAD,UACA+B,MAAAR,EACAS,OAAAV,GAOO,SAAAW,EAAAvC,EAAAC,EAAAC,EAAAC,GACP,OAASJ,EAAKC,EAAAC,EAAAC,EAAAC,EAAA,UAOP,SAAAqC,EAAAC,EAAAvC,EAAAK,EAAAJ,EAAAuC,EAAA,IAEP,OADAA,EAAAnC,WACSR,EAAKG,EAAAF,OAAAyC,EAAAE,MAAAzC,EAAAC,EAAA,aAAAuC,UCvFP,MAAAE,KACPC,aAAA7C,OAAeA,EAAAsC,WACf7D,OAAAqE,iBAAAnF,KAAA,CACAqC,OAAA,CAAehB,MAAAgB,GACfsC,OAAA,CAAetD,MAAAsD,KAIfO,SACA,MAAAE,EAAA,CAAkB1B,UAAA2B,EAAA1E,UAAA0E,EAAAC,iBAAAD,GAClB,IAAAE,EAAAvF,KACA,KAAAuF,IAAAzE,OAAAkB,WAAA,CACA,MAAAwD,EAAA1E,OAAA2E,0BAAAF,GACA,UAAA5D,EAAAN,KAAAP,OAAA4E,QAAAF,IACAnE,EAAAL,YAAAK,EAAAJ,OACAmE,EAAAzD,GAAA3B,KAAA2B,IAGA4D,EAAAzE,OAAA6E,eAAAJ,GAEA,OAAAH,GCbO,SAAAQ,EAAAC,EAAAC,GAAAC,iBAAmDA,GAAmB,IAC7E,IAAAF,EAAAG,MAAA,CACA,MAAAC,EAAAH,EAAAI,OAAAjF,IAAA4E,WACA,IAAAI,EACA,OAEA,eAAAA,EAAAvC,KAAA,CACA,MAAAyC,0BAAaA,GAA2BL,EAAAM,MACxC,GAAAD,EAAAE,IAAAJ,GAGA,OAAAE,EAAAlF,IAAAgF,GAEAH,EAAAM,MAAAD,0BAAAG,IAAAL,OAAAZ,GACA,MAAAkB,EAAAX,EAAAK,EAAAJ,QAAAC,GAEA,GADAA,EAAAM,MAAAD,0BAAAG,IAAAL,EAAAM,GACAA,EACA,OAAAV,EAGA,kBAAAI,EAAAvC,OAAAqC,IAAAF,EAAAW,UACA,OAAAX,EAGA,UAAAY,KAAAZ,EAAAY,QAAA,CACA,MAAAF,EAAAX,EAAAa,EAAAX,GACA,GAAAS,EACA,OAAAE,EAAAT,MACAO,EAEAE,GCYA,SAAAC,EAAAC,EAAAC,GACA,MAAAJ,EAAAG,EAAAE,QAAA,KACAL,IACAI,EAAAjC,OAAA6B,YAEAG,EAAAG,MAAA,MAAAH,EAAAI,MAAA,iCAOA,SAAAC,EAAAL,EAAAM,GACA,IAAAC,EApDA,SAAAP,EAAAM,GACA,MAAAE,EAAAR,EAAAE,QAAA,6CACA,IAAAM,EACA,OAEA,MAAAD,EAAA,IAAkBE,UAAI,CAAE/E,OAAAsE,EAAAtE,OAAAsC,OAAA,CAAoCwC,UAE5D,OADAD,EAAAvC,OAAA0C,KAAAV,EAAAE,QAAA,MAAAF,EAAAI,kCAA0FI,EAAAzD,QAC1FyD,EAAAzD,MACA,eACAiD,EAAAG,MAAA,MAAAH,EAAAI,MAAA,+CACA,MAAAN,EAAsBa,EAAWX,EAAAM,IAAAN,EAAAI,MAAA,2BACjCG,EAAAT,QAAAc,KAAAd,GACA,MAEA,eACA,mBACA,MAAAA,EAAsBe,EAA6Bb,EAAAM,IAAAN,EAAAI,iBAAoDI,EAAAzD,gBACvGwD,EAAAT,QAAAc,KAAAd,GACA,MAEA,cACAE,EAAAG,MAAA,MAAAH,EAAAI,MAAA,6CACA,MAAAU,EAAAd,EAAAE,WAA2Ca,IAAWf,EAAAI,oCAAmDW,EAAWlE,KAAA,SACpHmE,EAAA,IAA6BP,UAAI,CAAE/E,OAAAsE,EAAAtE,OAAAsC,OAAA,CAAoCwC,KAAAM,KACvEE,EAAAhD,OAAAiD,UAAAjB,EAAAE,QAAA,MAAAF,EAAAI,MAAA,uCACAY,EAAAjE,KAAAuD,EACA,MAAAY,EAAwBL,EAA6Bb,EAAAM,IAAAN,EAAAI,MAAA,qCACrDG,EAAAT,QAAAc,KAAAI,EAAAE,GACA,OAKA,OAFAX,EAAArB,SAAAc,EAAAI,oCAAkEI,EAAAzD,QAClEwD,EAAAvC,OAAAmD,MAAAnB,EAAAE,QAAA,MAAAF,EAAAI,uCAAgGI,EAAAzD,QAChGwD,EAmBAa,CAAApB,EAAAM,IAAiDe,EAAcrB,GAC/D,IAAAO,EAAA,CACA,MAAAC,EAAAR,EAAAE,QAAA,gBAAoDa,GACpD,IAAAP,EACA,OAEAD,EAAA,IAAcE,UAAI,CAAE/E,OAAAsE,EAAAtE,OAAAsC,OAAA,CAAoCwC,UACxDR,EAAAG,MAAA,MAAAH,EAAAI,kCAA0EI,EAAA9F,SAQ1E,MANA,YAAA6F,EAAAe,SAAAtB,EAAAG,MAAA,MACAH,EAAAI,MAAA,mCAEAG,EAAAxD,KAAAuD,GAAA,KACAP,EAAAC,EAAAO,GACAA,EAAAV,UAAA,QAAAU,EAAArB,SAAAc,EAAAI,MAAA,sCACAG,EA+BO,MAAME,kBAAanC,KAK1BC,aAAAyB,EAAAM,GACA,OAAAD,EAAAL,EAAAM,IA9BA,SAAAN,EAAAjD,GACA,MAAAiB,EAAA,GAEA,GADAA,EAAA0C,KAAAV,EAAAE,QAAA,MACAlC,EAAA0C,KAAA,OACA,MAAAH,EAAA,IAAkBE,UAAI,CAAE/E,OAAAsE,EAAAtE,OAAAsC,WAExB,IADAuC,EAAAxD,QAAA,OACA,CACA,MAAAwE,EAAgBV,EAA6Bb,MAAAI,MAAA,wDAC7C,QAAAmB,EAAArC,SAAAc,EAAAI,MAAA,iDACAG,EAAAT,QAAAc,KAAAW,GACA,MAAAC,EAAAxB,EAAAE,QAAA,MACA,IAAAsB,EAGA,MAFAD,EAAAvD,OAAAiD,UAAAO,EASA,OALAjB,EAAArB,QAAAlC,OAAA,GACAgD,EAAAI,MAAA,kEAEApC,EAAAmD,MAAAnB,EAAAE,QAAA,MAAAF,EAAAI,MAAA,2BACAL,EAAAC,EAAAO,GACAA,EASAkB,CAAAzB,EAAAM,GAGA/B,aAAA7C,OAAeA,EAAAsC,WACf0D,MAAA,CAAWhG,SAAAsC,WACX7D,OAAAC,eAAAf,KAAA,WAA4CqB,MAAA,KAC5CrB,KAAAsI,SAAA,GAGAL,cACA,OAAAjI,KAAAyG,QAAA9C,QAAA3D,KAAA2E,OAAAwC,KACAnH,KAAA2E,OAAAwC,KAAA9F,MAEA,GAEAmF,eACA,OAAA+B,QAAAvI,KAAA2E,OAAA6B,UAEAR,YACA,OAAAuC,QAAAvI,KAAAyG,QAAA9C,UAAA3D,KAAA2E,OAAAwC,KAEAtB,cACA,GAAA7F,KAAAyG,QAAA9C,OACA,OAAA3D,KAAAyG,QAQA,OAAW+B,EALX,CACAxI,KAAA2E,OAAA8D,OACAzI,KAAA2E,OAAAwC,KACAnH,KAAA2E,OAAA+D,SACAC,OAAArH,MAAAgC,IAAAhC,KAAAD,OAAAmC,KAAA,MAIA0B,UAAAY,GAKA,MAAA8C,GAAA5I,KAAAgG,OAAAF,EAAAI,OAAAjF,IAAAjB,KAAA6F,SACAgD,EACA7I,KAAAgG,MAAAhG,KACA4I,GAAA,YAAAA,EAAAlF,KAAAkF,EAAA/C,aACAR,EACA,GAAAwD,GAAA7I,KAAAwG,SAAA,CAEA,MAAAsC,EAAwBlD,EAAyBiD,EAAA/C,GACjD,GAAAgD,EAAA,CACA,MAAAC,GAAA/I,KAAAgG,MAAA8C,EAAA9I,MAAA2E,OAAAwC,KACA3E,EAAA,wDACcqC,EAAekE,EAAA/I,KAAA,yBAAAwC,SAI7B,UAAAiE,KAAAzG,KAAAyG,cACAA,EAAAuC,SAAAlD,ICtKO,MAAMmD,wBAAgBhE,KAI7BC,aAAAyB,GACA,MAAAuC,EAAAvC,EAAAE,QAAA,KACA,IAAAqC,EACA,YAEA,MAAAjD,EAAgBkD,EAAWxC,MAAAE,QAAA,0BAA0DF,EAAAI,MAAA,wBACrFqC,EAAA,CAAAnD,GACA,SAAAA,EAAAvC,KAAA,CACA,MAAAoE,EAAAnB,EAAAE,QAAA,MAAAF,EAAAI,MAAA,wCACAqC,EAAA7B,KAAAO,QACK,SAAA7B,EAAAvC,KAAyB,CAC9B,MAAAoE,EAAAnB,EAAAE,QAAA,MAAwCF,EAAAI,MAAA,0CACxCqC,EAAA7B,KAAAO,GAEA,WAAemB,gBAAO,CAAE5G,OAAAsE,EAAAtE,OAAAsC,OAAA,CAAoCuE,UAASE,eAGrElE,aAAA7C,OAAeA,EAAAsC,SAAAyE,eACff,MAAA,CAAWhG,SAAAsC,WACX7D,OAAAC,eAAAf,KAAA,cAA+CqB,MAAA+H,IAG/C1F,WACA,OAAW2F,EAAUrJ,KAAAoJ,WAAA,IAAA1F,KAErBrC,YACA,OAAWgI,EAAUrJ,KAAAoJ,WAAA,IAAA/H,MAErBiI,eACA,OAAWD,EAAUrJ,KAAAoJ,WAAA,IAAAE,UCpCd,MAAAC,kBAAAC,MACPtE,aAAA7C,OAAeA,EAAAsC,WACf0D,QACAvH,OAAAqE,iBAAAnF,KAAA,CACAqC,OAAA,CAAehB,MAAAgB,GACfsC,OAAA,CAAetD,MAAAsD,MCAf,MAAM8E,wDAAoCxE,KAI1CC,aAAAyB,GACA,MAAAhC,EAAA,CAAoBuE,OAAAvC,EAAAE,QAAA,MACpBK,EAAA,IAAoBuC,gDAA2B,CAAEpH,OAAAsE,EAAAtE,OAAAsC,WAejD,OAdAA,EAAAuE,SACAvE,EAAA+E,cAAA/C,EAAAE,QAAA,4CAEAlC,EAAA0C,KAAAV,EAAAE,QAAA,KACAlC,EAAA0C,MACAH,EAAAyC,KAAA,oBAAAzC,EAAA0C,QCiGO,SAAAjD,GACP,MAAAkD,EAAAF,EAAAhD,EAAA,CAA+BmD,OAASC,YAAKD,OAAAnD,EAAA,cAAAqD,SAAA,oBAC7CH,EAAAlG,QACAgD,EAAAI,MAAA,uCAEA,OAAA8C,EDpGQI,CAAWtD,GAEXuD,EAAavD,GACrBhC,EAAAmD,MAAAnB,EAAAE,QAAA,MAAAF,EAAAI,MAAA,yDACKG,EAAAiD,SAAAxF,EAAA+E,eACL/C,EAAAI,MAAA,uDAEAG,EAGA0C,cACA,OAAA5J,KAAA2E,OAAAuE,OACAlJ,KAAA2E,OAAA+E,cACA1J,KAAA2E,OAAA+E,cAAAhG,KADA,kBADA,MAMO,MAAM0G,oDAAgCnF,KAI7CC,aAAAyB,GACA,MAAAhG,EAAAgG,EAAAE,QAAA,cACA,GAAAlG,EACA,WAAiByJ,4CAAuB,CACxC/H,OAAAsE,EAAAtE,OACAsC,OAAA,CAAiBhE,QACjB0J,OAAgBZ,gDAA2Ba,MAAA3D,KAK3CzB,aAAA7C,OAAeA,EAAAsC,SAAA0F,WACfhC,MAAA,CAAWhG,SAAAsC,WACX7D,OAAAC,eAAAf,KAAA,UAA2CqB,MAAAgJ,IAG3C3G,WACA,2BAEA/C,WACA,OAAAX,KAAA2E,OAAAhE,KAAAU,MAEAkJ,UACA,MAAWX,QAAAlG,EAAAiB,SAAAgF,QAA8B3J,KAAAqK,OACzC,OAAA3G,EAIA,CAAYA,OAAArC,MADZ,oBAAAqC,EAAAiG,EAAAhF,EAAA+E,cAAArI,OAFA,KAKAmJ,gBACA,MAAAZ,QAAWA,EAAAD,QAAgB3J,KAAAqK,OAC3B,OAAAV,GAAA,oBAAAC,EAGAD,EAFA,GAKAzE,UAAAY,GACA,yBAAA9F,KAAAW,KAAA,CACA,MAAA6B,EAAA,gOAIYqC,EAAe7E,KAAA2E,OAAAhE,KAAAX,KAAA,uBAAAwC,EAAA,CAA2DE,MAAA,YAEtF,UAAA+H,KAAAzK,KAAAwK,gBACAC,EAAAzB,SAAAlD,IAOO,MAAM4E,+CAA2BnB,UAIxCrE,aAAAyB,GACA,MAAAhC,EAAA,GAEA,GADAA,EAAA0C,KAAAV,EAAAE,QAAA,MACAlC,EAAA0C,KAAA,WAAiCqD,uCAAkB,IACnD,MAAAxD,EAAA,IAAoBwD,uCAAkB,CAAErI,OAAAsE,EAAAtE,OAAAsC,WAYxC,OAXAuC,EAAAK,QAAgBoC,EAAIhD,EAAA,CACpBmD,OAAcM,4CAAuBE,MACrCN,SAAA,wBAEArF,EAAAmD,MAAAnB,EAAAE,QAAA,MAAAF,EAAAI,MAAA,kDACAG,EAAAvD,QACAgD,EAAAI,MAAA,qCAEAJ,EAAAG,MAAA,MACAH,EAAAI,MAAA,kEAEAG,EAGAhC,UAAAY,GACA,UAAA6E,KAAA3K,WACA2K,EAAA3B,SAAAlD,IE/GO,MAAM8E,0BAAiB3F,KAI9BC,aAAAyB,GACA,MAAAkE,EAAAlE,EAAArE,SACAqC,EAAA,GACAuC,EAAA,IAAoB0D,kBAAQ,CAAEvI,OAAAsE,EAAAtE,OAAAsC,WAI9B,OAHAuC,EAAAoB,SAAmBoC,uCAAkBJ,MAAA3D,GACrChC,EAAAmG,SAAAnE,EAAAE,QAAA,YACAK,EAAArB,QAAkB2B,EAA6Bb,EAAA,iBAC/CO,EAAArB,SAGAlB,EAAAmG,WACAnG,EAAAoG,SAAApE,EAAAE,QAAA,QAEAlC,EAAAhE,KAAAgG,EAAAE,QAAA,gBAAqDmE,GACrDrG,EAAAhE,MAGAuG,EAAA+D,QAAAtG,EAAAmG,SAAoC7B,gBAAOqB,MAAA3D,GAAA,KAC3CO,GAHAP,EAAAuE,UAAAL,IAPAlE,EAAAuE,UAAAL,GAaAnH,WACA,iBAEAoH,eACA,QAAA9K,KAAA2E,OAAAmG,SAEAC,eACA,QAAA/K,KAAA2E,OAAAoG,SAEApK,WACA,OAAW6H,EAAQxI,KAAA2E,OAAAhE,KAAAU,OAGnB6D,UAAAY,GAEA,SADA9F,KAAA6F,QAAAmD,SAAAlD,GACQF,EAAyB5F,KAAA6F,QAAAC,EAAA,CAAsBC,kBAAA,IACvD,GAAA/F,KAAA6F,QAAAW,SAAA,CACA,MAAAhE,EAAA,iDACcqC,EAAe7E,KAAA2E,OAAAhE,KAAAX,KAAA,uBAAAwC,QACtB,GAAAxC,KAAA8K,WAAA9K,KAAAiL,QAAA,CACP,MAAAzI,EAAA,yEACcqC,EAAe7E,KAAA2E,OAAAhE,KAAAX,KAAA,mBAAAwC,EAAA,CAC7BG,SAUA8H,EAVAzK,KAWA,KACAyK,EAAAQ,QAAkBhC,gBAAOqB,MAAA,IAAWa,oBAAS,cAF7C,IAAAV,GC/DO,MAAMV,oBAAc9E,KAK3BC,cAAAyB,EAAAjD,GACA,WACA,MAAArC,EAAAsF,EAAAE,QAAAnD,GACA,GAAArC,EACA,WAAmB0I,YAAK,CAAE1H,OAAAsE,EAAAtE,OAAAsC,OAAA,CAAoCtD,YAK9DA,YACA,OAAArB,KAAA2E,OAAAtD,aCbO,MAAM+J,4BAAkBnG,KAI/BC,aAAAyB,GAAA0E,QAA2BA,EAAAC,WAAmB,IAC9C,MAAA3G,EAAA,CAAoB0G,WACpBnE,EAAA,IAAoBkE,oBAAS,CAAE/I,OAAAsE,EAAAtE,OAAAsC,WAC/B,OAAA0G,GAAA,gBAAAA,EAAAhK,QACAsD,EAAA4G,YAAA5E,EAAAE,QAAA,KACAlC,EAAA4G,cACArE,EAAAsD,UAAA,GACAtD,IAGAmE,GAAAC,IACA3G,EAAA0G,QAAA1E,EAAAE,QAAA,8BAEAK,EAAArB,QAAkByB,EAAWX,MAAAI,MAAA,uBAC7BpC,EAAAhE,KAAAgG,EAAAE,QAAA,yBACAlC,EAAA0C,KAAAV,EAAAE,QAAA,MAAAF,EAAAI,MAAA,qBACAG,EAAAsD,UAAoBN,EAAavD,GACjChC,EAAAmD,MAAAnB,EAAAE,QAAA,MAAAF,EAAAI,MAAA,0BACApC,EAAA4G,YAAA5E,EAAAE,QAAA,MAA6CF,EAAAI,MAAA,wCAC7CG,GAGAxD,WACA,kBAEA/C,WACA,MAAAA,KAAWA,GAAOX,KAAA2E,OAClB,OAAAhE,EAGW6H,EAAQ7H,EAAAU,OAFnB,GAIAgK,cACA,OAAArL,KAAA2E,OAAA0G,QAGArL,KAAA2E,OAAA0G,QAAAhK,MAFA,GAKA6D,UAAAY,GACA,IAAA9F,KAAAW,MAAA,cAAA6K,SAAAxL,KAAAqL,SAAA,CACA,MAAA7I,EAAA,qFACYqC,EAAe7E,KAAA2E,OAAA0C,KAAArH,KAAA,gBAAAwC,GAE3BxC,KAAA6F,gBACA7F,KAAA6F,QAAAmD,SAAAlD,IAEA,UAAA2F,KAAAzL,KAAAwK,gBACAiB,EAAAzC,SAAAlD,ICrDO,MAAM4F,4BAAkBzG,KAI/BC,aAAAyB,GAAA0E,QAA2BA,EAAAM,aAAA,EAAAC,YAAA,GAA+C,IAC1E,MAAAf,EAAAlE,EAAArE,SACAqC,EAAA,CAAoB0G,WACpBnE,EAAA,IAAoBwE,oBAAS,CAAErJ,OAAAsE,EAAAtE,OAAAsC,WAY/B,GAXA0G,GAAAM,IACAhH,EAAA0G,QAAA1E,EAAAE,QAAA,YAEA,YAAAK,EAAAmE,SAAA1E,EAAAG,MAAA,aACAH,EAAAI,MAAA,4CAEApC,EAAAiH,SAAAjF,EAAAE,QAAA,YACA+E,IAAAjH,EAAAiH,UAAAjF,EAAAG,MAAA,cACAH,EAAAI,MAAA,+CAEApC,EAAAwC,KAAAR,EAAAE,QAAA,aACAlC,EAAAwC,KAAA,CAKA,OADAD,EAAArB,QAAkB2B,EAA6Bb,EAAA,mBAAAA,EAAAI,MAAA,0BAC/CG,EAAArB,QAAAoC,SACA,eACA,aAAAtB,EAAAI,kCAAiEG,EAAArB,QAAAoC,iBAIjE,OAFAtD,EAAAhE,KAAAgG,EAAAE,QAAA,kCAAAF,EAAAI,MAAA,0BACApC,EAAA4G,YAAA5E,EAAAE,QAAA,MAA6CF,EAAAI,MAAA,wCAC7CG,EAVAP,EAAAuE,UAAAL,GAaAnH,WACA,kBAEA2H,cACA,OAAArL,KAAA2E,OAAA0G,QAGArL,KAAA2E,OAAA0G,QAAAhK,MAFA,GAIAuK,eACA,QAAA5L,KAAA2E,OAAAiH,SAEAjL,WACA,OAAW6H,EAAQxI,KAAA2E,OAAAhE,KAAAU,OAGnB6D,UAAAY,SACA9F,KAAA6F,QAAAmD,SAAAlD,IJ1CO,SAAS0C,EAAQqD,GACxB,OAAAA,EAAAC,WAAA,KAAAD,EAAA9I,MAAA,GAAA8I,EAWO,SAAAlC,EAAAhD,GAAAmD,OAA0BA,EAAAiC,eAAA/B,WAAA,SACjC,MAAAgC,EAAAlC,EAAAnD,GACA,IAAAqF,EACA,SAEAA,EAAArH,OAAAiD,UAAAjB,EAAAE,QAAA,KACA,MAAAoF,EAAA,CAAAD,GACA,KAAAA,EAAArH,OAAAiD,WAAA,CACA,MAAAsE,EAAApC,EAAAnD,GACA,IAAAuF,EAAA,CACAH,GACApF,EAAAI,2BAA6CiD,KAE7C,MAIA,GAFAkC,EAAAvH,OAAAiD,UAAAjB,EAAAE,QAAA,KACAoF,EAAA1E,KAAA2E,IACAA,EAAAvH,OAAAiD,UAAA,MAEA,OAAAqE,EAMO,SAAA9C,EAAAxC,GACP,OAAAA,EAAAE,QAAA,iEAQO,SAAAwC,GAAA3F,KAAqBA,EAAArC,UAC5B,OAAAqC,GACA,WACA,YACA,OAAcA,KAAA,UAAArC,MAAA,SAAAqC,GACd,eACA,gBACA,OAAcA,KAAA,WAAA4F,SAAA5F,EAAAoI,WAAA,MACd,QACA,OAAcpI,KAAA,WAAArC,MAAA,IACd,QACA,OAAcqC,KAAA,cACd,cACA,cACA,OAAcA,KAAA,SAAArC,SACd,aACA,OAAcqC,KAAA,SAAArC,QAAA0B,MAAA,OACd,QACA,OAAcW,SAOP,SAAAsE,EAAArB,GAoBP,MAAAtE,OAASA,GAASsE,EAClBwF,EApBA,WACA,MAAA1D,EAAA9B,EAAAE,QAAA,YACAM,EAAAR,EAAAE,QAAA,gBACA,GAAAM,EAAA,CACA,MAAAuB,EAAA/B,EAAAE,QAAA,QACA,WAAiBO,UAAI,CAAE/E,SAAAsC,OAAA,CAAkB8D,SAAAtB,OAAAuB,aAEzCD,GAAA9B,EAAAI,MAAA,gCAaAqF,IAVA,WACA,MAAA3D,EAAA9B,EAAAE,QAAA,gBACAM,EAAAR,EAAAE,QAAA,kBACA,GAAAM,EACA,WAAiBC,UAAI,CAAE/E,SAAAsC,OAAA,CAAkB8D,SAAAtB,UAEzCsB,GAAA9B,EAAAI,MAAA,8BAIAsF,GACA,GAAAF,EAAA,OAAAA,EACA,MAAAhF,EAAAR,EAAAE,QAAA,0BACA,OAAAM,EACA,IAAeC,UAAI,CAAE/E,SAAAsC,OAAA,CAAkBwC,eADvC,EAmBO,SAAA+C,EAAAvD,GACP,OAAAgD,EAAAhD,EAAA,CAA0BmD,OAASc,kBAAQN,MAAAN,SAAA,mBAOpC,SAAAxC,EAAAb,EAAAM,GACP,MAAAqB,EAAmBoC,uCAAkBJ,MAAA3D,GACrCO,EAAcE,UAAIkD,MAAA3D,EAAAM,GAElB,OADAC,MAAAoB,YACApB,EAOO,SAAAI,EAAAX,EAAAM,GACP,MAAAiB,EAAcd,UAAIkD,MAAA3D,EAAAM,GAAA,eAClB,GAAAiB,EACA,OAAAA,EAEA,MAAAoE,EAAA3F,EAAAE,QAAA,QACA,GAAAyF,EAAA,CACA,MAAApF,EAAA,IAAoBE,UAAI,CAAE/E,OAAAsE,EAAAtE,OAAAsC,OAAA,CAAoCwC,KAAAmF,KAE9D,OADApF,EAAAxD,KAAA,cACAwD,GAOO,SAAAqF,EAAA5F,GACP,MAAA0E,EAAA1E,EAAAE,QAAA,eACA,GAAAwE,EAIA,OAHiBK,oBAASpB,MAAA3D,EAAA,CAAmB0E,aACzCD,oBAASd,MAAA3D,EAAA,CAAmB0E,aAChC1E,EAAAI,MAAA,4BAOO,SAAAyF,EAAAC,GACP,MAAAC,EAAAD,EAAA1I,MAAA,MAEA,GAAA2I,EAAA/I,OAAA,CACA,MAAAgJ,EAAAD,IAAA/I,OAAA,GAAAgJ,MAAA,QACA,GAAAA,EACA,OAAAA,EAAA,GAGA,SAgBO,SAAAC,EAAA3G,GACP,WACA,GAAAA,EAAAqC,SAAA3E,OAAA,CACA,MAAAgD,EAAA,IAA4BwE,oBAAS,mBACrC0B,EAAsBzC,4CAAuBE,MAAA3D,GAC7CkG,EAAAlI,OAAAiD,UAAAjB,EAAAE,QAAA,KACA,MAAAiG,EAAA7G,EAAAqC,SAAA,GACA,MAAAyE,KAAAD,EAAAnI,OAAAhE,KAAA4C,UACAuJ,EAAAnI,OAAAhE,KAAA4C,WAA0CuJ,EAAAnI,OAAAhE,KAAA4C,UAE1C0C,EAAAqC,SAAA0E,QAAAH,OACK,CACL5G,EAAAqC,SAAqBoC,uCAAkBJ,MAAA,IAAWa,oBAAS,qBAC3D,MAAA5H,EAAA0C,EAAAtB,OAAAwC,KAAA5D,OACA0C,EAAAqC,SAAA3D,OAAA0C,KAAA9D,SACA0C,EAAAtB,OAAAwC,KAAA5D,YAAoCiJ,EAAAjJ,OK/MpC,MAAA0J,EAAA,CAGAC,QAAA,sGACAC,QAAA,8CACAtB,WAAA,+BACAuB,OAAA,WACAC,WAAA,cACAC,QAAA,iDACAC,MAAA,wBAGO7F,EAAA,CACP,aACA,YACA,aAGOsD,EAAA,CACP,QACA,YACA,WACA,QACA,cACA,UACA,aACA,OACA,SACA,WACA,UACA,YACA,WACA,UACA,YACA,UACA,WACA,UACA,SACA,SACA,cACA,UACA,gBAGAwC,EAAA,CACA,YACA,cACA,WACA,MACA,UACA,QACA,UACA,OACA,cACA,SACA,QACA,QACA,OACA,QACA,OACA,QACA,WACA,KACA,WACA,SACA,WACA,QACA,OACA,WACA,QACAC,OAAAzC,EAAAtD,GAEAgG,EAAA,CACA,IACA,IACA,IACA,MACA,IACA,IACA,IACA,IACA,IACA,IACA,IACA,IACA,IACA,KAGAC,EAAA,CAEA,eACA,WACA,aAmGO,MAAMxC,oBAIbjG,YAAA0I,GACA5N,KAAAqC,OAlGA,SAAAoK,GACA,MAAA9H,EAAA,GACA,IAAAkJ,EAAA,EACAtK,EAAA,GACAK,EAAA,EACAoB,EAAA,EACA,KAAA6I,EAAApB,EAAA9I,QAAA,CACA,MAAAmK,EAAArB,EAAAsB,OAAAF,GACA,IAAAtH,GAAA,EAQA,GANA,YAAAwG,KAAAe,GACAvH,EAAAyH,EAAA,cAAgDC,eAAA,IAC3C,MAAAH,IACLvH,EAAAyH,EAAA,WAA6CC,eAAA,MAG7C,IAAA1H,EAAA,CACA,MAAA2H,EAAAvJ,EAAAwJ,MAAA9M,MACAuC,IAAAsK,EAAAvB,MAAA,YAAAhJ,OACAJ,GAAA2K,EACAlJ,GAAA,OACK,oBAAA+H,KAAAe,IAKL,IAHA,KADAvH,EAAAyH,EAAA,cAEAzH,EAAAyH,EAAA,aAEA,IAAAzH,EAAA,CACAA,EAAAyH,EAAA,cACA,MAAAI,EAAAzJ,EAAAhB,OAAA,EACAmB,EAAAH,EAAAyJ,GACA,QAAA7H,EAAA,CACA,GAAAoH,EAAAnC,SAAA1G,EAAAzD,OAAA,CACA,MAAAmB,KAA+BgG,EAAQ1D,EAAAzD,wDACvC,UAAAgN,iBAAuCzJ,EAAWD,EAAAyJ,EAAA,KAAA5L,IACvCgL,EAAAhC,SAAA1G,EAAAzD,SACXyD,EAAApB,KAAAoB,EAAAzD,aAIK,MAAAyM,IACLvH,EAAAyH,EAAA,WAGA,UAAAM,KAAAZ,EACA,GAAAjB,EAAAX,WAAAwC,EAAAT,GAAA,CACAlJ,EAAA4C,KAAA,CAAqB7D,KAAA4K,EAAAjN,MAAAiN,EAAA/K,SAAAK,OAAAoB,UACrBzB,EAAA,GAEAgD,EADAsH,GAAAS,EAAA3K,OAEA,MAQA,IAHA,IAAA4C,IACAA,EAAAyH,EAAA,WAEA,IAAAzH,EACA,UAAAgI,MAAA,gCAEAV,EAAAtH,EACAvB,GAAA,EAUA,OANAL,EAAA4C,KAAA,CACA7D,KAAA,MACArC,MAAA,GACAkC,WAGAoB,EAOA,SAAAqJ,EAAAtK,GAAAuK,cAAoCA,GAAgB,IACpD,MAAAO,EAAAvB,EAAAvJ,GACA8K,EAAAJ,UAAAP,EACA,MAAAtH,EAAAiI,EAAAC,KAAAhC,GACA,OAAAlG,GACA5B,EAAA4C,KAAA,CAAmB7D,OAAArC,MAAAkF,EAAA,GAAAhD,SAAAK,OAAAoB,UACnBiJ,IACA1K,EAAA,IAEAiL,EAAAJ,YAEA,GASAM,CAAAd,GACA5N,KAAAsC,SAAA,EAMA4C,MAAA1C,GACA,UAAA6L,iBAA+BzJ,EAAW5E,KAAAqC,OAAArC,KAAAsC,SAAAtC,KAAAuC,QAAAC,IAM1C0C,MAAAxB,GACA,OAAA1D,KAAAqC,OAAAsB,OAAA3D,KAAAsC,UAAAtC,KAAAqC,OAAArC,KAAAsC,UAAAoB,SAMAwB,WAAAyJ,GACA,UAAAjL,KAAAiL,EAAA,CACA,IAAA3O,KAAA8G,MAAApD,GAAA,SACA,MAAAoB,EAAA9E,KAAAqC,OAAArC,KAAAsC,UAEA,OADAtC,KAAAsC,WACAwC,GAOAI,UAAA5C,GACAtC,KAAAsC,YAIO,MAAA+L,yBAAAE,MACPrJ,aAAA1C,QAAeA,EAAAgC,cAAAF,UAAAV,OAAAa,aAAAC,QAAAC,WACf0D,MAAA7F,GAEAxC,KAAAW,KAAA,mBACAX,KAAAwE,cACAxE,KAAAsE,UACAtE,KAAA4D,OACA5D,KAAAyE,aACAzE,KAAA0E,QACA1E,KAAA2E,UCtPA,MAAMiK,uBAAkB7E,YAIxB7E,aAAAyB,GACA,MAAAtF,EAAAsF,EAAAE,QAAA,UACA,GAAAxF,EACA,WAAiBuN,eAAS,CAAEvM,OAAAsE,EAAAtE,OAAAsC,OAAA,CAAoCtD,WAIhEqC,WACA,mBAEArC,YACA,OAAAgH,MAAAhH,MAAA0B,MAAA,OAIO,MAAM8L,kBAAa5J,KAI1BC,aAAAyB,GACA,MAAAhC,EAAA,GAEA,GADAA,EAAAwC,KAAAR,EAAAE,QAAA,SACAlC,EAAAwC,KACA,OAEAxC,EAAAhE,KAAAgG,EAAAE,QAAA,eAAAF,EAAAI,MAAA,oBACA,MAAAG,EAAAP,EAAApE,QAAA,IAAwCsM,UAAI,CAAExM,OAAAsE,EAAAtE,OAAAsC,WAe9C,OAdAA,EAAA0C,KAAAV,EAAAE,QAAA,MAAsCF,EAAAI,MAAA,iBACtCG,EAAA4H,OAAiBnF,EAAIhD,EAAA,CACrBmD,OAAc8E,eAAStE,MACvByB,cAAA,EACA/B,SAAA,gBAEArD,EAAAG,MAAA,WACAH,EAAAI,MAAA,gCAEApC,EAAAmD,MAAAnB,EAAAE,QAAA,MAAuCF,EAAAI,MAAA,4BACvCG,EAAA4H,OAAAnL,QACAgD,EAAAI,MAAA,oBAEApC,EAAA4G,YAAA5E,EAAAE,QAAA,MAA6CF,EAAAI,MAAA,2BAC7CG,EAGAxD,WACA,aAEA/C,WACA,OAAW6H,EAAQxI,KAAA2E,OAAAhE,KAAAU,QCrDZ,MAAM0N,0BAAiB9J,KAI9BC,aAAAyB,GACA,MAAAkC,EAAAlC,EAAAE,QAAA,cACA,IAAAgC,EACA,OAEA,MAAAlE,EAAA,CAAoBkE,UAEpB,GADAlE,EAAA6G,SAAA7E,EAAAE,QAAA,YACAlC,EAAA6G,SAMA,OAFA7G,EAAAqK,MAAArI,EAAAE,QAAA,eAAAF,EAAAI,MAAA,iCACApC,EAAA4G,YAAA5E,EAAAE,QAAA,MAA6CF,EAAAI,MAAA,2CAC7C,IAAegI,kBAAQ,CAAE1M,OAAAsE,EAAAtE,OAAAsC,WALzBgC,EAAAuE,UAAArC,EAAA7D,OAQAtB,WACA,iBAEAmF,aACA,OAAWL,EAAQxI,KAAA2E,OAAAkE,OAAAxH,OAEnBmK,eACA,OAAWhD,EAAQxI,KAAA2E,OAAAqK,MAAA3N,QC3BZ,MAAM4N,wBAAgBhK,KAI7BC,aAAAyB,GACA,MAAAhC,EAAA,GACAuC,EAAA,IAAoB+H,gBAAO,CAAE5M,OAAAsE,EAAAtE,OAAAsC,WAE7B,GADAA,EAAAwC,KAAAR,EAAAE,QAAA,WACAlC,EAAAwC,KAOA,OAJAD,EAAArB,QAAkB2B,EAA6Bb,EAAA,iBAAAA,EAAAI,MAAA,wBAC/CpC,EAAAhE,KAAAgG,EAAAE,QAAA,eAAAF,EAAAI,MAAA,wBACAJ,EAAApE,QAAA2E,EACAvC,EAAA4G,YAAA5E,EAAAE,QAAA,MAA6CF,EAAAI,MAAA,sCAC7CG,EAGAxD,WACA,gBAEA/C,WACA,OAAW6H,EAAQxI,KAAA2E,OAAAhE,KAAAU,OAGnB6D,UAAAY,SACA9F,KAAA6F,QAAAmD,SAAAlD,IC1BO,MAAMoJ,kCAAyBjK,KAItCC,aAAAyB,EAAAQ,GACA,MAAAxC,EAAA,CAAoBwC,QACpBD,EAAA,IAAoBgI,0BAAgB,CAAE7M,OAAAsE,EAAAtE,OAAAsC,WAStC,OARAA,EAAAhE,KAAAgG,EAAAE,QAAA,eAAAF,EAAAI,MAAA,yBACAJ,EAAApE,QAAA2E,EACAvC,EAAAuE,OAAAvC,EAAAE,QAAA,MAAAF,EAAAI,MAAA,gCACAG,EAAArB,QAAkByB,EAAWX,MAAAI,MAAA,gCAC7BpC,EAAA0C,KAAAV,EAAAE,QAAA,MAAAF,EAAAI,MAAA,4CACAG,EAAAsD,UAAoBN,EAAavD,GACjChC,EAAAmD,MAAAnB,EAAAE,QAAA,MAAAF,EAAAI,MAAA,yBACApC,EAAA4G,YAAA5E,EAAAE,QAAA,MAA6CF,EAAAI,MAAA,uCAC7CG,EAGAxD,WACA,iBAEA/C,WACA,OAAW6H,EAAQxI,KAAA2E,OAAAhE,KAAAU,OAGnB6D,UAAAY,SACA9F,KAAA6F,QAAAmD,SAAAlD,ICbO,MAAMqJ,4BAAkBlK,KAM/BC,aAAAyB,EAAAyI,GAAA1L,KAAuCA,EAAA2L,cAAAC,mBACvC,MAAA3K,OAAaA,GAASyK,EAQtB,IAPAzK,EAAAhE,KAAAgG,EAAAE,QAAA,eAAAF,EAAAI,yBAA0FqI,EAAA1L,QAC1FiD,EAAApE,QAAA6M,EACAC,GACAvO,OAAAoI,OAAAvE,EApBA,SAAAgC,GACA,MAAA4I,EAAA5I,EAAAE,QAAA,KACA,OAAA0I,EAIA,CAAUA,QAAAjK,YADVqB,EAAAE,QAAA,eAAAF,EAAAI,MAAA,6BAFA,GAiBAzB,CAAAqB,IAEAhC,EAAA0C,KAAAV,EAAAE,QAAA,MAAwCF,EAAAI,kBAAkCrD,KAC1E0L,EAAAI,QAAA,KACA,CAEA,GADA7K,EAAAmD,MAAAnB,EAAAE,QAAA,KACAlC,EAAAmD,MAEA,OADAnD,EAAA4G,YAAA5E,EAAAE,QAAA,MAAmDF,EAAAI,iCAAiDrD,KACpG0L,EAEA,MAAAK,EAAmB/E,uCAAkBJ,MAAA3D,GACrC,IAAA+I,EACA,UAAA5F,KAAA6F,KAAAL,EAEA,GADAI,EAAA5F,EAAAnD,KAAAgJ,GAEA,MAGAD,GACA/I,EAAAI,MAAA,kBAEA2I,EAAApH,SAAAmH,EACAL,EAAAI,QAAAjI,KAAAmI,IAIAnL,cACA,QAAAvE,KAAA2E,OAAAJ,QAEA5D,WACA,OAAa6H,EAAQxI,KAAA2E,OAAAhE,KAAAU,OAErBiE,kBACA,OAAAtF,KAAA2E,OAAAW,YAGakD,EAAQxI,KAAA2E,OAAAW,YAAAjE,OAFrB,KAKA6D,UAAAY,GACA,UAAA8J,KAAA5P,KAAAwP,QACAI,EAAA5G,iBACA4G,EAAA5G,SAAAlD,KCjEO,MAAM+J,0BAAiB5K,KAI9BC,aAAAyB,GACA,MAAAhC,EAAA,GAEA,GADAA,EAAAwC,KAAAR,EAAAE,QAAA,UACAlC,EAAAwC,KACA,OAEA,IAAAtB,EAAkBmC,EAAcrB,GAChC,IAAAd,EAAA,CACA,MAAAsB,EAAAR,EAAAE,QAAA,eAAAF,EAAAI,MAAA,sBACAlB,EAAA,IAAoBuB,UAAI,CAAE/E,OAAAsE,EAAAtE,OAAAsC,OAAA,CAAoCwC,UAE9DR,EAAAG,MAAA,MACAH,EAAAI,MAAA,qCAEAlB,EAAAnC,KAAA,aACAiB,EAAAhE,KAAAgG,EAAAE,QAAA,eAAAF,EAAAI,MAAA,sBACApC,EAAAuE,OAAAvC,EAAAE,QAAA,MAAAF,EAAAI,MAAA,gCACApC,EAAAtD,MAAmB8H,EAAWxC,MAAAI,MAAA,uBAC9BpC,EAAA4G,YAAA5E,EAAAE,QAAA,MAA6CF,EAAAI,MAAA,oCAC7C,MAAAG,EAAA,IAAoB2I,kBAAQ,CAAExN,OAAAsE,EAAAtE,OAAAsC,WAE9B,OADAuC,EAAArB,UACAqB,EAGAxD,WACA,cAEA/C,WACA,OAAAmP,SAAA9P,KAAA2E,OAAAhE,KAAAU,OAEAA,YACA,OAAWgI,EAAUrJ,KAAA2E,OAAAtD,QCpCd,MAAM0O,8BAAqB9K,KAIlCC,aAAAyB,GACA,MAAAkE,EAAAlE,EAAArE,SACAqC,EAAA,GACAuC,EAAA,IAAoB6I,sBAAY,CAAE1N,OAAAsE,EAAAtE,OAAAsC,WASlC,GARAA,EAAAiH,SAAAjF,EAAAE,QAAA,YACAlC,EAAAiH,WACAjH,EAAAqL,MAAArJ,EAAAE,QAAA,UAEAlC,EAAAwC,KACAxC,EAAAiH,SAAAjF,EAAAE,QAAA,qBACAlC,EAAAqL,MAAArJ,EAAAE,QAAA,YACAF,EAAAE,QAAA,iCACAlC,EAAAwC,KAEA,YADAR,EAAAuE,UAAAL,GAIA,MAAAnH,KAAWA,GAAOwD,EAClB+I,EAAA,YAAAvM,GAAAwD,EAAA8I,MACAE,EAAAD,GAAA,aAAAvM,EAEAiB,EAAA0C,KAAAV,EAAAE,QAAA,MAAAF,EAAAI,yCAA+FrD,iBAC/F,MAAAsI,EAAkBxE,EAA6Bb,MAAAI,oCAA6DrD,iBAc5G,OAbAwD,EAAArB,QAAA,CAAAmG,GACAkE,IACAlE,EAAArH,OAAAiD,UAAAjB,EAAAE,QAAA,KACAmF,EAAArH,OAAAiD,UACAV,EAAArB,QAAA0B,KAAyBC,EAA6Bb,IAEtDsJ,GACAtJ,EAAAI,yCAA2DrD,kBAG3DiB,EAAAmD,MAAAnB,EAAAE,QAAA,MAAAF,EAAAI,4CAAmGrD,iBACnGiB,EAAA4G,YAAA5E,EAAAE,QAAA,MAA6CF,EAAAI,iCAAiDrD,iBAE9FwD,EAGAxD,WACA,OAAA1D,KAAA2E,OAAAwC,KAAA9F,MAEAuK,eACA,QAAA5L,KAAA2E,OAAAiH,SAEAoE,YACA,QAAAhQ,KAAA2E,OAAAqL,OClDO,MAAMG,gCAAoBlL,KAIjCC,aAAAyB,GACA,MAAAQ,EAAAR,EAAAE,QAAA,eACA,IAAAM,EACA,OAEA,MAAAxC,EAAA,CAAoBwC,QACpBxC,EAAA0C,KAAAV,EAAAE,QAAA,MAAAF,EAAAI,MAAA,mCACA,MAAA4I,EAAiBzF,EAAavD,GAC9BhC,EAAAmD,MAAAnB,EAAAE,QAAA,MAAAF,EAAAI,MAAA,4BACApC,EAAA4G,YAAA5E,EAAAE,QAAA,MAA6CF,EAAAI,MAAA,kCAC7C,MAAAG,EAAA,IAAoBiJ,wBAAW,CAAExL,WAEjC,OADAuC,EAAAsD,UAAAmF,EACAzI,EAGAxD,WACA,oBAGAwB,UAAAY,GACA9F,KAAA6F,gBACA7F,KAAA6F,QAAAmD,SAAAlD,IAEA,UAAA2F,KAAAzL,KAAAwK,gBACAiB,EAAAzC,SAAAlD,ICjBA,SAAAsK,EAAAzJ,GACA,MAAA0E,EAAA1E,EAAAE,QAAA,UACA,GAAAwE,EAIA,OAHiBK,oBAASpB,MAAA3D,EAAA,CAAmB0E,aACzCD,oBAASd,MAAA3D,EAAA,CAAmB0E,aAChC1E,EAAAI,MAAA,4BAIO,MAAMsJ,4BAAkBlB,oBAI/BjK,aAAAyB,EAAAQ,GAAA5C,QAAiCA,EAAA,MAAiB,IAClD,MAAAI,EAAA,CAAoBJ,UAAA4C,QACpB,OAAWgI,oBAAS7E,MAAA3D,EAAA,IAAsB0J,oBAAS,CAAEhO,OAAAsE,EAAAtE,OAAAsC,WAAmC,CACxFjB,KAAA,YACA2L,aAAA9K,EACA+K,eAAA,CACA,CAASO,kBAAQvF,OACjB,CAAS6F,wBAAW7F,OACpB,CAAA8F,GACA,CAAS7D,GACT,CAASwD,sBAAYzF,OACrB,CAASoB,oBAASpB,OAClB,CAASc,oBAASd,UAKlB5G,WACA,kBAGAwB,UAAAY,GAEA,SADA9F,KAAAsI,SAAAU,SAAAlD,IAEA9F,KAAAuE,SACAvE,KAAAsI,SAAAgI,MAAA3F,GAAA,YAAAA,EAAAhK,OACAX,KAAAsI,SAAAgI,MAAA3F,GAAA,sBAAAA,EAAAhK,MACA,CACA,MAAA6B,EAAA,oTAKYqC,EAAe7E,KAAA2E,OAAAhE,KAAAX,KAAA,kBAAAwC,EAAA,CAC3BG,QAAiBiK,EAAuB5M,QAGxC,MAAAuQ,EAAAvQ,KAAAsI,SAAAK,OAAAgC,GAAA,gBAAAA,EAAAhK,MACA,UAAA6P,KAAAD,EAAA,CACA,MAAA/N,EAAA,oRAIYqC,EAAe2L,EAAA7L,OAAAhE,KAAAX,KAAA,qBAAAwC,EAAA,CAC3BG,QAAA8N,EAAAzQ,KAAAwQ,WAIAnI,MAAAW,SAAAlD,GACA9F,KAAAuE,gBC1EO,UAAAuB,EAAA1F,GACP,MAAAsQ,EAAA,IAAAC,IAAAC,EAAAxQ,GAAAkD,IAAAuN,KAAAlQ,OACAmQ,EAAAhL,EAAAgL,SAAA7P,IAAAb,EAAAO,OAAA,GACAoQ,EAAAjL,EAAAkL,SAAA/P,IAAAb,EAAAO,OAAA,GACA,UAAAsQ,IAAA,IAAAH,KAAAC,GAAA,CACA,MAAAG,EAAAN,EAAAK,SACAE,EAAAD,EAAAR,EAAAO,EAAA7Q,GACA,UAAAgR,KAAAF,EACAR,EAAAW,IAAAD,EAAAzQ,MAIA,SAAAwQ,EAAAD,EAAAI,EAAAL,EAAA9J,GACA,UAAAiK,KAAAF,EAAA,CACA,MAAAvQ,KAAaA,GAAOyQ,EACpB,GAAAzQ,GAAA2Q,EAAAjL,IAAA1F,GAAA,CACA,MAAA6B,oBAA0C7B,uDAA0DwG,EAAAxG,6CACtFkE,EAAeuM,EAAAzM,OAAAhE,KAAAsQ,EAAA,oBAAAzO,KAK7B,SAAAoO,EAAAxQ,GACA,OAAAA,EAAAoP,QACA7G,OAAA,EAAgBjF,UAAK,cAAAA,IDmDR6N,CAA+BzL,EAAA9F,QAK5C,SAAAyQ,EAAAe,EAAAC,GACA,WACA,MAAAC,EAAwBlF,EAAkBgF,EAAAlJ,SAAA3D,OAAA0C,KAAA9D,QAC1CoO,EAAAH,EAAAhC,QAAA7L,OACM6I,EduIC,SAAAoF,EAAAC,GACP,GAAAA,EAAAvJ,SAAA3E,OACA,OAAAkO,EAAAvJ,SAAA3D,OAAA0C,KAEA,iBAAAwK,EAAAnO,OAAAmO,EAAAxG,QACA,OAAAuG,EAAAC,EAAAhM,SAGA,OADA/E,OAAAgO,OAAA+C,EAAAlN,QAAAmN,KAAA,CAAAC,EAAAC,IAAAD,EAAA/M,MAAAgN,EAAAhN,OACA,Gc/IyB4M,CAAaJ,EAAAhC,QAAA,IAAAjM,QdqG/B,SAAA0O,GACP,MAAAP,EAAAlF,EAAAyF,GACAC,EAAAR,EAAAlG,SAAA,gBACA,OAAAkG,EAAAQ,EcvGMC,CAAoBT,GAC1BU,EAA0BjC,wBAAW7F,MAAA,IAAWa,yBAAewG,oBAC/DS,EAAA9J,SAAA,GACA8J,EAAA5H,UAAAiH,EAAAjH,UAEA,MAAA6H,EAAAb,EAAAhC,QAAA8C,UAAA9R,GAAA,gBAAAA,EAAAkD,MACA8N,EAAAhC,QAAA+C,OAAAF,EAAA,IAAAD,GAEA,MAAAtK,MAAWA,GAAQ0J,EAAA7M,OACnBmD,EAAAvE,OAAAiI,SAAA,QACA1D,EAAAvE,aAA2BmO,KAG3B,MAAApJ,SAAWA,GAAWkJ,EACtBxM,EAAAsD,EAAAkK,QAAAf,GACAgB,EAAAnK,EAAAiK,OAAAvN,EAAA,GACAsD,EAAA3E,OAEK2E,EAAA3E,SAAAqB,EACLsD,EAAAtD,EAAA,GAAAL,OAAAiD,eAAAvC,EACKiD,EAAAtD,GAAAL,OAAAhE,KAAA4C,OAAAmP,SACLpK,EAAAtD,GAAAL,OAAAhE,KAAA4C,OAAAkP,EAAA,GAAA9N,OAAAhE,KAAA4C,QAJA+E,EAAA3D,OAAA0C,KAAAiB,EAAA3D,OAAAmD,WAAAzC,GElGO,MAAMsN,oBAAcxD,oBAI3BjK,aAAAyB,EAAAQ,GAAA5C,QAAiCA,GAAU,IAC3C,MAAAI,EAAA,CAAoBJ,UAAA4C,QAEpB,GADAxC,EAAAqK,MAAArI,EAAAE,QAAA,SACAlC,EAAAqK,MAGA,OAAWG,oBAAS7E,MAAA3D,EAAA,IAAsBgM,YAAK,CAAEtQ,OAAAsE,EAAAtE,OAAAsC,WAAmC,CACpFjB,KAAA,kBACA4L,eAAA,CACA,CAASO,kBAAQvF,OACjB,CAASiC,GACT,CAASb,oBAASpB,MAAA,CAASqB,WAAA,IAC3B,CAASP,oBAASd,MAAA,CAASgB,SAAA,OAK3B5H,WACA,yBCvBO,MAAMkP,oBAAc3N,KAI3BC,aAAAyB,GACA,MAAAhC,EAAA,GACAuC,EAAA,IAAoB0L,YAAK,CAAEvQ,OAAAsE,EAAAtE,OAAAsC,WAQ3B,OAPAuC,EAAAoB,SAAmBoC,uCAAkBJ,MAAA3D,GACrChC,EAAAkO,SAAAlM,EAAAE,QAAA,YACAK,EAAArB,QAAkB2B,EAA6Bb,EAAA,oBAAAA,EAAAI,MAAA,kCAC/CpC,EAAAhE,KAAAgG,EAAAE,QAAA,eAAAF,EAAAI,MAAA,kCACAG,EAAA+D,QAAkBhC,gBAAOqB,MAAA3D,GACzBhC,EAAAkO,UAAA3L,EAAA+D,SAAAtE,EAAAI,MAAA,2CACApC,EAAA4G,YAAA5E,EAAAE,QAAA,MAA6CF,EAAAI,MAAA,gDAC7CG,EAGAxD,WACA,cAEA/C,WACA,OAAW6H,EAAQxI,KAAA2E,OAAAhE,KAAAU,OAEnBwR,eACA,QAAA7S,KAAA2E,OAAAkO,SAGA3N,UAAAY,SACA9F,KAAA6F,QAAAmD,SAAAlD,IC9BO,MAAMgN,8BAAmB3D,oBAIhCjK,aAAAyB,GAAApC,QAA2BA,GAAU,IACrC,MAAAI,EAAA,CAAoBJ,WAEpB,GADAI,EAAAwC,KAAAR,EAAAE,QAAA,cACAlC,EAAAwC,KAGA,OAAWgI,oBAAS7E,MAAA3D,EAAA,IAAsBmM,sBAAU,CAAEzQ,OAAAsE,EAAAtE,OAAAsC,WAAmC,CACzFjB,KAAA,aACA2L,aAAA9K,EACA+K,eAAA,CACA,CAASsD,YAAKtI,UAKd5G,WACA,oBCjBO,MAAMqP,4BAAkB5D,oBAI/BjK,aAAAyB,GAAApC,QAA2BA,GAAU,IACrC,MAAAI,EAAA,CAAoBJ,WAEpB,GADAI,EAAAwC,KAAAR,EAAAE,QAAA,aACAlC,EAAAwC,KAGA,OAAWgI,oBAAS7E,MAAA3D,EAAA,IAAsBoM,oBAAS,CAAE1Q,OAAAsE,EAAAtE,OAAAsC,WAAmC,CACxFjB,KAAA,YACA4L,eAAA,CACA,CAAS5D,oBAASpB,MAAA,CAASqB,WAAA,EAAAC,UAAA,IAC3B,CAASR,oBAASd,MAAA,CAASgB,SAAA,OAK3B5H,WACA,kBAGAwB,UAAAY,GACA,IAAA9F,KAAAuE,SAAAvE,KAAAsI,SAAAgI,MAAA3F,GAAA,YAAAA,EAAAhK,MAAA,CACA,MAAA6B,EAAA,gTAKYqC,EAAe7E,KAAA2E,OAAAhE,KAAAX,KAAA,kBAAAwC,EAAA,CAC3BG,QAAiBiK,EAAuB5M,cAGxCqI,MAAAW,SAAAlD,ICnCO,MAAMkN,6CAA0B7D,oBAIvCjK,aAAAyB,EAAAsM,GAAA1O,QAAqCA,EAAA,MAAiB,IACtD,MAAAI,EAAA,CAAoBsO,YAEpB,GADAtO,EAAAwC,KAAAR,EAAAE,QAAA,aACAlC,EAAAwC,KAGA,OAAWgI,oBAAS7E,MAAA3D,EAAA,IAAsBqM,qCAAiB,CAAE3Q,OAAAsE,EAAAtE,OAAAsC,WAAmC,CAChGjB,KAAA,qBACA2L,aAAA9K,EACA+K,eAAA,CACA,CAASO,kBAAQvF,OACjB,CAASc,oBAASd,MAAA,CAASgB,SAAA,OAK3B5H,WACA,4BCPA,SAAAwP,EAAAvM,EAAA5B,GACA,MAAA1C,EAAAsE,EAAAtE,OAEA,SAAA0E,EAAA0F,GACA9F,EAAAI,MAAA0F,GAGA,SAAA5F,KAAA8H,GACA,OAAAhI,EAAAE,WAAA8H,GAYA,SAAAwE,EAAAC,GACA,MAAAjM,EAAAN,EAAA,aACA,GAAAM,EAIA,OAHgBwL,YAAKrI,MAAA3D,EAAAQ,EAAAiM,IACf/C,oBAAS/F,MAAA3D,EAAAQ,EAAAiM,IACfrM,EAAA,gCAaA,SAAAsM,IACA,OA5BA,WACA,MAAAJ,EAAApM,EAAA,YACA,GAAAoM,EACA,OAAAtM,EAAAG,MAAA,aACakM,qCAAiB1I,MAAA3D,EAAAsM,GAEnB/D,0BAAgB5E,MAAA3D,EAAAsM,GAsB3BA,IACAE,KAXA,WACA,MAAA5O,EAAAsC,EAAA,WACA,GAAAtC,EACA,OAAWuO,sBAAUxI,MAAA3D,EAAA,CAAmBpC,aACxC4O,EAAA,CAAkB5O,aACZwO,oBAASzI,MAAA3D,EAAA,CAAmBpC,aAClCwC,EAAA,qCAMAxC,IACMuO,sBAAUxI,MAAA3D,IACVkI,UAAIvE,MAAA3D,IACJsI,gBAAO3E,MAAA3D,IACPoI,kBAAQzE,MAAA3D,IACRoM,oBAASzI,MAAA3D,GAsBf,MAAA2M,EAnBA,WACA,IAAAjR,EAAAsB,OAAA,SACA,MAAAmC,EAAA,GACA,QACA,MAAA2J,EAAiB/E,uCAAkBJ,MAAA3D,GACnCV,EAAAoN,IACA,IAAApN,EAAA,CACAwJ,EAAA9L,QAAAoD,EAAA,6BACA,MAEAd,EAAAqC,SAAAmH,EACA3J,EAAAyB,KAAAtB,GAEA,MAAAsN,EAAA1M,EAAA,OAIA,OAHA9B,EAAAyO,UACA1N,EAAAyB,KAAAgM,GAEAzN,EAEA2N,GAEA,OADA9M,EAAArE,SAAAD,EAAAsB,QAAAoD,EAAA,uBACAuM,EAGO,SAAAhJ,EAAAmC,EAAA1H,EAAA,IACP,MAAA4B,EAAA,IAAwBwE,oBAASsB,GAIjC,YAHA,IAAA1H,EAAAN,aACAkC,EAAAtE,OAAA1B,KAAAoE,EAAAN,YAEAyO,EAAAvM,EAAA5B,GC/FA,SAAA2O,EAAAjJ,GACA,OAAAA,EAGA,MAAAkJ,EAAA,CACAC,KAAA3H,KAAAzI,KAAA,IACAD,OAAAmQ,EACA/S,KAAA+S,EACA5K,UAAA4K,EACAhQ,KAAAgQ,EACAzL,QAAAyL,EACAG,SAAAH,EACApO,YAAAoO,EACAL,WAAAK,EACAI,kBAAAJ,EACAK,2BAAAL,GAGO,SAAAM,EAAAC,GAAqBN,UAAAO,EAAAP,GAA4B,IAGxD,SAAA7K,EAAAqL,GAAAC,UAA2BA,EAAA9P,YAI3B,OAHA8P,IACAA,EAAAD,EAAArI,WAAA,KAAAqI,EAAApR,MAAA,GAAAoR,GAEAD,EAAApL,UAAAqL,EAAAC,EAAA9P,GAGA,SAAAQ,EAAAxD,EAAA+S,EAAAX,KAAA/D,GACA,IAAArO,EACA,SAEA,MAAAD,EAAAgT,EAAA/S,EAAAD,SAAAsO,GACA,OAAAuE,EAAAN,KAAA,CAAAM,EAAA3Q,OAAAjC,EAAAiC,QAAAlC,IAGA,SAAAiT,EAAAhT,EAAAgD,GACA,OAAAQ,EAAAxD,EAAAwH,EAAA,CAAgCxE,YAGhC,SAAAiQ,EAAAjT,EAAAmJ,GACA,OAAA3F,EAAAxD,EAAA4S,EAAAvT,KAAA8J,GAGA,SAAA+J,EAAAC,GACA,GAAAA,EAAAzO,OAAAyO,EAAAxM,QACA,OAAAiM,EAAAN,KAAA,CACA9O,EAAA2P,EAAA9P,OAAAwC,KAAA+M,EAAAjM,SACAnD,EAAA2P,EAAA9P,OAAA0C,SACAoN,EAAAhO,QAAAnD,IAAAI,GACAoB,EAAA2P,EAAA9P,OAAAmD,SAGA,MAAA4M,EAAAD,EAAA9P,OAAA8D,QAAAgM,EAAA9P,OAAAwC,KACAsB,EAAAgM,EAAA9P,OAAA8D,OAAA,CACAgM,EAAA9P,OAAA8D,OAAApH,MACA6S,EAAA3Q,OAAAkR,EAAA9P,OAAAwC,KAAA5D,SACA,GACAoR,EAAA7L,EAAAoL,EAAAN,KAAA,IACAnL,EACAgM,EAAA9P,OAAAwC,KAAA9F,MACAyD,EAAA2P,EAAA9P,OAAA+D,WACA,CAAS0L,UAAAK,EAAA5O,QAAAvB,QAAAmQ,IACT,OAAAP,EAAAN,KAAA,CAAAM,EAAA3Q,OAAAmR,EAAAnR,QAAAoR,IAEA,SAAAjR,EAAA+Q,GACA,OAAAP,EAAAN,KAAA,CACAgB,EAAAH,EAAAnM,UACAkM,EAAAC,GACA3P,EAAA2P,EAAA9P,OAAA6B,UACA1B,EAAA2P,EAAA9P,OAAAiD,aAGA,SAAAiN,EAAA5O,GACA,OAAAA,EAGAiO,EAAAN,KAAA,CACA9O,EAAAmB,EAAAtB,OAAAuE,WACAjD,EAAAmD,WAAA9F,IAAAhC,GAAAwD,EAAAxD,MAJA,GAOA,SAAAmK,EAAAhB,GACA,OAAAyJ,EAAAN,KAAA,CACAgB,EAAAnK,EAAAnC,UACAxD,EAAA2F,EAAA9F,OAAAmG,UACAoJ,EAAAxQ,OAAA+G,EAAA5E,UACAf,EAAA2F,EAAA9F,OAAAoG,UACAwJ,EAAA9J,EAAA9F,OAAAhE,KAAA,CAAmCkR,KAAApH,IACnCoK,EAAApK,EAAAQ,SACAnG,EAAA2F,EAAA9F,OAAAiD,aASA,SAAAkN,EAAAL,GACA,MAAA7K,QAAWA,GAAU6K,EAAApK,OACrB,OAAA6J,EAAAN,KAAA,CACAM,EAAA3Q,OAAAkR,EAAA9P,OAAAhE,KAAA4C,QACA2Q,EAAAJ,kBAAAI,EAAAN,KAAA,CACAM,EAAAH,2BAAAU,EAAA9T,MACAmE,EAAA2P,EAAApK,OAAA1F,OAAAuE,QACAoL,EAAAG,EAAApK,OAAA1F,OAAA+E,cAAA+K,GACA3P,EAAA2P,EAAApK,OAAA1F,OAAA0C,SACAoN,EAAApK,OAAAV,KACA8K,EAAApK,OAAAV,KAAArG,IACA,oBAAAsG,EAAAmL,IAjBA,SAAAA,EAAAzQ,GACA,OAAA4P,EAAAN,KAAA,CACAU,EAAAS,EAAApQ,OAAAtD,MAAAiD,GACAQ,EAAAiQ,EAAApQ,OAAAiD,cAcAiE,CAAAkJ,EAAAN,GAAAhJ,GAFA,GAIA3G,EAAA2P,EAAApK,OAAA1F,OAAAmD,UAEAhD,EAAA2P,EAAA9P,OAAAiD,aAGA,SAAAgN,EAAAI,GACA,OAAAA,EAAArR,OACAuQ,EAAAN,KAAA,CACA9O,EAAAkQ,EAAArQ,OAAA0C,SACA2N,EAAA1R,IAAAwR,GACAhQ,EAAAkQ,EAAArQ,OAAAmD,SAJA,GA0DA,SAAAmN,EAAAR,GACA,OAAAP,EAAAb,WAAAa,EAAAN,KAAA,CACAgB,EAAAH,EAAAnM,UACAxD,EAAA2P,EAAA9P,OAAAsO,UACAnO,EAAA2P,EAAA9P,OAAAJ,SACAO,EAAA2P,EAAA9P,OAAAwC,MACArC,EAAA2P,EAAA9P,OAAAqK,OACAuF,EAAAE,EAAA9P,OAAAhE,KAAA,CAAkCkR,KAAA4C,KAlBlCS,EAmBAT,EAlBAS,EAAAvQ,OAAAW,YAGA4O,EAAAN,KAAA,CACA9O,EAAAoQ,EAAAvQ,OAAA4K,OACA2E,EAAA3Q,OAAA2R,EAAAvQ,OAAAW,YAAA/B,QACA2Q,EAAA5O,YAAAwD,EAAAoM,EAAAvQ,OAAAW,YAAAjE,MAAA,CAA8DiD,QAAA4Q,OAL9D,IAkBApQ,EAAA2P,EAAA9P,OAAA0C,MACA8N,EAAAV,EAAAjF,QAAAiF,GACA3P,EAAA2P,EAAA9P,OAAAmD,OACAhD,EAAA2P,EAAA9P,OAAA4G,eACA,CAASsG,KAAA4C,IAxBT,IAAAS,EAoGA,SAAAE,EAAAX,EAAAY,GACA,OAAAnB,EAAAb,WAAAa,EAAAN,KAAA,CACAgB,EAAAH,EAAAnM,UACAxD,EAAA2P,EAAA9P,OAAAiH,UACA9G,EAAA2P,EAAA9P,OAAAqL,OACAlL,EAAA2P,EAAA9P,OAAAwC,KAAA+M,EAAAjM,SACAnD,EAAA2P,EAAA9P,OAAA0C,MACA6M,EAAAN,KAAAa,EAAA5O,QAAAvC,IAAAI,IACAoB,EAAA2P,EAAA9P,OAAAmD,OACAhD,EAAA2P,EAAA9P,OAAA4G,eACA,CAASsG,KAAA4C,EAAAY,WAhQTnB,EAAApT,OAAAoI,OAAA,GAAuByK,EAAAO,GAsQvB,MAAAoB,EAAA,CACAC,UAAAN,EACAO,kBAAAP,EACAQ,UAAAR,EACAS,UA/JA,SAAAjB,EAAAY,GACA,MAAAM,EAAAlB,EAAA5O,QAAA,CACAqO,EAAAxQ,OAAA+Q,EAAA5O,UACA0O,EAAAE,EAAA9P,OAAAhE,KAAA,CAAkCkR,KAAA4C,EAAAY,WAClCvQ,EAAA2P,EAAA9P,OAAA0C,MACA6M,EAAAN,KAAAa,EAAAjK,UAAAlH,IAAAmI,IACA3G,EAAA2P,EAAA9P,OAAAmD,QACA,GACA,OAAAoM,EAAAb,WAAAa,EAAAN,KAAA,CACAgB,EAAAH,EAAAnM,UACAmM,EAAA9P,OAAAhE,KAAAmE,EAAA2P,EAAA9P,OAAA0G,SAAAvG,EAAA2P,EAAA9P,OAAA0G,QAAA6I,EAAAL,SAAA,CAAyFhC,KAAA4C,EAAAY,cACzFM,EACA7Q,EAAA2P,EAAA9P,OAAA4G,eACA,CAASsG,KAAA4C,EAAAY,YAmJTO,UAhJA,SAAAnB,EAAAY,GACA,OAAAnB,EAAAb,WAAAa,EAAAN,KAAA,CACAgB,EAAAH,EAAAnM,UACAxD,EAAA2P,EAAA9P,OAAA0G,SACAvG,EAAA2P,EAAA9P,OAAAiH,UACA9G,EAAA2P,EAAA9P,OAAAwC,MACA+M,EAAAxQ,OAAA+Q,EAAA5O,UACA0O,EAAAE,EAAA9P,OAAAhE,KAAA,CAAkCkR,KAAA4C,EAAAY,WAClCvQ,EAAA2P,EAAA9P,OAAA4G,eACA,CAASsG,KAAA4C,EAAAY,YAwIT7E,YArIA,SAAAiE,EAAAY,GACA,OAAAnB,EAAAb,WAAAa,EAAAN,KAAA,CACAgB,EAAAH,EAAAnM,UACAxD,EAAA2P,EAAA9P,OAAAwC,KAAA+M,EAAAL,SAAA,CAA0ChC,KAAA4C,EAAAY,WAC1CvQ,EAAA2P,EAAA9P,OAAA0C,MACA6M,EAAAN,KAAAa,EAAAjK,UAAAlH,IAAAmI,IACA3G,EAAA2P,EAAA9P,OAAAmD,OACAhD,EAAA2P,EAAA9P,OAAA4G,eACA,CAASsG,KAAA4C,EAAAY,YA8HTQ,WAAAZ,EACAa,MAjGA,SAAArB,EAAAY,GACA,OAAAnB,EAAAb,WAAAa,EAAAN,KAAA,CACAgB,EAAAH,EAAAnM,UACAxD,EAAA2P,EAAA9P,OAAAkO,UACAqB,EAAAxQ,OAAA+Q,EAAA5O,UACA0O,EAAAE,EAAA9P,OAAAhE,KAAA,CAAkCkR,KAAA4C,EAAAY,WAClCR,EAAAJ,EAAAxJ,SACAnG,EAAA2P,EAAA9P,OAAA4G,eACA,CAASsG,KAAA4C,EAAAY,YA0FTU,MAxFA,SAAAtB,EAAAY,GACA,OAAAnB,EAAAb,WAAAa,EAAAN,KAAA,CACAgB,EAAAH,EAAAnM,UACAxD,EAAA2P,EAAA9P,OAAAwC,MACA+M,EAAAxQ,OAAA+Q,EAAA5O,UACA0O,EAAAE,EAAA9P,OAAAhE,KAAA,CAAkCkR,KAAA4C,EAAAY,WAClCvQ,EAAA2P,EAAA9P,OAAAuE,QACApE,EAAA2P,EAAA9P,OAAAtD,OACAyD,EAAA2P,EAAA9P,OAAA4G,eACA,CAASsG,KAAA4C,EAAAY,YAgFTzM,QA9EA,SAAA6L,GACA,OAAAP,EAAAb,WAAAa,EAAAN,KAAA,CACAgB,EAAAH,EAAAnM,UACAxD,EAAA2P,EAAA9P,OAAAwC,MACA+M,EAAAxQ,OAAA+Q,EAAA5O,UACA0O,EAAAE,EAAA9P,OAAAhE,KAAA,CAAkCkR,KAAA4C,IAClC3P,EAAA2P,EAAA9P,OAAA4G,eACA,CAASsG,KAAA4C,KAwETjJ,SAtEA,SAAAiJ,GACA,OAAAP,EAAAb,WAAAa,EAAAN,KAAA,CACAgB,EAAAH,EAAAnM,UACAgM,EAAAG,EAAA9P,OAAAkE,OAAA4L,GACA3P,EAAA2P,EAAA9P,OAAA6G,UACA8I,EAAAG,EAAA9P,OAAAqK,MAAAyF,GACA3P,EAAA2P,EAAA9P,OAAA4G,eACA,CAASsG,KAAA4C,KAgETxB,SA9DA,SAAAwB,GACA,OAAAP,EAAAb,WAAAa,EAAAN,KAAA,CACAgB,EAAAH,EAAAnM,UACAxD,EAAA2P,EAAA9P,OAAAwC,MACAoN,EAAAE,EAAA9P,OAAAhE,KAAA,CAAkCkR,KAAA4C,IAClC3P,EAAA2P,EAAA9P,OAAAuE,QACAgL,EAAAxQ,OAAA+Q,EAAA5O,UACAf,EAAA2P,EAAA9P,OAAA0C,SACAoN,EAAAjK,UAAAlH,IAAAmI,GACA3G,EAAA2P,EAAA9P,OAAAmD,OACAhD,EAAA2P,EAAA9P,OAAA4G,eACA,CAASsG,KAAA4C,KAoDTuB,KAlDA,SAAAvB,GACA,OAAAP,EAAAb,WAAAa,EAAAN,KAAA,CACAgB,EAAAH,EAAAnM,UACAxD,EAAA2P,EAAA9P,OAAAwC,MACAoN,EAAAE,EAAA9P,OAAAhE,KAAA,CAAkCkR,KAAA4C,IAClC3P,EAAA2P,EAAA9P,OAAA0C,MACA8N,EAAAV,EAAA3F,OAAA2F,GACA3P,EAAA2P,EAAA9P,OAAAmD,OACAhD,EAAA2P,EAAA9P,OAAA4G,eACA,CAASsG,KAAA4C,KA0CTwB,aAxCA,SAAAC,EAAAb,GACA,OAAAnB,EAAAN,KAAA,CACAM,EAAA3Q,OAAA2S,EAAAvR,OAAAtD,MAAAkC,QACA2Q,EAAAb,WACAa,EAAAN,KAAA,KAAAM,EAAAvT,KAAAuV,EAAA7U,MAAA,CAAwCwQ,KAAAqE,EAAAb,WAAkB,MAC1D,CAASxD,KAAAqE,EAAAb,WAETvQ,EAAAoR,EAAAvR,OAAAiD,cAkCAuO,SAAAf,EACAgB,QAAAhB,EACAiB,QAAAjB,EACAkB,qBAAArB,EACA1B,IAvBA,SAAAkB,GACA,OAAAP,EAAA3Q,OAAAkR,EAAAlR,UA+BA,SAAA4R,EAAAoB,EAAAlB,GACA,IAAAkB,EAAA,OACA,MAAAC,EAAAD,EAAAjT,IAAAmT,IATA,SAAAhC,EAAAY,GAEA,IADAC,EAAAb,EAAA/Q,MAEA,UAAA6K,eAA+BkG,EAAA/Q,wBAE/B,OAAA4R,EAAAb,EAAA/Q,MAAA+Q,EAAAY,IAIAqB,CAAAD,EAAApB,IACA,OAAAnB,EAAAN,KAAA4C,GAEA,OAAArB,EAAAlB,GCxTA,SAAA0C,EAAAC,EAAA1Q,GACA,MAAA5C,EAAA,IAAAuT,IACArL,EAAAoL,EAAAjO,OAAA1C,GAAA,aAAAA,EAAAvC,MACA,UAAAoT,KAAAtL,EAAA,CACA,MAAAwD,EAAA9I,EAAAjF,IAAA6V,EAAAtL,UACA,IAAAwD,EACA,SAEA,MAAA+H,EAAAzT,EAAArC,IAAA6V,EAAAjO,QACAkO,EACAA,EAAAxP,KAAAyH,GAEA1L,EAAAgD,IAAAwQ,EAAAjO,OAAA,CAAAmG,IAGA,OAAA1L,EA8CA,SAAA0T,EAAA/C,GACA,MAAAnO,EA5CA,SAAA8Q,GACA,MAAA1Q,EAAA,IAAA2Q,IACAI,EAAA,IAAAtG,IACAG,EAAA,IAAA+F,IACA,UAAA5Q,KAAA2Q,EACA,GAAA3Q,EAAA1B,QAAA,CACA,MAAAwS,EAAAjG,EAAA7P,IAAAgF,EAAAtF,MACAoW,EACAA,EAAAxP,KAAAtB,GAEA6K,EAAAxK,IAAAL,EAAAtF,KAAA,CAAAsF,SAIAA,EAAAtF,OAGAuF,EAAAG,IAAAJ,EAAAtF,MAGAsW,EAAA5F,IAAApL,GAFAC,EAAAI,IAAAL,EAAAtF,KAAAsF,IAKA,OACA2Q,MACA1Q,SACA4K,WACAmG,aACAjG,SAAA2F,EAAAC,EAAA1Q,GACAE,MAAA,CACAD,0BAAA,IAAA+Q,UAcAC,CAAAlD,GACA,UAAAhO,KAAAH,EAAA8Q,IACA3Q,EAAA+C,iBACA/C,EAAA+C,SAAAlD,UAZA,WAAAI,OAAgCA,EAAA+Q,eAChC,UAAAG,KAAAH,EAAA,CACA,MAAAtW,KAAWA,GAAOyW,EAClB5U,eAAiC7B,eAAkBuF,EAAAjF,IAAAN,GAAA+C,+BACzCmB,EAAKuS,EAAAzS,OAAAhE,KAAAyW,EAAA,eAAA5U,IAWf6U,CAAAvR,GAcO,SAAAkD,EAAAiL,GACP,UAAA+C,GAXAD,EAWA9C,EAVA8C,EAAAO,KACAP,EAAAO,OAEA,GAAA7J,UAAAsJ,MAJA,IAAAA,EC5EA7W,EAAAQ,EAAA6W,EAAA,0BAAAjN,IAAApK,EAAAQ,EAAA6W,EAAA,0BAAAvD,IAAA9T,EAAAQ,EAAA6W,EAAA,6BAAAvO,IAAA9I,EAAAQ,EAAA6W,EAAA,qCAAAlJ","file":"webidl2.js","sourcesContent":["(function webpackUniversalModuleDefinition(root, factory) {\n\tif(typeof exports === 'object' && typeof module === 'object')\n\t\tmodule.exports = factory();\n\telse if(typeof define === 'function' && define.amd)\n\t\tdefine([], factory);\n\telse if(typeof exports === 'object')\n\t\texports[\"WebIDL2\"] = factory();\n\telse\n\t\troot[\"WebIDL2\"] = factory();\n})(this, function() {\nreturn "," \t// The module cache\n \tvar installedModules = {};\n\n \t// The require function\n \tfunction __webpack_require__(moduleId) {\n\n \t\t// Check if module is in cache\n \t\tif(installedModules[moduleId]) {\n \t\t\treturn installedModules[moduleId].exports;\n \t\t}\n \t\t// Create a new module (and put it into the cache)\n \t\tvar module = installedModules[moduleId] = {\n \t\t\ti: moduleId,\n \t\t\tl: false,\n \t\t\texports: {}\n \t\t};\n\n \t\t// Execute the module function\n \t\tmodules[moduleId].call(module.exports, module, module.exports, __webpack_require__);\n\n \t\t// Flag the module as loaded\n \t\tmodule.l = true;\n\n \t\t// Return the exports of the module\n \t\treturn module.exports;\n \t}\n\n\n \t// expose the modules object (__webpack_modules__)\n \t__webpack_require__.m = modules;\n\n \t// expose the module cache\n \t__webpack_require__.c = installedModules;\n\n \t// define getter function for harmony exports\n \t__webpack_require__.d = function(exports, name, getter) {\n \t\tif(!__webpack_require__.o(exports, name)) {\n \t\t\tObject.defineProperty(exports, name, { enumerable: true, get: getter });\n \t\t}\n \t};\n\n \t// define __esModule on exports\n \t__webpack_require__.r = function(exports) {\n \t\tif(typeof Symbol !== 'undefined' && Symbol.toStringTag) {\n \t\t\tObject.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });\n \t\t}\n \t\tObject.defineProperty(exports, '__esModule', { value: true });\n \t};\n\n \t// create a fake namespace object\n \t// mode & 1: value is a module id, require it\n \t// mode & 2: merge all properties of value into the ns\n \t// mode & 4: return value when already ns object\n \t// mode & 8|1: behave like require\n \t__webpack_require__.t = function(value, mode) {\n \t\tif(mode & 1) value = __webpack_require__(value);\n \t\tif(mode & 8) return value;\n \t\tif((mode & 4) && typeof value === 'object' && value && value.__esModule) return value;\n \t\tvar ns = Object.create(null);\n \t\t__webpack_require__.r(ns);\n \t\tObject.defineProperty(ns, 'default', { enumerable: true, value: value });\n \t\tif(mode & 2 && typeof value != 'string') for(var key in value) __webpack_require__.d(ns, key, function(key) { return value[key]; }.bind(null, key));\n \t\treturn ns;\n \t};\n\n \t// getDefaultExport function for compatibility with non-harmony modules\n \t__webpack_require__.n = function(module) {\n \t\tvar getter = module && module.__esModule ?\n \t\t\tfunction getDefault() { return module['default']; } :\n \t\t\tfunction getModuleExports() { return module; };\n \t\t__webpack_require__.d(getter, 'a', getter);\n \t\treturn getter;\n \t};\n\n \t// Object.prototype.hasOwnProperty.call\n \t__webpack_require__.o = function(object, property) { return Object.prototype.hasOwnProperty.call(object, property); };\n\n \t// __webpack_public_path__\n \t__webpack_require__.p = \"\";\n\n\n \t// Load entry module and return exports\n \treturn __webpack_require__(__webpack_require__.s = 0);\n","/**\n * @param {string} text\n */\nfunction lastLine(text) {\n  const splitted = text.split(\"\\n\");\n  return splitted[splitted.length - 1];\n}\n\n/**\n * @typedef {object} WebIDL2ErrorOptions\n * @property {\"error\" | \"warning\"} level\n * @property {Function} autofix\n *\n * @param {string} message error message\n * @param {\"Syntax\" | \"Validation\"} kind error type\n * @param {WebIDL2ErrorOptions} [options]\n */\nfunction error(source, position, current, message, kind, { level = \"error\", autofix, ruleName } = {}) {\n  /**\n   * @param {number} count\n   */\n  function sliceTokens(count) {\n    return count > 0 ?\n      source.slice(position, position + count) :\n      source.slice(Math.max(position + count, 0), position);\n  }\n\n  function tokensToText(inputs, { precedes } = {}) {\n    const text = inputs.map(t => t.trivia + t.value).join(\"\");\n    const nextToken = source[position];\n    if (nextToken.type === \"eof\") {\n      return text;\n    }\n    if (precedes) {\n      return text + nextToken.trivia;\n    }\n    return text.slice(nextToken.trivia.length);\n  }\n\n  const maxTokens = 5; // arbitrary but works well enough\n  const line =\n    source[position].type !== \"eof\" ? source[position].line :\n    source.length > 1 ? source[position - 1].line :\n    1;\n\n  const precedingLastLine = lastLine(\n    tokensToText(sliceTokens(-maxTokens), { precedes: true })\n  );\n\n  const subsequentTokens = sliceTokens(maxTokens);\n  const subsequentText = tokensToText(subsequentTokens);\n  const subsequentFirstLine = subsequentText.split(\"\\n\")[0];\n\n  const spaced = \" \".repeat(precedingLastLine.length) + \"^\";\n  const sourceContext = precedingLastLine + subsequentFirstLine + \"\\n\" + spaced;\n\n  const contextType = kind === \"Syntax\" ? \"since\" : \"inside\";\n  const inSourceName = source.name ? ` in ${source.name}` : \"\";\n  const grammaticalContext = (current && current.name) ? `, ${contextType} \\`${current.partial ? \"partial \" : \"\"}${current.type} ${current.name}\\`` : \"\";\n  const context = `${kind} error at line ${line}${inSourceName}${grammaticalContext}:\\n${sourceContext}`;\n  return {\n    message: `${context} ${message}`,\n    bareMessage: message,\n    context,\n    line,\n    sourceName: source.name,\n    level,\n    ruleName,\n    autofix,\n    input: subsequentText,\n    tokens: subsequentTokens\n  };\n}\n\n/**\n * @param {string} message error message\n */\nexport function syntaxError(source, position, current, message) {\n  return error(source, position, current, message, \"Syntax\");\n}\n\n/**\n * @param {string} message error message\n * @param {WebIDL2ErrorOptions} [options]\n */\nexport function validationError(token, current, ruleName, message, options = {}) {\n  options.ruleName = ruleName;\n  return error(current.source, token.index, current, message, \"Validation\", options);\n}\n","export class Base {\n  constructor({ source, tokens }) {\n    Object.defineProperties(this, {\n      source: { value: source },\n      tokens: { value: tokens }\n    });\n  }\n\n  toJSON() {\n    const json = { type: undefined, name: undefined, inheritance: undefined };\n    let proto = this;\n    while (proto !== Object.prototype) {\n      const descMap = Object.getOwnPropertyDescriptors(proto);\n      for (const [key, value] of Object.entries(descMap)) {\n        if (value.enumerable || value.get) {\n          json[key] = this[key];\n        }\n      }\n      proto = Object.getPrototypeOf(proto);\n    }\n    return json;\n  }\n}\n","/**\n * @param {*} idlType\n * @param {*[]} defs\n * @param {object} [options]\n * @param {boolean} [options.useNullableInner] use when the input idlType is nullable and you want to use its inner type\n * @return the type reference that ultimately includes dictionary.\n */\nexport function idlTypeIncludesDictionary(idlType, defs, { useNullableInner } = {}) {\n  if (!idlType.union) {\n    const def = defs.unique.get(idlType.idlType);\n    if (!def) {\n      return;\n    }\n    if (def.type === \"typedef\") {\n      const { typedefIncludesDictionary} = defs.cache;\n      if (typedefIncludesDictionary.has(def)) {\n        // Note that this also halts when it met indeterminate state\n        // to prevent infinite recursion\n        return typedefIncludesDictionary.get(def);\n      }\n      defs.cache.typedefIncludesDictionary.set(def, undefined); // indeterminate state\n      const result = idlTypeIncludesDictionary(def.idlType, defs);\n      defs.cache.typedefIncludesDictionary.set(def, result);\n      if (result) {\n        return idlType;\n      }\n    }\n    if (def.type === \"dictionary\" && (useNullableInner || !idlType.nullable)) {\n      return idlType;\n    }\n  }\n  for (const subtype of idlType.subtype) {\n    const result = idlTypeIncludesDictionary(subtype, defs);\n    if (result) {\n      if (subtype.union) {\n        return result;\n      }\n      return subtype;\n    }\n  }\n}\n","import { Base } from \"./base.js\";\nimport { unescape, type_with_extended_attributes, return_type, primitive_type } from \"./helpers.js\";\nimport { stringTypes } from \"../tokeniser.js\";\nimport { validationError } from \"../error.js\";\nimport { idlTypeIncludesDictionary } from \"../validators/helpers.js\";\n\n/**\n * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n * @param {string} typeName\n */\nfunction generic_type(tokeniser, typeName) {\n  const base = tokeniser.consume(\"FrozenArray\", \"Promise\", \"sequence\", \"record\");\n  if (!base) {\n    return;\n  }\n  const ret = new Type({ source: tokeniser.source, tokens: { base } });\n  ret.tokens.open = tokeniser.consume(\"<\") || tokeniser.error(`No opening bracket after ${base.type}`);\n  switch (base.type) {\n    case \"Promise\": {\n      if (tokeniser.probe(\"[\")) tokeniser.error(\"Promise type cannot have extended attribute\");\n      const subtype = return_type(tokeniser, typeName) || tokeniser.error(\"Missing Promise subtype\");\n      ret.subtype.push(subtype);\n      break;\n    }\n    case \"sequence\":\n    case \"FrozenArray\": {\n      const subtype = type_with_extended_attributes(tokeniser, typeName) || tokeniser.error(`Missing ${base.type} subtype`);\n      ret.subtype.push(subtype);\n      break;\n    }\n    case \"record\": {\n      if (tokeniser.probe(\"[\")) tokeniser.error(\"Record key cannot have extended attribute\");\n      const keyType = tokeniser.consume(...stringTypes) || tokeniser.error(`Record key must be one of: ${stringTypes.join(\", \")}`);\n      const keyIdlType = new Type({ source: tokeniser.source, tokens: { base: keyType }});\n      keyIdlType.tokens.separator = tokeniser.consume(\",\") || tokeniser.error(\"Missing comma after record key type\");\n      keyIdlType.type = typeName;\n      const valueType = type_with_extended_attributes(tokeniser, typeName) || tokeniser.error(\"Error parsing generic type record\");\n      ret.subtype.push(keyIdlType, valueType);\n      break;\n    }\n  }\n  if (!ret.idlType) tokeniser.error(`Error parsing generic type ${base.type}`);\n  ret.tokens.close = tokeniser.consume(\">\") || tokeniser.error(`Missing closing bracket after ${base.type}`);\n  return ret;\n}\n\n/**\n * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n */\nfunction type_suffix(tokeniser, obj) {\n  const nullable = tokeniser.consume(\"?\");\n  if (nullable) {\n    obj.tokens.nullable = nullable;\n  }\n  if (tokeniser.probe(\"?\")) tokeniser.error(\"Can't nullable more than once\");\n}\n\n/**\n * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n * @param {string} typeName\n */\nfunction single_type(tokeniser, typeName) {\n  let ret = generic_type(tokeniser, typeName) || primitive_type(tokeniser);\n  if (!ret) {\n    const base = tokeniser.consume(\"identifier\", ...stringTypes);\n    if (!base) {\n      return;\n    }\n    ret = new Type({ source: tokeniser.source, tokens: { base } });\n    if (tokeniser.probe(\"<\")) tokeniser.error(`Unsupported generic type ${base.value}`);\n  }\n  if (ret.generic === \"Promise\" && tokeniser.probe(\"?\")) {\n    tokeniser.error(\"Promise type cannot be nullable\");\n  }\n  ret.type = typeName || null;\n  type_suffix(tokeniser, ret);\n  if (ret.nullable && ret.idlType === \"any\") tokeniser.error(\"Type `any` cannot be made nullable\");\n  return ret;\n}\n\n/**\n * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n * @param {string} type\n */\nfunction union_type(tokeniser, type) {\n  const tokens = {};\n  tokens.open = tokeniser.consume(\"(\");\n  if (!tokens.open) return;\n  const ret = new Type({ source: tokeniser.source, tokens });\n  ret.type = type || null;\n  while (true) {\n    const typ = type_with_extended_attributes(tokeniser) || tokeniser.error(\"No type after open parenthesis or 'or' in union type\");\n    if (typ.idlType === \"any\") tokeniser.error(\"Type `any` cannot be included in a union type\");\n    ret.subtype.push(typ);\n    const or = tokeniser.consume(\"or\");\n    if (or) {\n      typ.tokens.separator = or;\n    }\n    else break;\n  }\n  if (ret.idlType.length < 2) {\n    tokeniser.error(\"At least two types are expected in a union type but found less\");\n  }\n  tokens.close = tokeniser.consume(\")\") || tokeniser.error(\"Unterminated union type\");\n  type_suffix(tokeniser, ret);\n  return ret;\n}\n\nexport class Type extends Base {\n  /**\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n   * @param {string} typeName\n   */\n  static parse(tokeniser, typeName) {\n    return single_type(tokeniser, typeName) || union_type(tokeniser, typeName);\n  }\n\n  constructor({ source, tokens }) {\n    super({ source, tokens });\n    Object.defineProperty(this, \"subtype\", { value: [] });\n    this.extAttrs = [];\n  }\n\n  get generic() {\n    if (this.subtype.length && this.tokens.base) {\n      return this.tokens.base.value;\n    }\n    return \"\";\n  }\n  get nullable() {\n    return Boolean(this.tokens.nullable);\n  }\n  get union() {\n    return Boolean(this.subtype.length) && !this.tokens.base;\n  }\n  get idlType() {\n    if (this.subtype.length) {\n      return this.subtype;\n    }\n    // Adding prefixes/postfixes for \"unrestricted float\", etc.\n    const name = [\n      this.tokens.prefix,\n      this.tokens.base,\n      this.tokens.postfix\n    ].filter(t => t).map(t => t.value).join(\" \");\n    return unescape(name);\n  }\n\n  *validate(defs) {\n    /*\n     * If a union is nullable, its subunions cannot include a dictionary\n     * If not, subunions may include dictionaries if each union is not nullable\n     */\n    const typedef = !this.union && defs.unique.get(this.idlType);\n    const target =\n      this.union ? this :\n      (typedef && typedef.type === \"typedef\") ? typedef.idlType :\n      undefined;\n    if (target && this.nullable) {\n      // do not allow any dictionary\n      const reference = idlTypeIncludesDictionary(target, defs);\n      if (reference) {\n        const targetToken = (this.union ? reference : this).tokens.base;\n        const message = `Nullable union cannot include a dictionary type`;\n        yield validationError(targetToken, this, \"no-nullable-union-dict\", message);\n      }\n    } else {\n      // allow some dictionary\n      for (const subtype of this.subtype) {\n        yield* subtype.validate(defs);\n      }\n    }\n  }\n}\n","import { Base } from \"./base.js\";\nimport { const_data, const_value } from \"./helpers.js\";\n\nexport class Default extends Base {\n  /**\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    const assign = tokeniser.consume(\"=\");\n    if (!assign) {\n      return null;\n    }\n    const def = const_value(tokeniser) || tokeniser.consume(\"string\", \"null\", \"[\", \"{\") || tokeniser.error(\"No value for default\");\n    const expression = [def];\n    if (def.type === \"[\") {\n      const close = tokeniser.consume(\"]\") || tokeniser.error(\"Default sequence value must be empty\");\n      expression.push(close);\n    } else if (def.type === \"{\") {\n      const close = tokeniser.consume(\"}\") || tokeniser.error(\"Default dictionary value must be empty\");\n      expression.push(close);\n    }\n    return new Default({ source: tokeniser.source, tokens: { assign }, expression });\n  }\n\n  constructor({ source, tokens, expression }) {\n    super({ source, tokens });\n    Object.defineProperty(this, \"expression\", { value: expression });\n  }\n\n  get type() {\n    return const_data(this.expression[0]).type;\n  }\n  get value() {\n    return const_data(this.expression[0]).value;\n  }\n  get negative() {\n    return const_data(this.expression[0]).negative;\n  }\n}\n","export class ArrayBase extends Array {\n  constructor({ source, tokens }) {\n    super();\n    Object.defineProperties(this, {\n      source: { value: source },\n      tokens: { value: tokens }\n    });\n  }\n}\n","import { Base } from \"./base.js\";\nimport { ArrayBase } from \"./array-base.js\";\nimport { list, identifiers, argument_list } from \"./helpers.js\";\nimport { validationError } from \"../error.js\";\n\nclass ExtendedAttributeParameters extends Base {\n  /**\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    const tokens = { assign: tokeniser.consume(\"=\") };\n    const ret = new ExtendedAttributeParameters({ source: tokeniser.source, tokens });\n    if (tokens.assign) {\n      tokens.secondaryName = tokeniser.consume(\"identifier\", \"decimal\", \"integer\", \"string\");\n    }\n    tokens.open = tokeniser.consume(\"(\");\n    if (tokens.open) {\n      ret.list = ret.rhsType === \"identifier-list\" ?\n        // [Exposed=(Window,Worker)]\n        identifiers(tokeniser) :\n        // [NamedConstructor=Audio(DOMString src)] or [Constructor(DOMString str)]\n        argument_list(tokeniser);\n      tokens.close = tokeniser.consume(\")\") || tokeniser.error(\"Unexpected token in extended attribute argument list\");\n    } else if (ret.hasRhs && !tokens.secondaryName) {\n      tokeniser.error(\"No right hand side to extended attribute assignment\");\n    }\n    return ret;\n  }\n\n  get rhsType() {\n    return !this.tokens.assign ? null :\n      !this.tokens.secondaryName ? \"identifier-list\" :\n        this.tokens.secondaryName.type;\n  }\n}\n\nexport class SimpleExtendedAttribute extends Base {\n  /**\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    const name = tokeniser.consume(\"identifier\");\n    if (name) {\n      return new SimpleExtendedAttribute({\n        source: tokeniser.source,\n        tokens: { name },\n        params: ExtendedAttributeParameters.parse(tokeniser)\n      });\n    }\n  }\n\n  constructor({ source, tokens, params }) {\n    super({ source, tokens });\n    Object.defineProperty(this, \"params\", { value: params });\n  }\n\n  get type() {\n    return \"extended-attribute\";\n  }\n  get name() {\n    return this.tokens.name.value;\n  }\n  get rhs() {\n    const { rhsType: type, tokens, list } = this.params;\n    if (!type) {\n      return null;\n    }\n    const value = type === \"identifier-list\" ? list : tokens.secondaryName.value;\n    return { type, value };\n  }\n  get arguments() {\n    const { rhsType, list } = this.params;\n    if (!list || rhsType === \"identifier-list\") {\n      return [];\n    }\n    return list;\n  }\n\n  *validate(defs) {\n    if (this.name === \"NoInterfaceObject\") {\n      const message = `\\`[NoInterfaceObject]\\` extended attribute is an \\\nundesirable feature that may be removed from Web IDL in the future. Refer to the \\\n[relevant upstream PR](https://github.com/heycam/webidl/pull/609) for more \\\ninformation.`;\n      yield validationError(this.tokens.name, this, \"no-nointerfaceobject\", message, { level: \"warning\" });\n    }\n    for (const arg of this.arguments) {\n      yield* arg.validate(defs);\n    }\n  }\n}\n\n// Note: we parse something simpler than the official syntax. It's all that ever\n// seems to be used\nexport class ExtendedAttributes extends ArrayBase {\n  /**\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    const tokens = {};\n    tokens.open = tokeniser.consume(\"[\");\n    if (!tokens.open) return new ExtendedAttributes({});\n    const ret = new ExtendedAttributes({ source: tokeniser.source, tokens });\n    ret.push(...list(tokeniser, {\n      parser: SimpleExtendedAttribute.parse,\n      listName: \"extended attribute\"\n    }));\n    tokens.close = tokeniser.consume(\"]\") || tokeniser.error(\"Unexpected closing token of extended attribute\");\n    if (!ret.length) {\n      tokeniser.error(\"Found an empty extended attribute\");\n    }\n    if (tokeniser.probe(\"[\")) {\n      tokeniser.error(\"Illegal double extended attribute lists, consider merging them\");\n    }\n    return ret;\n  }\n\n  *validate(defs) {\n    for (const extAttr of this) {\n      yield* extAttr.validate(defs);\n    }\n  }\n}\n","import { Type } from \"./type.js\";\nimport { Argument } from \"./argument.js\";\nimport { Token } from \"./token.js\";\nimport { ExtendedAttributes, SimpleExtendedAttribute } from \"./extended-attributes.js\";\nimport { Operation } from \"./operation.js\";\nimport { Attribute } from \"./attribute.js\";\nimport { Tokeniser } from \"../tokeniser.js\";\n\n/**\n * @param {string} identifier\n */\nexport function unescape(identifier) {\n  return identifier.startsWith('_') ? identifier.slice(1) : identifier;\n}\n\n/**\n * Parses comma-separated list\n * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n * @param {object} args\n * @param {Function} args.parser parser function for each item\n * @param {boolean} [args.allowDangler] whether to allow dangling comma\n * @param {string} [args.listName] the name to be shown on error messages\n */\nexport function list(tokeniser, { parser, allowDangler, listName = \"list\" }) {\n  const first = parser(tokeniser);\n  if (!first) {\n    return [];\n  }\n  first.tokens.separator = tokeniser.consume(\",\");\n  const items = [first];\n  while (first.tokens.separator) {\n    const item = parser(tokeniser);\n    if (!item) {\n      if (!allowDangler) {\n        tokeniser.error(`Trailing comma in ${listName}`);\n      }\n      break;\n    }\n    item.tokens.separator = tokeniser.consume(\",\");\n    items.push(item);\n    if (!item.tokens.separator) break;\n  }\n  return items;\n}\n\n/**\n * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n */\nexport function const_value(tokeniser) {\n  return tokeniser.consume(\"true\", \"false\", \"Infinity\", \"-Infinity\", \"NaN\", \"decimal\", \"integer\");\n}\n\n/**\n * @param {object} token\n * @param {string} token.type\n * @param {string} token.value\n */\nexport function const_data({ type, value }) {\n  switch (type) {\n    case \"true\":\n    case \"false\":\n      return { type: \"boolean\", value: type === \"true\" };\n    case \"Infinity\":\n    case \"-Infinity\":\n      return { type: \"Infinity\", negative: type.startsWith(\"-\") };\n    case \"[\":\n      return { type: \"sequence\", value: [] };\n    case \"{\":\n      return { type: \"dictionary\" };\n    case \"decimal\":\n    case \"integer\":\n      return { type: \"number\", value };\n    case \"string\":\n      return { type: \"string\", value: value.slice(1, -1) };\n    default:\n      return { type };\n  }\n}\n\n/**\n * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n */\nexport function primitive_type(tokeniser) {\n  function integer_type() {\n    const prefix = tokeniser.consume(\"unsigned\");\n    const base = tokeniser.consume(\"short\", \"long\");\n    if (base) {\n      const postfix = tokeniser.consume(\"long\");\n      return new Type({ source, tokens: { prefix, base, postfix } });\n    }\n    if (prefix) tokeniser.error(\"Failed to parse integer type\");\n  }\n\n  function decimal_type() {\n    const prefix = tokeniser.consume(\"unrestricted\");\n    const base = tokeniser.consume(\"float\", \"double\");\n    if (base) {\n      return new Type({ source, tokens: { prefix, base } });\n    }\n    if (prefix) tokeniser.error(\"Failed to parse float type\");\n  }\n\n  const { source } = tokeniser;\n  const num_type = integer_type(tokeniser) || decimal_type(tokeniser);\n  if (num_type) return num_type;\n  const base = tokeniser.consume(\"boolean\", \"byte\", \"octet\");\n  if (base) {\n    return new Type({ source, tokens: { base } });\n  }\n}\n\n/**\n * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n */\nexport function identifiers(tokeniser) {\n  const ids = list(tokeniser, { parser: Token.parser(tokeniser, \"identifier\"), listName: \"identifier list\" });\n  if (!ids.length) {\n    tokeniser.error(\"Expected identifiers but none found\");\n  }\n  return ids;\n}\n\n/**\n * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n */\nexport function argument_list(tokeniser) {\n  return list(tokeniser, { parser: Argument.parse, listName: \"arguments list\" });\n}\n\n/**\n * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n * @param {string} typeName\n */\nexport function type_with_extended_attributes(tokeniser, typeName) {\n  const extAttrs = ExtendedAttributes.parse(tokeniser);\n  const ret = Type.parse(tokeniser, typeName);\n  if (ret) ret.extAttrs = extAttrs;\n  return ret;\n}\n\n/**\n * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n * @param {string} typeName\n */\nexport function return_type(tokeniser, typeName) {\n  const typ = Type.parse(tokeniser, typeName || \"return-type\");\n  if (typ) {\n    return typ;\n  }\n  const voidToken = tokeniser.consume(\"void\");\n  if (voidToken) {\n    const ret = new Type({ source: tokeniser.source, tokens: { base: voidToken } });\n    ret.type = \"return-type\";\n    return ret;\n  }\n}\n\n/**\n * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n */\nexport function stringifier(tokeniser) {\n  const special = tokeniser.consume(\"stringifier\");\n  if (!special) return;\n  const member = Attribute.parse(tokeniser, { special }) ||\n    Operation.parse(tokeniser, { special }) ||\n    tokeniser.error(\"Unterminated stringifier\");\n  return member;\n}\n\n/**\n * @param {string} str\n */\nexport function getLastIndentation(str) {\n  const lines = str.split(\"\\n\");\n  // the first line visually binds to the preceding token\n  if (lines.length) {\n    const match = lines[lines.length - 1].match(/^\\s+/);\n    if (match) {\n      return match[0];\n    }\n  }\n  return \"\";\n}\n\n/**\n * @param {string} parentTrivia\n */\nexport function getMemberIndentation(parentTrivia) {\n  const indentation = getLastIndentation(parentTrivia);\n  const indentCh = indentation.includes(\"\\t\") ? \"\\t\" : \"  \";\n  return indentation + indentCh;\n}\n\n/**\n * @param {object} def\n * @param {import(\"./extended-attributes.js\").ExtendedAttributes} def.extAttrs\n */\nexport function autofixAddExposedWindow(def) {\n  return () => {\n    if (def.extAttrs.length){\n      const tokeniser = new Tokeniser(\"Exposed=Window,\");\n      const exposed = SimpleExtendedAttribute.parse(tokeniser);\n      exposed.tokens.separator = tokeniser.consume(\",\");\n      const existing = def.extAttrs[0];\n      if (!/^\\s/.test(existing.tokens.name.trivia)) {\n        existing.tokens.name.trivia = ` ${existing.tokens.name.trivia}`;\n      }\n      def.extAttrs.unshift(exposed);\n    } else {\n      def.extAttrs = ExtendedAttributes.parse(new Tokeniser(\"[Exposed=Window]\"));\n      const trivia = def.tokens.base.trivia;\n      def.extAttrs.tokens.open.trivia = trivia;\n      def.tokens.base.trivia = `\\n${getLastIndentation(trivia)}`;\n    }\n  };\n}\n\n/**\n * Get the first syntax token for the given IDL object.\n * @param {*} data\n */\nexport function getFirstToken(data) {\n  if (data.extAttrs.length) {\n    return data.extAttrs.tokens.open;\n  }\n  if (data.type === \"operation\" && !data.special) {\n    return getFirstToken(data.idlType);\n  }\n  const tokens = Object.values(data.tokens).sort((x, y) => x.index - y.index);\n  return tokens[0];\n}\n","import { Base } from \"./base.js\";\nimport { Default } from \"./default.js\";\nimport { ExtendedAttributes } from \"./extended-attributes.js\";\nimport { unescape, type_with_extended_attributes } from \"./helpers.js\";\nimport { argumentNameKeywords, Tokeniser } from \"../tokeniser.js\";\nimport { validationError } from \"../error.js\";\nimport { idlTypeIncludesDictionary } from \"../validators/helpers.js\";\n\nexport class Argument extends Base {\n  /**\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    const start_position = tokeniser.position;\n    const tokens = {};\n    const ret = new Argument({ source: tokeniser.source, tokens });\n    ret.extAttrs = ExtendedAttributes.parse(tokeniser);\n    tokens.optional = tokeniser.consume(\"optional\");\n    ret.idlType = type_with_extended_attributes(tokeniser, \"argument-type\");\n    if (!ret.idlType) {\n      return tokeniser.unconsume(start_position);\n    }\n    if (!tokens.optional) {\n      tokens.variadic = tokeniser.consume(\"...\");\n    }\n    tokens.name = tokeniser.consume(\"identifier\", ...argumentNameKeywords);\n    if (!tokens.name) {\n      return tokeniser.unconsume(start_position);\n    }\n    ret.default = tokens.optional ? Default.parse(tokeniser) : null;\n    return ret;\n  }\n\n  get type() {\n    return \"argument\";\n  }\n  get optional() {\n    return !!this.tokens.optional;\n  }\n  get variadic() {\n    return !!this.tokens.variadic;\n  }\n  get name() {\n    return unescape(this.tokens.name.value);\n  }\n\n  *validate(defs) {\n    yield* this.idlType.validate(defs);\n    if (idlTypeIncludesDictionary(this.idlType, defs, { useNullableInner: true })) {\n      if (this.idlType.nullable) {\n        const message = `Dictionary arguments cannot be nullable.`;\n        yield validationError(this.tokens.name, this, \"no-nullable-dict-arg\", message);\n      } else if (this.optional && !this.default) {\n        const message = `Optional dictionary arguments must have a default value of \\`{}\\`.`;\n        yield validationError(this.tokens.name, this, \"dict-arg-default\", message, {\n          autofix: autofixOptionalDictionaryDefaultValue(this)\n        });\n      }\n    }\n  }\n}\n\n/**\n * @param {Argument} arg\n */\nfunction autofixOptionalDictionaryDefaultValue(arg) {\n  return () => {\n    arg.default = Default.parse(new Tokeniser(\" = {}\"));\n  };\n}\n","import { Base } from \"./base.js\";\n\nexport class Token extends Base {\n  /**\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n   * @param {string} type\n   */\n  static parser(tokeniser, type) {\n    return () => {\n      const value = tokeniser.consume(type);\n      if (value) {\n        return new Token({ source: tokeniser.source, tokens: { value } });\n      }\n    };\n  }\n\n  get value() {\n    return this.tokens.value.value;\n  }\n}\n","import { Base } from \"./base.js\";\nimport { return_type, argument_list, unescape } from \"./helpers.js\";\nimport { validationError } from \"../error.js\";\n\nexport class Operation extends Base {\n  /**\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser, { special, regular } = {}) {\n    const tokens = { special };\n    const ret = new Operation({ source: tokeniser.source, tokens });\n    if (special && special.value === \"stringifier\") {\n      tokens.termination = tokeniser.consume(\";\");\n      if (tokens.termination) {\n        ret.arguments = [];\n        return ret;\n      }\n    }\n    if (!special && !regular) {\n      tokens.special = tokeniser.consume(\"getter\", \"setter\", \"deleter\");\n    }\n    ret.idlType = return_type(tokeniser) || tokeniser.error(\"Missing return type\");\n    tokens.name = tokeniser.consume(\"identifier\", \"includes\");\n    tokens.open = tokeniser.consume(\"(\") || tokeniser.error(\"Invalid operation\");\n    ret.arguments = argument_list(tokeniser);\n    tokens.close = tokeniser.consume(\")\") || tokeniser.error(\"Unterminated operation\");\n    tokens.termination = tokeniser.consume(\";\") || tokeniser.error(\"Unterminated operation, expected `;`\");\n    return ret;\n  }\n\n  get type() {\n    return \"operation\";\n  }\n  get name() {\n    const { name } = this.tokens;\n    if (!name) {\n      return \"\";\n    }\n    return unescape(name.value);\n  }\n  get special() {\n    if (!this.tokens.special) {\n      return \"\";\n    }\n    return this.tokens.special.value;\n  }\n\n  *validate(defs) {\n    if (!this.name && [\"\", \"static\"].includes(this.special)) {\n      const message = `Regular or static operations must have both a return type and an identifier.`;\n      yield validationError(this.tokens.open, this, \"incomplete-op\", message);\n    }\n    if (this.idlType) {\n      yield* this.idlType.validate(defs);\n    }\n    for (const argument of this.arguments) {\n      yield* argument.validate(defs);\n    }\n  }\n}\n","import { Base } from \"./base.js\";\nimport { type_with_extended_attributes, unescape } from \"./helpers.js\";\n\nexport class Attribute extends Base {\n  /**\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser, { special, noInherit = false, readonly = false } = {}) {\n    const start_position = tokeniser.position;\n    const tokens = { special };\n    const ret = new Attribute({ source: tokeniser.source, tokens });\n    if (!special && !noInherit) {\n      tokens.special = tokeniser.consume(\"inherit\");\n    }\n    if (ret.special === \"inherit\" && tokeniser.probe(\"readonly\")) {\n      tokeniser.error(\"Inherited attributes cannot be read-only\");\n    }\n    tokens.readonly = tokeniser.consume(\"readonly\");\n    if (readonly && !tokens.readonly && tokeniser.probe(\"attribute\")) {\n      tokeniser.error(\"Attributes must be readonly in this context\");\n    }\n    tokens.base = tokeniser.consume(\"attribute\");\n    if (!tokens.base) {\n      tokeniser.unconsume(start_position);\n      return;\n    }\n    ret.idlType = type_with_extended_attributes(tokeniser, \"attribute-type\") || tokeniser.error(\"Attribute lacks a type\");\n    switch (ret.idlType.generic) {\n      case \"sequence\":\n      case \"record\": tokeniser.error(`Attributes cannot accept ${ret.idlType.generic} types`);\n    }\n    tokens.name = tokeniser.consume(\"identifier\", \"async\", \"required\") || tokeniser.error(\"Attribute lacks a name\");\n    tokens.termination = tokeniser.consume(\";\") || tokeniser.error(\"Unterminated attribute, expected `;`\");\n    return ret;\n  }\n\n  get type() {\n    return \"attribute\";\n  }\n  get special() {\n    if (!this.tokens.special) {\n      return \"\";\n    }\n    return this.tokens.special.value;\n  }\n  get readonly() {\n    return !!this.tokens.readonly;\n  }\n  get name() {\n    return unescape(this.tokens.name.value);\n  }\n\n  *validate(defs) {\n    yield* this.idlType.validate(defs);\n  }\n}\n","import { syntaxError } from \"./error.js\";\nimport { unescape } from \"./productions/helpers.js\";\n\n// These regular expressions use the sticky flag so they will only match at\n// the current location (ie. the offset of lastIndex).\nconst tokenRe = {\n  // This expression uses a lookahead assertion to catch false matches\n  // against integers early.\n  \"decimal\": /-?(?=[0-9]*\\.|[0-9]+[eE])(([0-9]+\\.[0-9]*|[0-9]*\\.[0-9]+)([Ee][-+]?[0-9]+)?|[0-9]+[Ee][-+]?[0-9]+)/y,\n  \"integer\": /-?(0([Xx][0-9A-Fa-f]+|[0-7]*)|[1-9][0-9]*)/y,\n  \"identifier\": /[_-]?[A-Za-z][0-9A-Z_a-z-]*/y,\n  \"string\": /\"[^\"]*\"/y,\n  \"whitespace\": /[\\t\\n\\r ]+/y,\n  \"comment\": /((\\/(\\/.*|\\*([^*]|\\*[^/])*\\*\\/)[\\t\\n\\r ]*)+)/y,\n  \"other\": /[^\\t\\n\\r 0-9A-Za-z]/y\n};\n\nexport const stringTypes = [\n  \"ByteString\",\n  \"DOMString\",\n  \"USVString\"\n];\n\nexport const argumentNameKeywords = [\n  \"async\",\n  \"attribute\",\n  \"callback\",\n  \"const\",\n  \"constructor\",\n  \"deleter\",\n  \"dictionary\",\n  \"enum\",\n  \"getter\",\n  \"includes\",\n  \"inherit\",\n  \"interface\",\n  \"iterable\",\n  \"maplike\",\n  \"namespace\",\n  \"partial\",\n  \"required\",\n  \"setlike\",\n  \"setter\",\n  \"static\",\n  \"stringifier\",\n  \"typedef\",\n  \"unrestricted\"\n];\n\nconst nonRegexTerminals = [\n  \"-Infinity\",\n  \"FrozenArray\",\n  \"Infinity\",\n  \"NaN\",\n  \"Promise\",\n  \"async\",\n  \"boolean\",\n  \"byte\",\n  \"constructor\",\n  \"double\",\n  \"false\",\n  \"float\",\n  \"long\",\n  \"mixin\",\n  \"null\",\n  \"octet\",\n  \"optional\",\n  \"or\",\n  \"readonly\",\n  \"record\",\n  \"sequence\",\n  \"short\",\n  \"true\",\n  \"unsigned\",\n  \"void\"\n].concat(argumentNameKeywords, stringTypes);\n\nconst punctuations = [\n  \"(\",\n  \")\",\n  \",\",\n  \"...\",\n  \":\",\n  \";\",\n  \"<\",\n  \"=\",\n  \">\",\n  \"?\",\n  \"[\",\n  \"]\",\n  \"{\",\n  \"}\"\n];\n\nconst reserved = [\n  // \"constructor\" is now a keyword\n  \"_constructor\",\n  \"toString\",\n  \"_toString\",\n];\n\n/**\n * @param {string} str\n */\nfunction tokenise(str) {\n  const tokens = [];\n  let lastCharIndex = 0;\n  let trivia = \"\";\n  let line = 1;\n  let index = 0;\n  while (lastCharIndex < str.length) {\n    const nextChar = str.charAt(lastCharIndex);\n    let result = -1;\n\n    if (/[\\t\\n\\r ]/.test(nextChar)) {\n      result = attemptTokenMatch(\"whitespace\", { noFlushTrivia: true });\n    } else if (nextChar === '/') {\n      result = attemptTokenMatch(\"comment\", { noFlushTrivia: true });\n    }\n\n    if (result !== -1) {\n      const currentTrivia = tokens.pop().value;\n      line += (currentTrivia.match(/\\n/g) || []).length;\n      trivia += currentTrivia;\n      index -= 1;\n    } else if (/[-0-9.A-Z_a-z]/.test(nextChar)) {\n      result = attemptTokenMatch(\"decimal\");\n      if (result === -1) {\n        result = attemptTokenMatch(\"integer\");\n      }\n      if (result === -1) {\n        result = attemptTokenMatch(\"identifier\");\n        const lastIndex = tokens.length - 1;\n        const token = tokens[lastIndex];\n        if (result !== -1) {\n          if (reserved.includes(token.value)) {\n            const message = `${unescape(token.value)} is a reserved identifier and must not be used.`;\n            throw new WebIDLParseError(syntaxError(tokens, lastIndex, null, message));\n          } else if (nonRegexTerminals.includes(token.value)) {\n            token.type = token.value;\n          }\n        }\n      }\n    } else if (nextChar === '\"') {\n      result = attemptTokenMatch(\"string\");\n    }\n\n    for (const punctuation of punctuations) {\n      if (str.startsWith(punctuation, lastCharIndex)) {\n        tokens.push({ type: punctuation, value: punctuation, trivia, line, index });\n        trivia = \"\";\n        lastCharIndex += punctuation.length;\n        result = lastCharIndex;\n        break;\n      }\n    }\n\n    // other as the last try\n    if (result === -1) {\n      result = attemptTokenMatch(\"other\");\n    }\n    if (result === -1) {\n      throw new Error(\"Token stream not progressing\");\n    }\n    lastCharIndex = result;\n    index += 1;\n  }\n\n  // remaining trivia as eof\n  tokens.push({\n    type: \"eof\",\n    value: \"\",\n    trivia\n  });\n\n  return tokens;\n\n  /**\n   * @param {keyof tokenRe} type\n   * @param {object} [options]\n   * @param {boolean} [options.noFlushTrivia]\n   */\n  function attemptTokenMatch(type, { noFlushTrivia } = {}) {\n    const re = tokenRe[type];\n    re.lastIndex = lastCharIndex;\n    const result = re.exec(str);\n    if (result) {\n      tokens.push({ type, value: result[0], trivia, line, index });\n      if (!noFlushTrivia) {\n        trivia = \"\";\n      }\n      return re.lastIndex;\n    }\n    return -1;\n  }\n}\n\nexport class Tokeniser {\n  /**\n   * @param {string} idl\n   */\n  constructor(idl) {\n    this.source = tokenise(idl);\n    this.position = 0;\n  }\n\n  /**\n   * @param {string} message\n   */\n  error(message) {\n    throw new WebIDLParseError(syntaxError(this.source, this.position, this.current, message));\n  }\n\n  /**\n   * @param {string} type\n   */\n  probe(type) {\n    return this.source.length > this.position && this.source[this.position].type === type;\n  }\n\n  /**\n   * @param  {...string} candidates\n   */\n  consume(...candidates) {\n    for (const type of candidates) {\n      if (!this.probe(type)) continue;\n      const token = this.source[this.position];\n      this.position++;\n      return token;\n    }\n  }\n\n  /**\n   * @param {number} position\n   */\n  unconsume(position) {\n    this.position = position;\n  }\n}\n\nexport class WebIDLParseError extends Error {\n  constructor({ message, bareMessage, context, line, sourceName, input, tokens }) {\n    super(message);\n\n    this.name = \"WebIDLParseError\"; // not to be mangled\n    this.bareMessage = bareMessage;\n    this.context = context;\n    this.line = line;\n    this.sourceName = sourceName;\n    this.input = input;\n    this.tokens = tokens;\n  }\n}\n","import { list, unescape } from \"./helpers.js\";\nimport { Token } from \"./token.js\";\nimport { Base } from \"./base.js\";\n\nclass EnumValue extends Token {\n  /**\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    const value = tokeniser.consume(\"string\");\n    if (value) {\n      return new EnumValue({ source: tokeniser.source, tokens: { value } });\n    }\n  }\n\n  get type() {\n    return \"enum-value\";\n  }\n  get value() {\n    return super.value.slice(1, -1);\n  }\n}\n\nexport class Enum extends Base {\n  /**\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    const tokens = {};\n    tokens.base = tokeniser.consume(\"enum\");\n    if (!tokens.base) {\n      return;\n    }\n    tokens.name = tokeniser.consume(\"identifier\") || tokeniser.error(\"No name for enum\");\n    const ret = tokeniser.current = new Enum({ source: tokeniser.source, tokens });\n    tokens.open = tokeniser.consume(\"{\") || tokeniser.error(\"Bodyless enum\");\n    ret.values = list(tokeniser, {\n      parser: EnumValue.parse,\n      allowDangler: true,\n      listName: \"enumeration\"\n    });\n    if (tokeniser.probe(\"string\")) {\n      tokeniser.error(\"No comma between enum values\");\n    }\n    tokens.close = tokeniser.consume(\"}\") || tokeniser.error(\"Unexpected value in enum\");\n    if (!ret.values.length) {\n      tokeniser.error(\"No value in enum\");\n    }\n    tokens.termination = tokeniser.consume(\";\") || tokeniser.error(\"No semicolon after enum\");\n    return ret;\n  }\n\n  get type() {\n    return \"enum\";\n  }\n  get name() {\n    return unescape(this.tokens.name.value);\n  }\n}\n","import { Base } from \"./base.js\";\nimport { unescape } from \"./helpers.js\";\n\nexport class Includes extends Base {\n  /**\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    const target = tokeniser.consume(\"identifier\");\n    if (!target) {\n      return;\n    }\n    const tokens = { target };\n    tokens.includes = tokeniser.consume(\"includes\");\n    if (!tokens.includes) {\n      tokeniser.unconsume(target.index);\n      return;\n    }\n    tokens.mixin = tokeniser.consume(\"identifier\") || tokeniser.error(\"Incomplete includes statement\");\n    tokens.termination = tokeniser.consume(\";\") || tokeniser.error(\"No terminating ; for includes statement\");\n    return new Includes({ source: tokeniser.source, tokens });\n  }\n\n  get type() {\n    return \"includes\";\n  }\n  get target() {\n    return unescape(this.tokens.target.value);\n  }\n  get includes() {\n    return unescape(this.tokens.mixin.value);\n  }\n}\n","import { Base } from \"./base.js\";\nimport { type_with_extended_attributes, unescape } from \"./helpers.js\";\n\nexport class Typedef extends Base {\n  /**\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    const tokens = {};\n    const ret = new Typedef({ source: tokeniser.source, tokens });\n    tokens.base = tokeniser.consume(\"typedef\");\n    if (!tokens.base) {\n      return;\n    }\n    ret.idlType = type_with_extended_attributes(tokeniser, \"typedef-type\") || tokeniser.error(\"Typedef lacks a type\");\n    tokens.name = tokeniser.consume(\"identifier\") || tokeniser.error(\"Typedef lacks a name\");\n    tokeniser.current = ret;\n    tokens.termination = tokeniser.consume(\";\") || tokeniser.error(\"Unterminated typedef, expected `;`\");\n    return ret;\n  }\n\n  get type() {\n    return \"typedef\";\n  }\n  get name() {\n    return unescape(this.tokens.name.value);\n  }\n\n  *validate(defs) {\n    yield* this.idlType.validate(defs);\n  }\n}\n","import { Base } from \"./base.js\";\nimport { return_type, argument_list, unescape } from \"./helpers.js\";\n\nexport class CallbackFunction extends Base {\n  /**\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser, base) {\n    const tokens = { base };\n    const ret = new CallbackFunction({ source: tokeniser.source, tokens });\n    tokens.name = tokeniser.consume(\"identifier\") || tokeniser.error(\"Callback lacks a name\");\n    tokeniser.current = ret;\n    tokens.assign = tokeniser.consume(\"=\") || tokeniser.error(\"Callback lacks an assignment\");\n    ret.idlType = return_type(tokeniser) || tokeniser.error(\"Callback lacks a return type\");\n    tokens.open = tokeniser.consume(\"(\") || tokeniser.error(\"Callback lacks parentheses for arguments\");\n    ret.arguments = argument_list(tokeniser);\n    tokens.close = tokeniser.consume(\")\") || tokeniser.error(\"Unterminated callback\");\n    tokens.termination = tokeniser.consume(\";\") || tokeniser.error(\"Unterminated callback, expected `;`\");\n    return ret;\n  }\n\n  get type() {\n    return \"callback\";\n  }\n  get name() {\n    return unescape(this.tokens.name.value);\n  }\n\n  *validate(defs) {\n    yield* this.idlType.validate(defs);\n  }\n}\n","import { Base } from \"./base.js\";\nimport { ExtendedAttributes } from \"./extended-attributes.js\";\nimport { unescape } from \"./helpers.js\";\n\n/**\n * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n */\nfunction inheritance(tokeniser) {\n  const colon = tokeniser.consume(\":\");\n  if (!colon) {\n    return {};\n  }\n  const inheritance = tokeniser.consume(\"identifier\") || tokeniser.error(\"Inheritance lacks a type\");\n  return { colon, inheritance };\n}\n\nexport class Container extends Base {\n    /**\n     * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n     * @param {*} instance\n     * @param {*} args\n     */\n    static parse(tokeniser, instance, { type, inheritable, allowedMembers }) {\n      const { tokens } = instance;\n      tokens.name = tokeniser.consume(\"identifier\") || tokeniser.error(`Missing name in ${instance.type}`);\n      tokeniser.current = instance;\n      if (inheritable) {\n        Object.assign(tokens, inheritance(tokeniser));\n      }\n      tokens.open = tokeniser.consume(\"{\") || tokeniser.error(`Bodyless ${type}`);\n      instance.members = [];\n      while (true) {\n        tokens.close = tokeniser.consume(\"}\");\n        if (tokens.close) {\n          tokens.termination = tokeniser.consume(\";\") || tokeniser.error(`Missing semicolon after ${type}`);\n          return instance;\n        }\n        const ea = ExtendedAttributes.parse(tokeniser);\n        let mem;\n        for (const [parser, ...args] of allowedMembers) {\n          mem = parser(tokeniser, ...args);\n          if (mem) {\n            break;\n          }\n        }\n        if (!mem) {\n          tokeniser.error(\"Unknown member\");\n        }\n        mem.extAttrs = ea;\n        instance.members.push(mem);\n      }\n    }\n\n    get partial() {\n      return !!this.tokens.partial;\n    }\n    get name() {\n      return unescape(this.tokens.name.value);\n    }\n    get inheritance() {\n      if (!this.tokens.inheritance) {\n        return null;\n      }\n      return unescape(this.tokens.inheritance.value);\n    }\n\n    *validate(defs) {\n      for (const member of this.members) {\n        if (member.validate) {\n          yield* member.validate(defs);\n        }\n      }\n    }\n  }\n","import { Base } from \"./base.js\";\nimport { Type } from \"./type.js\";\nimport { const_data, const_value, primitive_type } from \"./helpers.js\";\n\nexport class Constant extends Base {\n  /**\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    const tokens = {};\n    tokens.base = tokeniser.consume(\"const\");\n    if (!tokens.base) {\n      return;\n    }\n    let idlType = primitive_type(tokeniser);\n    if (!idlType) {\n      const base = tokeniser.consume(\"identifier\") || tokeniser.error(\"Const lacks a type\");\n      idlType = new Type({ source: tokeniser.source, tokens: { base } });\n    }\n    if (tokeniser.probe(\"?\")) {\n      tokeniser.error(\"Unexpected nullable constant type\");\n    }\n    idlType.type = \"const-type\";\n    tokens.name = tokeniser.consume(\"identifier\") || tokeniser.error(\"Const lacks a name\");\n    tokens.assign = tokeniser.consume(\"=\") || tokeniser.error(\"Const lacks value assignment\");\n    tokens.value = const_value(tokeniser) || tokeniser.error(\"Const lacks a value\");\n    tokens.termination = tokeniser.consume(\";\") || tokeniser.error(\"Unterminated const, expected `;`\");\n    const ret = new Constant({ source: tokeniser.source, tokens });\n    ret.idlType = idlType;\n    return ret;\n  }\n\n  get type() {\n    return \"const\";\n  }\n  get name() {\n    return unescape(this.tokens.name.value);\n  }\n  get value() {\n    return const_data(this.tokens.value);\n  }\n}\n","import { Base } from \"./base.js\";\nimport { type_with_extended_attributes } from \"./helpers.js\";\n\nexport class IterableLike extends Base {\n  /**\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    const start_position = tokeniser.position;\n    const tokens = {};\n    const ret = new IterableLike({ source: tokeniser.source, tokens });\n    tokens.readonly = tokeniser.consume(\"readonly\");\n    if (!tokens.readonly) {\n      tokens.async = tokeniser.consume(\"async\");\n    }\n    tokens.base =\n      tokens.readonly ? tokeniser.consume(\"maplike\", \"setlike\") :\n      tokens.async ? tokeniser.consume(\"iterable\") :\n      tokeniser.consume(\"iterable\", \"maplike\", \"setlike\");\n    if (!tokens.base) {\n      tokeniser.unconsume(start_position);\n      return;\n    }\n\n    const { type } = ret;\n    const secondTypeRequired = type === \"maplike\" || ret.async;\n    const secondTypeAllowed = secondTypeRequired || type === \"iterable\";\n\n    tokens.open = tokeniser.consume(\"<\") || tokeniser.error(`Missing less-than sign \\`<\\` in ${type} declaration`);\n    const first = type_with_extended_attributes(tokeniser) || tokeniser.error(`Missing a type argument in ${type} declaration`);\n    ret.idlType = [first];\n    if (secondTypeAllowed) {\n      first.tokens.separator = tokeniser.consume(\",\");\n      if (first.tokens.separator) {\n        ret.idlType.push(type_with_extended_attributes(tokeniser));\n      }\n      else if (secondTypeRequired) {\n        tokeniser.error(`Missing second type argument in ${type} declaration`);\n      }\n    }\n    tokens.close = tokeniser.consume(\">\") || tokeniser.error(`Missing greater-than sign \\`>\\` in ${type} declaration`);\n    tokens.termination = tokeniser.consume(\";\") || tokeniser.error(`Missing semicolon after ${type} declaration`);\n\n    return ret;\n  }\n\n  get type() {\n    return this.tokens.base.value;\n  }\n  get readonly() {\n    return !!this.tokens.readonly;\n  }\n  get async() {\n    return !!this.tokens.async;\n  }\n}\n","import { Base } from \"./base.js\";\nimport { argument_list } from \"./helpers.js\";\n\nexport class Constructor extends Base {\n  /**\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    const base = tokeniser.consume(\"constructor\");\n    if (!base) {\n      return;\n    }\n    const tokens = { base };\n    tokens.open = tokeniser.consume(\"(\") || tokeniser.error(\"No argument list in constructor\");\n    const args = argument_list(tokeniser);\n    tokens.close = tokeniser.consume(\")\") || tokeniser.error(\"Unterminated constructor\");\n    tokens.termination = tokeniser.consume(\";\") || tokeniser.error(\"No semicolon after constructor\");\n    const ret = new Constructor({ tokens });\n    ret.arguments = args;\n    return ret;\n  }\n\n  get type() {\n    return \"constructor\";\n  }\n\n  *validate(defs) {\n    if (this.idlType) {\n      yield* this.idlType.validate(defs);\n    }\n    for (const argument of this.arguments) {\n      yield* argument.validate(defs);\n    }\n  }\n}\n","import { Container } from \"./container.js\";\nimport { Attribute } from \"./attribute.js\";\nimport { Operation } from \"./operation.js\";\nimport { Constant } from \"./constant.js\";\nimport { IterableLike } from \"./iterable.js\";\nimport { stringifier, autofixAddExposedWindow, getMemberIndentation, getLastIndentation, getFirstToken } from \"./helpers.js\";\nimport { validationError } from \"../error.js\";\nimport { checkInterfaceMemberDuplication } from \"../validators/interface.js\";\nimport { Constructor } from \"./constructor.js\";\nimport { Tokeniser } from \"../tokeniser.js\";\n\n/**\n * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n */\nfunction static_member(tokeniser) {\n  const special = tokeniser.consume(\"static\");\n  if (!special) return;\n  const member = Attribute.parse(tokeniser, { special }) ||\n    Operation.parse(tokeniser, { special }) ||\n    tokeniser.error(\"No body in static member\");\n  return member;\n}\n\nexport class Interface extends Container {\n  /**\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser, base, { partial = null } = {}) {\n    const tokens = { partial, base };\n    return Container.parse(tokeniser, new Interface({ source: tokeniser.source, tokens }), {\n      type: \"interface\",\n      inheritable: !partial,\n      allowedMembers: [\n        [Constant.parse],\n        [Constructor.parse],\n        [static_member],\n        [stringifier],\n        [IterableLike.parse],\n        [Attribute.parse],\n        [Operation.parse]\n      ]\n    });\n  }\n\n  get type() {\n    return \"interface\";\n  }\n\n  *validate(defs) {\n    yield* this.extAttrs.validate(defs);\n    if (\n      !this.partial &&\n      this.extAttrs.every(extAttr => extAttr.name !== \"Exposed\") &&\n      this.extAttrs.every(extAttr => extAttr.name !== \"NoInterfaceObject\")\n    ) {\n      const message = `Interfaces must have \\`[Exposed]\\` extended attribute. \\\nTo fix, add, for example, \\`[Exposed=Window]\\`. Please also consider carefully \\\nif your interface should also be exposed in a Worker scope. Refer to the \\\n[WebIDL spec section on Exposed](https://heycam.github.io/webidl/#Exposed) \\\nfor more information.`;\n      yield validationError(this.tokens.name, this, \"require-exposed\", message, {\n        autofix: autofixAddExposedWindow(this)\n      });\n    }\n    const constructors = this.extAttrs.filter(extAttr => extAttr.name === \"Constructor\");\n    for (const constructor of constructors) {\n      const message = `Constructors should now be represented as a \\`constructor()\\` operation on the interface \\\ninstead of \\`[Constructor]\\` extended attribute. Refer to the \\\n[WebIDL spec section on constructor operations](https://heycam.github.io/webidl/#idl-constructors) \\\nfor more information.`;\n      yield validationError(constructor.tokens.name, this, \"constructor-member\", message, {\n        autofix: autofixConstructor(this, constructor)\n      });\n    }\n\n    yield* super.validate(defs);\n    if (!this.partial) {\n      yield* checkInterfaceMemberDuplication(defs, this);\n    }\n  }\n}\n\nfunction autofixConstructor(interfaceDef, constructorExtAttr) {\n  return () => {\n    const indentation = getLastIndentation(interfaceDef.extAttrs.tokens.open.trivia);\n    const memberIndent = interfaceDef.members.length ?\n      getLastIndentation(getFirstToken(interfaceDef.members[0]).trivia) :\n      getMemberIndentation(indentation);\n    const constructorOp = Constructor.parse(new Tokeniser(`\\n${memberIndent}constructor();`));\n    constructorOp.extAttrs = [];\n    constructorOp.arguments = constructorExtAttr.arguments;\n\n    const existingIndex = interfaceDef.members.findIndex(m => m.type === \"constructor\");\n    interfaceDef.members.splice(existingIndex + 1, 0, constructorOp);\n\n    const { close }  = interfaceDef.tokens;\n    if (!close.trivia.includes(\"\\n\")) {\n      close.trivia += `\\n${indentation}`;\n    }\n\n    const { extAttrs } = interfaceDef;\n    const index = extAttrs.indexOf(constructorExtAttr);\n    const removed = extAttrs.splice(index, 1);\n    if (!extAttrs.length) {\n      extAttrs.tokens.open = extAttrs.tokens.close = undefined;\n    } else if (extAttrs.length === index) {\n      extAttrs[index - 1].tokens.separator = undefined;\n    } else if (!extAttrs[index].tokens.name.trivia.trim()) {\n      extAttrs[index].tokens.name.trivia = removed[0].tokens.name.trivia;\n    }\n  };\n}\n","import { validationError } from \"../error.js\";\n\nexport function* checkInterfaceMemberDuplication(defs, i) {\n  const opNames = new Set(getOperations(i).map(op => op.name));\n  const partials = defs.partials.get(i.name) || [];\n  const mixins = defs.mixinMap.get(i.name) || [];\n  for (const ext of [...partials, ...mixins]) {\n    const additions = getOperations(ext);\n    yield* forEachExtension(additions, opNames, ext, i);\n    for (const addition of additions) {\n      opNames.add(addition.name);\n    }\n  }\n\n  function* forEachExtension(additions, existings, ext, base) {\n    for (const addition of additions) {\n      const { name } = addition;\n      if (name && existings.has(name)) {\n        const message = `The operation \"${name}\" has already been defined for the base interface \"${base.name}\" either in itself or in a mixin`;\n        yield validationError(addition.tokens.name, ext, \"no-cross-overload\", message);\n      }\n    }\n  }\n\n  function getOperations(i) {\n    return i.members\n      .filter(({type}) => type === \"operation\");\n  }\n}\n","import { Container } from \"./container.js\";\nimport { Constant } from \"./constant.js\";\nimport { Attribute } from \"./attribute.js\";\nimport { Operation } from \"./operation.js\";\nimport { stringifier } from \"./helpers.js\";\n\nexport class Mixin extends Container {\n  /**\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser, base, { partial } = {}) {\n    const tokens = { partial, base };\n    tokens.mixin = tokeniser.consume(\"mixin\");\n    if (!tokens.mixin) {\n      return;\n    }\n    return Container.parse(tokeniser, new Mixin({ source: tokeniser.source, tokens }), {\n      type: \"interface mixin\",\n      allowedMembers: [\n        [Constant.parse],\n        [stringifier],\n        [Attribute.parse, { noInherit: true }],\n        [Operation.parse, { regular: true }]\n      ]\n    });\n  }\n\n  get type() {\n    return \"interface mixin\";\n  }\n}\n","import { Base } from \"./base.js\";\nimport { unescape, type_with_extended_attributes } from \"./helpers.js\";\nimport { ExtendedAttributes } from \"./extended-attributes.js\";\nimport { Default } from \"./default.js\";\n\nexport class Field extends Base {\n  /**\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    const tokens = {};\n    const ret = new Field({ source: tokeniser.source, tokens });\n    ret.extAttrs = ExtendedAttributes.parse(tokeniser);\n    tokens.required = tokeniser.consume(\"required\");\n    ret.idlType = type_with_extended_attributes(tokeniser, \"dictionary-type\") || tokeniser.error(\"Dictionary member lacks a type\");\n    tokens.name = tokeniser.consume(\"identifier\") || tokeniser.error(\"Dictionary member lacks a name\");\n    ret.default = Default.parse(tokeniser);\n    if (tokens.required && ret.default) tokeniser.error(\"Required member must not have a default\");\n    tokens.termination = tokeniser.consume(\";\") || tokeniser.error(\"Unterminated dictionary member, expected `;`\");\n    return ret;\n  }\n\n  get type() {\n    return \"field\";\n  }\n  get name() {\n    return unescape(this.tokens.name.value);\n  }\n  get required() {\n    return !!this.tokens.required;\n  }\n\n  *validate(defs) {\n    yield* this.idlType.validate(defs);\n  }\n}\n","import { Container } from \"./container.js\";\nimport { Field } from \"./field.js\";\n\nexport class Dictionary extends Container {\n  /**\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser, { partial } = {}) {\n    const tokens = { partial };\n    tokens.base = tokeniser.consume(\"dictionary\");\n    if (!tokens.base) {\n      return;\n    }\n    return Container.parse(tokeniser, new Dictionary({ source: tokeniser.source, tokens }), {\n      type: \"dictionary\",\n      inheritable: !partial,\n      allowedMembers: [\n        [Field.parse],\n      ]\n    });\n  }\n\n  get type() {\n    return \"dictionary\";\n  }\n}\n","import { Container } from \"./container.js\";\nimport { Attribute } from \"./attribute.js\";\nimport { Operation } from \"./operation.js\";\nimport { validationError } from \"../error.js\";\nimport { autofixAddExposedWindow } from \"./helpers.js\";\n\nexport class Namespace extends Container {\n  /**\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser, { partial } = {}) {\n    const tokens = { partial };\n    tokens.base = tokeniser.consume(\"namespace\");\n    if (!tokens.base) {\n      return;\n    }\n    return Container.parse(tokeniser, new Namespace({ source: tokeniser.source, tokens }), {\n      type: \"namespace\",\n      allowedMembers: [\n        [Attribute.parse, { noInherit: true, readonly: true }],\n        [Operation.parse, { regular: true }]\n      ]\n    });\n  }\n\n  get type() {\n    return \"namespace\";\n  }\n\n  *validate(defs) {\n    if (!this.partial && this.extAttrs.every(extAttr => extAttr.name !== \"Exposed\")) {\n      const message = `Namespaces must have [Exposed] extended attribute. \\\nTo fix, add, for example, [Exposed=Window]. Please also consider carefully \\\nif your namespace should also be exposed in a Worker scope. Refer to the \\\n[WebIDL spec section on Exposed](https://heycam.github.io/webidl/#Exposed) \\\nfor more information.`;\n      yield validationError(this.tokens.name, this, \"require-exposed\", message, {\n        autofix: autofixAddExposedWindow(this)\n      });\n    }\n    yield* super.validate(defs);\n  }\n}\n","import { Container } from \"./container.js\";\nimport { Operation } from \"./operation.js\";\nimport { Constant } from \"./constant.js\";\n\n\nexport class CallbackInterface extends Container {\n  /**\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser, callback, { partial = null } = {}) {\n    const tokens = { callback };\n    tokens.base = tokeniser.consume(\"interface\");\n    if (!tokens.base) {\n      return;\n    }\n    return Container.parse(tokeniser, new CallbackInterface({ source: tokeniser.source, tokens }), {\n      type: \"callback interface\",\n      inheritable: !partial,\n      allowedMembers: [\n        [Constant.parse],\n        [Operation.parse, { regular: true }]\n      ]\n    });\n  }\n\n  get type() {\n    return \"callback interface\";\n  }\n}\n","\"use strict\";\n\nimport { Tokeniser } from \"./tokeniser.js\";\nimport { Enum } from \"./productions/enum.js\";\nimport { Includes } from \"./productions/includes.js\";\nimport { ExtendedAttributes } from \"./productions/extended-attributes.js\";\nimport { Typedef } from \"./productions/typedef.js\";\nimport { CallbackFunction } from \"./productions/callback.js\";\nimport { Interface } from \"./productions/interface.js\";\nimport { Mixin } from \"./productions/mixin.js\";\nimport { Dictionary } from \"./productions/dictionary.js\";\nimport { Namespace } from \"./productions/namespace.js\";\nimport { CallbackInterface } from \"./productions/callback-interface.js\";\n\n/**\n * @param {Tokeniser} tokeniser\n * @param {object} options\n * @param {boolean} [options.concrete]\n */\nfunction parseByTokens(tokeniser, options) {\n  const source = tokeniser.source;\n\n  function error(str) {\n    tokeniser.error(str);\n  }\n\n  function consume(...candidates) {\n    return tokeniser.consume(...candidates);\n  }\n\n  function callback() {\n    const callback = consume(\"callback\");\n    if (!callback) return;\n    if (tokeniser.probe(\"interface\")) {\n      return CallbackInterface.parse(tokeniser, callback);\n    }\n    return CallbackFunction.parse(tokeniser, callback);\n  }\n\n  function interface_(opts) {\n    const base = consume(\"interface\");\n    if (!base) return;\n    const ret = Mixin.parse(tokeniser, base, opts) ||\n      Interface.parse(tokeniser, base, opts) ||\n      error(\"Interface has no proper body\");\n    return ret;\n  }\n\n  function partial() {\n    const partial = consume(\"partial\");\n    if (!partial) return;\n    return Dictionary.parse(tokeniser, { partial }) ||\n      interface_({ partial }) ||\n      Namespace.parse(tokeniser, { partial }) ||\n      error(\"Partial doesn't apply to anything\");\n  }\n\n  function definition() {\n    return callback() ||\n      interface_() ||\n      partial() ||\n      Dictionary.parse(tokeniser) ||\n      Enum.parse(tokeniser) ||\n      Typedef.parse(tokeniser) ||\n      Includes.parse(tokeniser) ||\n      Namespace.parse(tokeniser);\n  }\n\n  function definitions() {\n    if (!source.length) return [];\n    const defs = [];\n    while (true) {\n      const ea = ExtendedAttributes.parse(tokeniser);\n      const def = definition();\n      if (!def) {\n        if (ea.length) error(\"Stray extended attributes\");\n        break;\n      }\n      def.extAttrs = ea;\n      defs.push(def);\n    }\n    const eof = consume(\"eof\");\n    if (options.concrete) {\n      defs.push(eof);\n    }\n    return defs;\n  }\n  const res = definitions();\n  if (tokeniser.position < source.length) error(\"Unrecognised tokens\");\n  return res;\n}\n\nexport function parse(str, options = {}) {\n  const tokeniser = new Tokeniser(str);\n  if (typeof options.sourceName !== \"undefined\") {\n    tokeniser.source.name = options.sourceName;\n  }\n  return parseByTokens(tokeniser, options);\n}\n","\"use strict\";\n\nfunction noop(arg) {\n  return arg;\n}\n\nconst templates = {\n  wrap: items => items.join(\"\"),\n  trivia: noop,\n  name: noop,\n  reference: noop,\n  type: noop,\n  generic: noop,\n  nameless: noop,\n  inheritance: noop,\n  definition: noop,\n  extendedAttribute: noop,\n  extendedAttributeReference: noop\n};\n\nexport function write(ast, { templates: ts = templates } = {}) {\n  ts = Object.assign({}, templates, ts);\n\n  function reference(raw, { unescaped, context }) {\n    if (!unescaped) {\n      unescaped = raw.startsWith(\"_\") ? raw.slice(1) : raw;\n    }\n    return ts.reference(raw, unescaped, context);\n  }\n\n  function token(t, wrapper = noop, ...args) {\n    if (!t) {\n      return \"\";\n    }\n    const value = wrapper(t.value, ...args);\n    return ts.wrap([ts.trivia(t.trivia), value]);\n  }\n\n  function reference_token(t, context) {\n    return token(t, reference, { context });\n  }\n\n  function name_token(t, arg) {\n    return token(t, ts.name, arg);\n  }\n\n  function type_body(it) {\n    if (it.union || it.generic) {\n      return ts.wrap([\n        token(it.tokens.base, ts.generic),\n        token(it.tokens.open),\n        ...it.subtype.map(type),\n        token(it.tokens.close)\n      ]);\n    }\n    const firstToken = it.tokens.prefix || it.tokens.base;\n    const prefix = it.tokens.prefix ? [\n      it.tokens.prefix.value,\n      ts.trivia(it.tokens.base.trivia)\n    ] : [];\n    const ref = reference(ts.wrap([\n      ...prefix,\n      it.tokens.base.value,\n      token(it.tokens.postfix)\n    ]), { unescaped: it.idlType, context: it });\n    return ts.wrap([ts.trivia(firstToken.trivia), ref]);\n  }\n  function type(it) {\n    return ts.wrap([\n      extended_attributes(it.extAttrs),\n      type_body(it),\n      token(it.tokens.nullable),\n      token(it.tokens.separator)\n    ]);\n  }\n  function default_(def) {\n    if (!def) {\n      return \"\";\n    }\n    return ts.wrap([\n      token(def.tokens.assign),\n      ...def.expression.map(t => token(t))\n    ]);\n  }\n  function argument(arg) {\n    return ts.wrap([\n      extended_attributes(arg.extAttrs),\n      token(arg.tokens.optional),\n      ts.type(type(arg.idlType)),\n      token(arg.tokens.variadic),\n      name_token(arg.tokens.name, { data: arg }),\n      default_(arg.default),\n      token(arg.tokens.separator)\n    ]);\n  }\n  function identifier(id, context) {\n    return ts.wrap([\n      reference_token(id.tokens.value, context),\n      token(id.tokens.separator)\n    ]);\n  }\n  function make_ext_at(it) {\n    const { rhsType } = it.params;\n    return ts.wrap([\n      ts.trivia(it.tokens.name.trivia),\n      ts.extendedAttribute(ts.wrap([\n        ts.extendedAttributeReference(it.name),\n        token(it.params.tokens.assign),\n        reference_token(it.params.tokens.secondaryName, it),\n        token(it.params.tokens.open),\n        ...!it.params.list ? [] :\n          it.params.list.map(\n            rhsType === \"identifier-list\" ? id => identifier(id, it) : argument\n          ),\n        token(it.params.tokens.close)\n      ])),\n      token(it.tokens.separator)\n    ]);\n  }\n  function extended_attributes(eats) {\n    if (!eats.length) return \"\";\n    return ts.wrap([\n      token(eats.tokens.open),\n      ...eats.map(make_ext_at),\n      token(eats.tokens.close)\n    ]);\n  }\n\n  function operation(it, parent) {\n    const body = it.idlType ? [\n      ts.type(type(it.idlType)),\n      name_token(it.tokens.name, { data: it, parent }),\n      token(it.tokens.open),\n      ts.wrap(it.arguments.map(argument)),\n      token(it.tokens.close),\n    ] : [];\n    return ts.definition(ts.wrap([\n      extended_attributes(it.extAttrs),\n      it.tokens.name ? token(it.tokens.special) : token(it.tokens.special, ts.nameless, { data: it, parent }),\n      ...body,\n      token(it.tokens.termination)\n    ]), { data: it, parent });\n  }\n\n  function attribute(it, parent) {\n    return ts.definition(ts.wrap([\n      extended_attributes(it.extAttrs),\n      token(it.tokens.special),\n      token(it.tokens.readonly),\n      token(it.tokens.base),\n      ts.type(type(it.idlType)),\n      name_token(it.tokens.name, { data: it, parent }),\n      token(it.tokens.termination)\n    ]), { data: it, parent });\n  }\n\n  function constructor(it, parent) {\n    return ts.definition(ts.wrap([\n      extended_attributes(it.extAttrs),\n      token(it.tokens.base, ts.nameless, { data: it, parent }),\n      token(it.tokens.open),\n      ts.wrap(it.arguments.map(argument)),\n      token(it.tokens.close),\n      token(it.tokens.termination)\n    ]), { data: it, parent });\n  }\n\n  function inheritance(inh) {\n    if (!inh.tokens.inheritance) {\n      return \"\";\n    }\n    return ts.wrap([\n      token(inh.tokens.colon),\n      ts.trivia(inh.tokens.inheritance.trivia),\n      ts.inheritance(reference(inh.tokens.inheritance.value, { context: inh }))\n    ]);\n  }\n\n  function container(it) {\n    return ts.definition(ts.wrap([\n      extended_attributes(it.extAttrs),\n      token(it.tokens.callback),\n      token(it.tokens.partial),\n      token(it.tokens.base),\n      token(it.tokens.mixin),\n      name_token(it.tokens.name, { data: it }),\n      inheritance(it),\n      token(it.tokens.open),\n      iterate(it.members, it),\n      token(it.tokens.close),\n      token(it.tokens.termination)\n    ]), { data: it });\n  }\n\n  function field(it, parent) {\n    return ts.definition(ts.wrap([\n      extended_attributes(it.extAttrs),\n      token(it.tokens.required),\n      ts.type(type(it.idlType)),\n      name_token(it.tokens.name, { data: it, parent }),\n      default_(it.default),\n      token(it.tokens.termination)\n    ]), { data: it, parent });\n  }\n  function const_(it, parent) {\n    return ts.definition(ts.wrap([\n      extended_attributes(it.extAttrs),\n      token(it.tokens.base),\n      ts.type(type(it.idlType)),\n      name_token(it.tokens.name, { data: it, parent }),\n      token(it.tokens.assign),\n      token(it.tokens.value),\n      token(it.tokens.termination)\n    ]), { data: it, parent });\n  }\n  function typedef(it) {\n    return ts.definition(ts.wrap([\n      extended_attributes(it.extAttrs),\n      token(it.tokens.base),\n      ts.type(type(it.idlType)),\n      name_token(it.tokens.name, { data: it }),\n      token(it.tokens.termination)\n    ]), { data: it });\n  }\n  function includes(it) {\n    return ts.definition(ts.wrap([\n      extended_attributes(it.extAttrs),\n      reference_token(it.tokens.target, it),\n      token(it.tokens.includes),\n      reference_token(it.tokens.mixin, it),\n      token(it.tokens.termination)\n    ]), { data: it });\n  }\n  function callback(it) {\n    return ts.definition(ts.wrap([\n      extended_attributes(it.extAttrs),\n      token(it.tokens.base),\n      name_token(it.tokens.name, { data: it }),\n      token(it.tokens.assign),\n      ts.type(type(it.idlType)),\n      token(it.tokens.open),\n      ...it.arguments.map(argument),\n      token(it.tokens.close),\n      token(it.tokens.termination),\n    ]), { data: it });\n  }\n  function enum_(it) {\n    return ts.definition(ts.wrap([\n      extended_attributes(it.extAttrs),\n      token(it.tokens.base),\n      name_token(it.tokens.name, { data: it }),\n      token(it.tokens.open),\n      iterate(it.values, it),\n      token(it.tokens.close),\n      token(it.tokens.termination)\n    ]), { data: it });\n  }\n  function enum_value(v, parent) {\n    return ts.wrap([\n      ts.trivia(v.tokens.value.trivia),\n      ts.definition(\n        ts.wrap(['\"', ts.name(v.value, { data: v, parent }), '\"']),\n        { data: v, parent }\n      ),\n      token(v.tokens.separator)\n    ]);\n  }\n  function iterable_like(it, parent) {\n    return ts.definition(ts.wrap([\n      extended_attributes(it.extAttrs),\n      token(it.tokens.readonly),\n      token(it.tokens.async),\n      token(it.tokens.base, ts.generic),\n      token(it.tokens.open),\n      ts.wrap(it.idlType.map(type)),\n      token(it.tokens.close),\n      token(it.tokens.termination)\n    ]), { data: it, parent });\n  }\n  function eof(it) {\n    return ts.trivia(it.trivia);\n  }\n\n  const table = {\n    interface: container,\n    \"interface mixin\": container,\n    namespace: container,\n    operation,\n    attribute,\n    constructor,\n    dictionary: container,\n    field,\n    const: const_,\n    typedef,\n    includes,\n    callback,\n    enum: enum_,\n    \"enum-value\": enum_value,\n    iterable: iterable_like,\n    maplike: iterable_like,\n    setlike: iterable_like,\n    \"callback interface\": container,\n    eof\n  };\n  function dispatch(it, parent) {\n    const dispatcher = table[it.type];\n    if (!dispatcher) {\n      throw new Error(`Type \"${it.type}\" is unsupported`);\n    }\n    return table[it.type](it, parent);\n  }\n  function iterate(things, parent) {\n    if (!things) return;\n    const results = things.map(thing => dispatch(thing, parent));\n    return ts.wrap(results);\n  }\n  return iterate(ast);\n}\n","\"use strict\";\n\nimport { validationError as error } from \"./error.js\";\n\nfunction getMixinMap(all, unique) {\n  const map = new Map();\n  const includes = all.filter(def => def.type === \"includes\");\n  for (const include of includes) {\n    const mixin = unique.get(include.includes);\n    if (!mixin) {\n      continue;\n    }\n    const array = map.get(include.target);\n    if (array) {\n      array.push(mixin);\n    } else {\n      map.set(include.target, [mixin]);\n    }\n  }\n  return map;\n}\n\nfunction groupDefinitions(all) {\n  const unique = new Map();\n  const duplicates = new Set();\n  const partials = new Map();\n  for (const def of all) {\n    if (def.partial) {\n      const array = partials.get(def.name);\n      if (array) {\n        array.push(def);\n      } else {\n        partials.set(def.name, [def]);\n      }\n      continue;\n    }\n    if (!def.name) {\n      continue;\n    }\n    if (!unique.has(def.name)) {\n      unique.set(def.name, def);\n    } else {\n      duplicates.add(def);\n    }\n  }\n  return {\n    all,\n    unique,\n    partials,\n    duplicates,\n    mixinMap: getMixinMap(all, unique),\n    cache: {\n      typedefIncludesDictionary: new WeakMap()\n    },\n  };\n}\n\nfunction* checkDuplicatedNames({ unique, duplicates }) {\n  for (const dup of duplicates) {\n    const { name } = dup;\n    const message = `The name \"${name}\" of type \"${unique.get(name).type}\" was already seen`;\n    yield error(dup.tokens.name, dup, \"no-duplicate\", message);\n  }\n}\n\nfunction* validateIterable(ast) {\n  const defs = groupDefinitions(ast);\n  for (const def of defs.all) {\n    if (def.validate) {\n      yield* def.validate(defs);\n    }\n  }\n  yield* checkDuplicatedNames(defs);\n}\n\n// Remove this once all of our support targets expose `.flat()` by default\nfunction flatten(array) {\n  if (array.flat) {\n    return array.flat();\n  }\n  return [].concat(...array);\n}\n\n/**\n * @param {*} ast AST or array of ASTs\n */\nexport function validate(ast) {\n  return [...validateIterable(flatten(ast))];\n}\n","export { parse } from \"./lib/webidl2.js\";\nexport { write } from \"./lib/writer.js\";\nexport { validate } from \"./lib/validator.js\";\nexport { WebIDLParseError } from \"./lib/tokeniser.js\";\n"],"sourceRoot":""}
