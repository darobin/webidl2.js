{"version":3,"sources":["webpack://WebIDL2/webpack/universalModuleDefinition","webpack://WebIDL2/webpack/bootstrap","webpack://WebIDL2/./lib/error.js","webpack://WebIDL2/./lib/tokeniser.js","webpack://WebIDL2/./lib/productions/base.js","webpack://WebIDL2/./lib/productions/type.js","webpack://WebIDL2/./lib/productions/default.js","webpack://WebIDL2/./lib/productions/array-base.js","webpack://WebIDL2/./lib/productions/extended-attributes.js","webpack://WebIDL2/./lib/productions/helpers.js","webpack://WebIDL2/./lib/productions/argument.js","webpack://WebIDL2/./lib/productions/token.js","webpack://WebIDL2/./lib/validators/helpers.js","webpack://WebIDL2/./lib/productions/operation.js","webpack://WebIDL2/./lib/productions/attribute.js","webpack://WebIDL2/./lib/productions/enum.js","webpack://WebIDL2/./lib/productions/includes.js","webpack://WebIDL2/./lib/productions/typedef.js","webpack://WebIDL2/./lib/productions/callback.js","webpack://WebIDL2/./lib/productions/container.js","webpack://WebIDL2/./lib/productions/constant.js","webpack://WebIDL2/./lib/productions/iterable.js","webpack://WebIDL2/./lib/productions/interface.js","webpack://WebIDL2/./lib/validators/interface.js","webpack://WebIDL2/./lib/productions/mixin.js","webpack://WebIDL2/./lib/productions/field.js","webpack://WebIDL2/./lib/productions/dictionary.js","webpack://WebIDL2/./lib/productions/namespace.js","webpack://WebIDL2/./lib/productions/callback-interface.js","webpack://WebIDL2/./lib/webidl2.js","webpack://WebIDL2/./lib/writer.js","webpack://WebIDL2/./lib/validator.js","webpack://WebIDL2/./index.js"],"names":["root","factory","exports","module","define","amd","this","installedModules","__webpack_require__","moduleId","i","l","modules","call","m","c","d","name","getter","o","Object","defineProperty","enumerable","get","r","Symbol","toStringTag","value","t","mode","__esModule","ns","create","key","bind","n","object","property","prototype","hasOwnProperty","p","s","error_error","source","position","current","message","type","sliceTokens","count","slice","Math","max","tokensToText","inputs","precedes","text","map","trivia","join","nextToken","length","line","precedingLine","splitted","split","lastLine","subsequentTokens","subsequentText","contextualMessage","repeat","partial","input","tokens","validationError","token","index","tokenRe","decimal","integer","identifier","string","whitespace","comment","other","stringTypes","argumentNameKeywords","nonRegexTerminals","concat","punctuations","tokeniser_Tokeniser","[object Object]","idl","str","lastCharIndex","nextChar","charAt","result","test","attemptTokenMatch","noFlushTrivia","currentTrivia","pop","match","includes","punctuation","startsWith","push","Error","re","lastIndex","exec","tokenise","WebIDLParseError","syntaxError","candidates","probe","super","Base","defineProperties","json","undefined","inheritance","proto","descMap","getOwnPropertyDescriptors","entries","getPrototypeOf","type_suffix","tokeniser","obj","nullable","consume","error","single_type","typeName","ret","base","type_Type","open","subtype","return_type","type_with_extended_attributes","keyType","keyIdlType","separator","valueType","idlType","close","generic_type","primitive_type","generic","typ","or","union_type","extAttrs","Boolean","union","helpers_unescape","prefix","postfix","filter","default_Default","assign","def","const_value","expression","const_data","negative","ArrayBase","Array","extended_attributes_ExtendedAttributeParameters","secondaryName","list","rhsType","ids","parser","token_Token","listName","identifiers","argument_list","hasRhs","extended_attributes_SimpleExtendedAttribute","params","parse","rhs","arguments","extended_attributes_ExtendedAttributes","argument_Argument","start_position","optional","variadic","default","unconsume","idlTypeIncludesDictionary","defs","unique","operation_Operation","special","regular","termination","argument","attribute_Attribute","noInherit","readonly","allowDangler","first","items","item","num_type","integer_type","decimal_type","voidToken","stringifier","enum_EnumValue","enum_Enum","values","includes_Includes","target","mixin","typedef_Typedef","callback_CallbackFunction","container_Container","instance","inheritable","allowedMembers","colon","members","ea","mem","args","member","validate","constant_Constant","unescape","iterable_IterableLike","secondTypeRequired","secondTypeAllowed","static_member","interface_Interface","every","extAttr","opNames","Set","getOperations","op","partials","mixins","mixinMap","ext","additions","forEachExtension","addition","add","existings","has","checkInterfaceMemberDuplication","mixin_Mixin","field_Field","required","dictionary_Dictionary","namespace_Namespace","callback_interface_CallbackInterface","callback","parseByTokens","options","interface_","opts","definition","res","eof","concrete","definitions","noop","arg","templates","wrap","reference","extendedAttribute","extendedAttributeReference","write","ast","ts","raw","unescaped","context","wrapper","reference_token","name_token","type_body","it","firstToken","ref","extended_attributes","default_","data","make_ext_at","id","eats","container","inh","iterate","iterable_like","parent","table","interface","interface mixin","namespace","operation","body","attribute","dictionary","field","const","typedef","enum","enum-value","v","iterable","legacyiterable","maplike","setlike","callback interface","things","results","thing","dispatch","getMixinMap","all","Map","include","array","set","validateIterable","duplicates","groupDefinitions","dup","checkDuplicatedNames","flat","__webpack_exports__"],"mappings":"CAAA,SAAAA,EAAAC,GACA,iBAAAC,SAAA,iBAAAC,OACAA,OAAAD,QAAAD,IACA,mBAAAG,eAAAC,IACAD,OAAA,GAAAH,GACA,iBAAAC,QACAA,QAAA,QAAAD,IAEAD,EAAA,QAAAC,IARA,CASCK,KAAA,WACD,mBCTA,IAAAC,EAAA,GAGA,SAAAC,EAAAC,GAGA,GAAAF,EAAAE,GACA,OAAAF,EAAAE,GAAAP,QAGA,IAAAC,EAAAI,EAAAE,GAAA,CACAC,EAAAD,EACAE,GAAA,EACAT,QAAA,IAUA,OANAU,EAAAH,GAAAI,KAAAV,EAAAD,QAAAC,IAAAD,QAAAM,GAGAL,EAAAQ,GAAA,EAGAR,EAAAD,QA0DA,OArDAM,EAAAM,EAAAF,EAGAJ,EAAAO,EAAAR,EAGAC,EAAAQ,EAAA,SAAAd,EAAAe,EAAAC,GACAV,EAAAW,EAAAjB,EAAAe,IACAG,OAAAC,eAAAnB,EAAAe,EAAA,CAA0CK,YAAA,EAAAC,IAAAL,KAK1CV,EAAAgB,EAAA,SAAAtB,GACA,oBAAAuB,eAAAC,aACAN,OAAAC,eAAAnB,EAAAuB,OAAAC,YAAA,CAAwDC,MAAA,WAExDP,OAAAC,eAAAnB,EAAA,cAAiDyB,OAAA,KAQjDnB,EAAAoB,EAAA,SAAAD,EAAAE,GAEA,GADA,EAAAA,IAAAF,EAAAnB,EAAAmB,IACA,EAAAE,EAAA,OAAAF,EACA,KAAAE,GAAA,iBAAAF,QAAAG,WAAA,OAAAH,EACA,IAAAI,EAAAX,OAAAY,OAAA,MAGA,GAFAxB,EAAAgB,EAAAO,GACAX,OAAAC,eAAAU,EAAA,WAAyCT,YAAA,EAAAK,UACzC,EAAAE,GAAA,iBAAAF,EAAA,QAAAM,KAAAN,EAAAnB,EAAAQ,EAAAe,EAAAE,EAAA,SAAAA,GAAgH,OAAAN,EAAAM,IAAqBC,KAAA,KAAAD,IACrI,OAAAF,GAIAvB,EAAA2B,EAAA,SAAAhC,GACA,IAAAe,EAAAf,KAAA2B,WACA,WAA2B,OAAA3B,EAAA,SAC3B,WAAiC,OAAAA,GAEjC,OADAK,EAAAQ,EAAAE,EAAA,IAAAA,GACAA,GAIAV,EAAAW,EAAA,SAAAiB,EAAAC,GAAsD,OAAAjB,OAAAkB,UAAAC,eAAA1B,KAAAuB,EAAAC,IAGtD7B,EAAAgC,EAAA,GAIAhC,IAAAiC,EAAA,kCCtEA,SAASC,EAAKC,EAAAC,EAAAC,EAAAC,EAAAC,GAId,SAAAC,EAAAC,GACA,OAAAA,EAAA,EACAN,EAAAO,MAAAN,IAAAK,GACAN,EAAAO,MAAAC,KAAAC,IAAAR,EAAAK,EAAA,GAAAL,GAGA,SAAAS,EAAAC,GAAAC,SAAiCA,GAAW,IAC5C,MAAAC,EAAAF,EAAAG,IAAA7B,KAAA8B,OAAA9B,EAAAD,OAAAgC,KAAA,IACAC,EAAAjB,EAAAC,GACA,cAAAgB,EAAAb,KACAS,EAEAD,EACAC,EAAAI,EAAAF,OAEAF,EAAAN,MAAAU,EAAAF,OAAAG,QAGA,MACAC,EACA,QAAAnB,EAAAC,GAAAG,KAAAJ,EAAAC,GAAAkB,KACAnB,EAAAkB,OAAA,EAAAlB,EAAAC,EAAA,GAAAkB,KACA,EAEAC,EArCA,SAAAP,GACA,MAAAQ,EAAAR,EAAAS,MAAA,MACA,OAAAD,IAAAH,OAAA,GAmCAK,CACAb,EAAAL,GAPA,GAOA,CAA2CO,UAAA,KAG3CY,EAAAnB,EAVA,GAWAoB,EAAAf,EAAAc,GAIAE,EAAAN,EAHAK,EAAAH,MAAA,SAGA,MADA,IAAAK,OAAAP,EAAAF,QAAA,KAAAf,GAKA,OACAA,WAAgBC,mBAAsBe,IAFtCjB,OADA,WAAAE,EAAA,sBAC6DF,EAAA0B,QAAA,gBAAoC1B,EAAAE,QAAgBF,EAAA5B,SAAa,QAEzDoD,IACrEP,OACAU,MAAAJ,EACAK,OAAAN,GAcO,SAAAO,EAAA/B,EAAAgC,EAAA9B,EAAAC,GACP,OAASJ,EAAKC,EAAAgC,EAAAC,MAAA/B,EAAAC,EAAA,cAAAA,eCpEd,MAAA+B,EAAA,CAGAC,QAAA,sGACAC,QAAA,8CACAC,WAAA,+BACAC,OAAA,WACAC,WAAA,cACAC,QAAA,iDACAC,MAAA,wBAGOC,EAAA,CACP,aACA,YACA,aAGOC,EAAA,CACP,YACA,WACA,QACA,UACA,aACA,OACA,SACA,WACA,UACA,YACA,WACA,UACA,YACA,UACA,WACA,UACA,SACA,SACA,cACA,UACA,gBAGAC,EAAA,CACA,YACA,cACA,WACA,MACA,UACA,UACA,OACA,SACA,QACA,QACA,aACA,iBACA,OACA,QACA,OACA,QACA,WACA,KACA,WACA,SACA,WACA,QACA,OACA,WACA,QACAC,OAAAF,EAAAD,GAEAI,EAAA,CACA,IACA,IACA,IACA,MACA,IACA,IACA,IACA,IACA,IACA,IACA,IACA,IACA,IACA,KA6FO,MAAMC,EAIbC,YAAAC,GACAtF,KAAAqC,OA5FA,SAAAkD,GACA,MAAApB,EAAA,GACA,IAAAqB,EAAA,EACApC,EAAA,GACAI,EAAA,EACAc,EAAA,EACA,KAAAkB,EAAAD,EAAAhC,QAAA,CACA,MAAAkC,EAAAF,EAAAG,OAAAF,GACA,IAAAG,GAAA,EAQA,GANA,YAAAC,KAAAH,GACAE,EAAAE,EAAA,cAAgDC,eAAA,IAC3C,MAAAL,IACLE,EAAAE,EAAA,WAA6CC,eAAA,MAG7C,IAAAH,EAAA,CACA,MAAAI,EAAA5B,EAAA6B,MAAA3E,MACAmC,IAAAuC,EAAAE,MAAA,YAAA1C,OACAH,GAAA2C,EACAzB,GAAA,OACK,oBAAAsB,KAAAH,IAKL,IAHA,KADAE,EAAAE,EAAA,cAEAF,EAAAE,EAAA,aAEA,IAAAF,EAAA,CACAA,EAAAE,EAAA,cACA,MAAAxB,EAAAF,IAAAZ,OAAA,IACA,IAAAoC,GAAAV,EAAAiB,SAAA7B,EAAAhD,SACAgD,EAAA5B,KAAA4B,EAAAhD,YAGK,MAAAoE,IACLE,EAAAE,EAAA,WAGA,UAAAM,KAAAhB,EACA,GAAAI,EAAAa,WAAAD,EAAAX,GAAA,CACArB,EAAAkC,KAAA,CAAqB5D,KAAA0D,EAAA9E,MAAA8E,EAAA/C,SAAAI,OAAAc,UACrBlB,EAAA,GAEAuC,EADAH,GAAAW,EAAA5C,OAEA,MAQA,IAHA,IAAAoC,IACAA,EAAAE,EAAA,WAEA,IAAAF,EACA,UAAAW,MAAA,gCAEAd,EAAAG,EACArB,GAAA,EAUA,OANAH,EAAAkC,KAAA,CACA5D,KAAA,MACApB,MAAA,GACA+B,WAGAe,EAOA,SAAA0B,EAAApD,GAAAqD,cAAoCA,GAAgB,IACpD,MAAAS,EAAAhC,EAAA9B,GACA8D,EAAAC,UAAAhB,EACA,MAAAG,EAAAY,EAAAE,KAAAlB,GACA,OAAAI,GACAxB,EAAAkC,KAAA,CAAmB5D,OAAApB,MAAAsE,EAAA,GAAAvC,SAAAI,OAAAc,UACnBwB,IACA1C,EAAA,IAEAmD,EAAAC,YAEA,GASAE,CAAApB,GACAtF,KAAAsC,SAAA,EAMA+C,MAAA7C,GACA,UAAAmE,EDlIO,SAAAtE,EAAAC,EAAAC,EAAAC,GACP,OAASJ,EAAKC,EAAAC,EAAAC,EAAAC,EAAA,UCiIiBoE,CAAW5G,KAAAqC,OAAArC,KAAAsC,SAAAtC,KAAAuC,QAAAC,IAM1C6C,MAAA5C,GACA,OAAAzC,KAAAqC,OAAAkB,OAAAvD,KAAAsC,UAAAtC,KAAAqC,OAAArC,KAAAsC,UAAAG,SAMA4C,WAAAwB,GACA,UAAApE,KAAAoE,EAAA,CACA,IAAA7G,KAAA8G,MAAArE,GAAA,SACA,MAAA4B,EAAArE,KAAAqC,OAAArC,KAAAsC,UAEA,OADAtC,KAAAsC,WACA+B,GAOAgB,UAAA/C,GACAtC,KAAAsC,YAIA,MAAAqE,UAAAL,MACAjB,aAAA7C,QAAeA,EAAAgB,OAAAU,QAAAC,WACf4C,MAAAvE,GACAxC,KAAAW,KAAA,mBACAX,KAAAwD,OACAxD,KAAAkE,QACAlE,KAAAmE,UCtOO,MAAA6C,EACP3B,aAAAhD,OAAeA,EAAA8B,WACfrD,OAAAmG,iBAAAjH,KAAA,CACAqC,OAAA,CAAehB,MAAAgB,GACf8B,OAAA,CAAe9C,MAAA8C,KAIfkB,SACA,MAAA6B,EAAA,CAAkBzE,UAAA0E,EAAAxG,UAAAwG,EAAAC,iBAAAD,GAClB,IAAAE,EAAArH,KACA,KAAAqH,IAAAvG,OAAAkB,WAAA,CACA,MAAAsF,EAAAxG,OAAAyG,0BAAAF,GACA,UAAA1F,EAAAN,KAAAP,OAAA0G,QAAAF,IACAjG,EAAAL,YAAAK,EAAAJ,OACAiG,EAAAvF,GAAA3B,KAAA2B,IAGA0F,EAAAvG,OAAA2G,eAAAJ,GAEA,OAAAH,GC2BA,SAAAQ,EAAAC,EAAAC,GACA,MAAAC,EAAAF,EAAAG,QAAA,KACAD,IACAD,EAAAzD,OAAA0D,YAEAF,EAAAb,MAAA,MAAAa,EAAAI,MAAA,iCAOA,SAAAC,EAAAL,EAAAM,GACA,IAAAC,EApDA,SAAAP,EAAAM,GACA,MAAAE,EAAAR,EAAAG,QAAA,6CACA,IAAAK,EACA,OAEA,MAAAD,EAAA,IAAkBE,EAAI,CAAE/F,OAAAsF,EAAAtF,OAAA8B,OAAA,CAAoCgE,UAE5D,OADAD,EAAA/D,OAAAkE,KAAAV,EAAAG,QAAA,MAAAH,EAAAI,kCAA0FI,EAAA1F,QAC1F0F,EAAA1F,MACA,eACAkF,EAAAb,MAAA,MAAAa,EAAAI,MAAA,+CACA,MAAAO,EAAsBC,EAAWZ,EAAAM,IAAAN,EAAAI,MAAA,2BACjCG,EAAAI,QAAAjC,KAAAiC,GACA,MAEA,eACA,mBACA,MAAAA,EAAsBE,EAA6Bb,EAAAM,IAAAN,EAAAI,iBAAoDI,EAAA1F,gBACvGyF,EAAAI,QAAAjC,KAAAiC,GACA,MAEA,cACAX,EAAAb,MAAA,MAAAa,EAAAI,MAAA,6CACA,MAAAU,EAAAd,EAAAG,WAA2C/C,IAAW4C,EAAAI,oCAAmDhD,EAAW1B,KAAA,SACpHqF,EAAA,IAA6BN,EAAI,CAAE/F,OAAAsF,EAAAtF,OAAA8B,OAAA,CAAoCgE,KAAAM,KACvEC,EAAAvE,OAAAwE,UAAAhB,EAAAG,QAAA,MAAAH,EAAAI,MAAA,uCACAW,EAAAjG,KAAAwF,EACA,MAAAW,EAAwBJ,EAA6Bb,EAAAM,IAAAN,EAAAI,MAAA,qCACrDG,EAAAI,QAAAjC,KAAAqC,EAAAE,GACA,OAKA,OAFAV,EAAAW,SAAAlB,EAAAI,oCAAkEI,EAAA1F,QAClEyF,EAAA/D,OAAA2E,MAAAnB,EAAAG,QAAA,MAAAH,EAAAI,uCAAgGI,EAAA1F,QAChGyF,EAmBAa,CAAApB,EAAAM,IAAiDe,EAAcrB,GAC/D,IAAAO,EAAA,CACA,MAAAC,EAAAR,EAAAG,QAAA,gBAAoD/C,GACpD,IAAAoD,EACA,OAEAD,EAAA,IAAcE,EAAI,CAAE/F,OAAAsF,EAAAtF,OAAA8B,OAAA,CAAoCgE,UACxDR,EAAAb,MAAA,MAAAa,EAAAI,kCAA0EI,EAAA9G,SAQ1E,MANA,YAAA6G,EAAAe,SAAAtB,EAAAb,MAAA,MACAa,EAAAI,MAAA,mCAEAG,EAAAzF,KAAAwF,GAAA,KACAP,EAAAC,EAAAO,GACAA,EAAAL,UAAA,QAAAK,EAAAW,SAAAlB,EAAAI,MAAA,sCACAG,EA+BO,MAAME,UAAapB,EAK1B3B,aAAAsC,EAAAM,GACA,OAAAD,EAAAL,EAAAM,IA9BA,SAAAN,EAAAlF,GACA,MAAA0B,EAAA,GAEA,GADAA,EAAAkE,KAAAV,EAAAG,QAAA,MACA3D,EAAAkE,KAAA,OACA,MAAAH,EAAA,IAAkBE,EAAI,CAAE/F,OAAAsF,EAAAtF,OAAA8B,WAExB,IADA+D,EAAAzF,QAAA,OACA,CACA,MAAAyG,EAAgBV,EAA6Bb,MAAAI,MAAA,wDAC7C,QAAAmB,EAAAL,SAAAlB,EAAAI,MAAA,iDACAG,EAAAI,QAAAjC,KAAA6C,GACA,MAAAC,EAAAxB,EAAAG,QAAA,MACA,IAAAqB,EAGA,MAFAD,EAAA/E,OAAAwE,UAAAQ,EASA,OALAjB,EAAAW,QAAAtF,OAAA,GACAoE,EAAAI,MAAA,kEAEA5D,EAAA2E,MAAAnB,EAAAG,QAAA,MAAAH,EAAAI,MAAA,2BACAL,EAAAC,EAAAO,GACAA,EASAkB,CAAAzB,EAAAM,GAGA5C,aAAAhD,OAAeA,EAAA8B,WACf4C,MAAA,CAAW1E,SAAA8B,WACXrD,OAAAC,eAAAf,KAAA,WAA4CqB,MAAA,KAC5CrB,KAAAqJ,SAAA,GAGAJ,cACA,OAAAjJ,KAAAsI,QAAA/E,QAAAvD,KAAAmE,OAAAgE,KACAnI,KAAAmE,OAAAgE,KAAA9G,MAEA,GAEAwG,eACA,OAAAyB,QAAAtJ,KAAAmE,OAAA0D,UAEA0B,YACA,OAAAD,QAAAtJ,KAAAsI,QAAA/E,UAAAvD,KAAAmE,OAAAgE,KAEAU,cACA,GAAA7I,KAAAsI,QAAA/E,OACA,OAAAvD,KAAAsI,QAQA,OAAWkB,EALX,CACAxJ,KAAAmE,OAAAsF,OACAzJ,KAAAmE,OAAAgE,KACAnI,KAAAmE,OAAAuF,SACAC,OAAArI,MAAA6B,IAAA7B,KAAAD,OAAAgC,KAAA,OC3IO,MAAMuG,UAAgB5C,EAI7B3B,aAAAsC,GACA,MAAAkC,EAAAlC,EAAAG,QAAA,KACA,IAAA+B,EACA,YAEA,MAAAC,EAAgBC,EAAWpC,MAAAG,QAAA,0BAA0DH,EAAAI,MAAA,wBACrFiC,EAAA,CAAAF,GACA,SAAAA,EAAArH,KAAA,CACA,MAAAqG,EAAAnB,EAAAG,QAAA,MAAAH,EAAAI,MAAA,wCACAiC,EAAA3D,KAAAyC,QACK,SAAAgB,EAAArH,KAAyB,CAC9B,MAAAqG,EAAAnB,EAAAG,QAAA,MAAwCH,EAAAI,MAAA,0CACxCiC,EAAA3D,KAAAyC,GAEA,WAAec,EAAO,CAAEvH,OAAAsF,EAAAtF,OAAA8B,OAAA,CAAoC0F,UAASG,eAGrE3E,aAAAhD,OAAeA,EAAA8B,SAAA6F,eACfjD,MAAA,CAAW1E,SAAA8B,WACXrD,OAAAC,eAAAf,KAAA,cAA+CqB,MAAA2I,IAG/CvH,WACA,OAAWwH,EAAUjK,KAAAgK,WAAA,IAAAvH,KAErBpB,YACA,OAAW4I,EAAUjK,KAAAgK,WAAA,IAAA3I,MAErB6I,eACA,OAAWD,EAAUjK,KAAAgK,WAAA,IAAAE,UCpCd,MAAAC,UAAAC,MACP/E,aAAAhD,OAAeA,EAAA8B,WACf4C,QACAjG,OAAAmG,iBAAAjH,KAAA,CACAqC,OAAA,CAAehB,MAAAgB,GACf8B,OAAA,CAAe9C,MAAA8C,MCDf,MAAMkG,UAAoCrD,EAI1C3B,aAAAsC,GACA,MAAAxD,EAAA,CAAoB0F,OAAAlC,EAAAG,QAAA,MACpBI,EAAA,IAAoBmC,EAA2B,CAAEhI,OAAAsF,EAAAtF,OAAA8B,WAejD,OAdAA,EAAA0F,SACA1F,EAAAmG,cAAA3C,EAAAG,QAAA,4CAEA3D,EAAAkE,KAAAV,EAAAG,QAAA,KACA3D,EAAAkE,MACAH,EAAAqC,KAAA,oBAAArC,EAAAsC,QCiGO,SAAA7C,GACP,MAAA8C,EAAAF,EAAA5C,EAAA,CAA+B+C,OAASC,EAAKD,OAAA/C,EAAA,cAAAiD,SAAA,oBAC7CH,EAAAlH,QACAoE,EAAAI,MAAA,uCAEA,OAAA0C,EDpGQI,CAAWlD,GAEXmD,EAAanD,GACrBxD,EAAA2E,MAAAnB,EAAAG,QAAA,MAAAH,EAAAI,MAAA,yDACKG,EAAA6C,SAAA5G,EAAAmG,eACL3C,EAAAI,MAAA,uDAEAG,EAGAsC,cACA,OAAAxK,KAAAmE,OAAA0F,OACA7J,KAAAmE,OAAAmG,cACAtK,KAAAmE,OAAAmG,cAAA7H,KADA,kBADA,MAMA,MAAMuI,UAAgChE,EAItC3B,aAAAsC,GACA,MAAAhH,EAAAgH,EAAAG,QAAA,cACA,GAAAnH,EACA,WAAiBqK,EAAuB,CACxC7G,OAAA,CAAiBxD,QACjBsK,OAAgBZ,EAA2Ba,MAAAvD,KAK3CtC,aAAAhD,OAAeA,EAAA8B,SAAA8G,WACflE,MAAA,CAAW1E,SAAA8B,WACXrD,OAAAC,eAAAf,KAAA,UAA2CqB,MAAA4J,IAG3CxI,WACA,2BAEA9B,WACA,OAAAX,KAAAmE,OAAAxD,KAAAU,MAEA8J,UACA,MAAWX,QAAA/H,EAAA0B,SAAAoG,QAA8BvK,KAAAiL,OACzC,OAAAxI,EAIA,CAAYA,OAAApB,MADZ,oBAAAoB,EAAA8H,EAAApG,EAAAmG,cAAAjJ,OAFA,KAKA+J,gBACA,MAAAZ,QAAWA,EAAAD,QAAgBvK,KAAAiL,OAC3B,OAAAV,GAAA,oBAAAC,EAGAD,EAFA,IAQO,MAAMc,UAA2BlB,EAIxC9E,aAAAsC,GACA,MAAAxD,EAAA,GAEA,GADAA,EAAAkE,KAAAV,EAAAG,QAAA,MACA3D,EAAAkE,KAAA,SACA,MAAAH,EAAA,IAAoBmD,EAAkB,CAAEhJ,OAAAsF,EAAAtF,OAAA8B,WAYxC,OAXA+D,EAAA7B,QAAgBkE,EAAI5C,EAAA,CACpB+C,OAAcM,EAAuBE,MACrCN,SAAA,wBAEAzG,EAAA2E,MAAAnB,EAAAG,QAAA,MAAAH,EAAAI,MAAA,kDACAG,EAAA3E,QACAoE,EAAAI,MAAA,qCAEAJ,EAAAb,MAAA,MACAa,EAAAI,MAAA,kEAEAG,GE7FO,MAAMoD,UAAiBtE,EAI9B3B,aAAAsC,GACA,MAAA4D,EAAA5D,EAAArF,SACA6B,EAAA,GACA+D,EAAA,IAAoBoD,EAAQ,CAAEjJ,OAAAsF,EAAAtF,OAAA8B,WAI9B,OAHA+D,EAAAmB,SAAmBgC,EAAkBH,MAAAvD,GACrCxD,EAAAqH,SAAA7D,EAAAG,QAAA,YACAI,EAAAW,QAAkBL,EAA6Bb,EAAA,iBAC/CO,EAAAW,SAGA1E,EAAAqH,WACArH,EAAAsH,SAAA9D,EAAAG,QAAA,QAEA3D,EAAAxD,KAAAgH,EAAAG,QAAA,gBAAqD9C,GACrDb,EAAAxD,MAGAuH,EAAAwD,QAAAvH,EAAAqH,SAAoC5B,EAAOsB,MAAAvD,GAAA,KAC3CO,GAHAP,EAAAgE,UAAAJ,IAPA5D,EAAAgE,UAAAJ,GAaAC,eACA,QAAAxL,KAAAmE,OAAAqH,SAEAC,eACA,QAAAzL,KAAAmE,OAAAsH,SAEA9K,WACA,OAAW6I,EAAQxJ,KAAAmE,OAAAxD,KAAAU,QCpCZ,MAAMsJ,UAAc3D,EAK3B3B,cAAAsC,EAAAlF,GACA,WACA,MAAApB,EAAAsG,EAAAG,QAAArF,GACA,GAAApB,EACA,WAAmBsJ,EAAK,CAAEtI,OAAAsF,EAAAtF,OAAA8B,OAAA,CAAoC9C,YAK9DA,YACA,OAAArB,KAAAmE,OAAA9C,aCjBO,SAAAuK,EAAA/C,EAAAgD,GACP,IAAAhD,EAAAU,MAAA,CACA,MAAAO,EAAA+B,EAAAC,OAAA7K,IAAA4H,WACA,QAAAiB,IAGA,YAAAA,EAAArH,KACAmJ,EAAA9B,EAAAjB,QAAAgD,GAEA,eAAA/B,EAAArH,MAEA,UAAA6F,KAAAO,EAAAP,QACA,GAAAsD,EAAAtD,EAAAuD,GACA,SAGA,SCXO,MAAME,UAAkB/E,EAI/B3B,aAAAsC,GAAAqE,QAA2BA,EAAAC,WAAmB,IAC9C,MAAA9H,EAAA,CAAoB6H,WACpB9D,EAAA,IAAoB6D,EAAS,CAAE1J,OAAAsF,EAAAtF,OAAA8B,WAC/B,OAAA6H,GAAA,gBAAAA,EAAA3K,QACA8C,EAAA+H,YAAAvE,EAAAG,QAAA,KACA3D,EAAA+H,cACAhE,EAAAkD,UAAA,GACAlD,IAGA8D,GAAAC,IACA9H,EAAA6H,QAAArE,EAAAG,QAAA,8BAEAI,EAAAW,QAAkBN,EAAWZ,MAAAI,MAAA,uBAC7B5D,EAAAxD,KAAAgH,EAAAG,QAAA,cACA3D,EAAAkE,KAAAV,EAAAG,QAAA,MAAAH,EAAAI,MAAA,qBACAG,EAAAkD,UAAoBN,EAAanD,GACjCxD,EAAA2E,MAAAnB,EAAAG,QAAA,MAAAH,EAAAI,MAAA,0BACA5D,EAAA+H,YAAAvE,EAAAG,QAAA,MAA6CH,EAAAI,MAAA,wCAC7CG,GAGAzF,WACA,kBAEA9B,WACA,MAAAA,KAAWA,GAAOX,KAAAmE,OAClB,OAAAxD,EAGW6I,EAAQ7I,EAAAU,OAFnB,GAIA2K,cACA,OAAAhM,KAAAmE,OAAA6H,QAGAhM,KAAAmE,OAAA6H,QAAA3K,MAFA,GAKAgE,UAAAwG,GACA,UAAAM,KAAAnM,KAAAoL,UACA,GAAUQ,EAAyBO,EAAAtD,QAAAgD,KACnCM,EAAAT,QAAA,CACA,MAAAlJ,EAAA,yEACgB4B,EAAepE,KAAAqC,OAAA8J,EAAAhI,OAAAxD,KAAAX,KAAAwC,KClDxB,MAAM4J,UAAkBpF,EAI/B3B,aAAAsC,GAAAqE,QAA2BA,EAAAK,aAAA,EAAAC,YAAA,GAA+C,IAC1E,MAAAf,EAAA5D,EAAArF,SACA6B,EAAA,CAAoB6H,WACpB9D,EAAA,IAAoBkE,EAAS,CAAE/J,OAAAsF,EAAAtF,OAAA8B,WAY/B,GAXA6H,GAAAK,IACAlI,EAAA6H,QAAArE,EAAAG,QAAA,YAEA,YAAAI,EAAA8D,SAAArE,EAAAb,MAAA,aACAa,EAAAI,MAAA,4CAEA5D,EAAAmI,SAAA3E,EAAAG,QAAA,YACAwE,IAAAnI,EAAAmI,UAAA3E,EAAAb,MAAA,cACAa,EAAAI,MAAA,+CAEA5D,EAAAgE,KAAAR,EAAAG,QAAA,aACA3D,EAAAgE,KAAA,CAKA,OADAD,EAAAW,QAAkBL,EAA6Bb,EAAA,mBAAAA,EAAAI,MAAA,0BAC/CG,EAAAW,QAAAI,SACA,eACA,aAAAtB,EAAAI,kCAAiEG,EAAAW,QAAAI,iBAIjE,OAFA9E,EAAAxD,KAAAgH,EAAAG,QAAA,0BAAAH,EAAAI,MAAA,0BACA5D,EAAA+H,YAAAvE,EAAAG,QAAA,MAA6CH,EAAAI,MAAA,wCAC7CG,EAVAP,EAAAgE,UAAAJ,GAaA9I,WACA,kBAEAuJ,cACA,OAAAhM,KAAAmE,OAAA6H,QAGAhM,KAAAmE,OAAA6H,QAAA3K,MAFA,GAIAiL,eACA,QAAAtM,KAAAmE,OAAAmI,SAEA3L,WACA,OAAW6I,EAAQxJ,KAAAmE,OAAAxD,KAAAU,QLvCZ,SAASmI,EAAQ9E,GACxB,OAAAA,EAAA0B,WAAA,KAAA1B,EAAA9B,MAAA,GAAA8B,EAWO,SAAA6F,EAAA5C,GAAA+C,OAA0BA,EAAA6B,eAAA3B,WAAA,SACjC,MAAA4B,EAAA9B,EAAA/C,GACA,IAAA6E,EACA,SAEAA,EAAArI,OAAAwE,UAAAhB,EAAAG,QAAA,KACA,MAAA2E,EAAA,CAAAD,GACA,KAAAA,EAAArI,OAAAwE,WAAA,CACA,MAAA+D,EAAAhC,EAAA/C,GACA,IAAA+E,EAAA,CACAH,GACA5E,EAAAI,2BAA6C6C,KAE7C,MAIA,GAFA8B,EAAAvI,OAAAwE,UAAAhB,EAAAG,QAAA,KACA2E,EAAApG,KAAAqG,IACAA,EAAAvI,OAAAwE,UAAA,MAEA,OAAA8D,EAMO,SAAA1C,EAAApC,GACP,OAAAA,EAAAG,QAAA,iEAQO,SAAAmC,GAAAxH,KAAqBA,EAAApB,UAC5B,OAAAoB,GACA,WACA,YACA,OAAcA,KAAA,UAAApB,MAAA,SAAAoB,GACd,eACA,gBACA,OAAcA,KAAA,WAAAyH,SAAAzH,EAAA2D,WAAA,MACd,QACA,OAAc3D,KAAA,WAAApB,MAAA,IACd,QACA,OAAcoB,KAAA,cACd,cACA,cACA,OAAcA,KAAA,SAAApB,SACd,aACA,OAAcoB,KAAA,SAAApB,QAAAuB,MAAA,OACd,QACA,OAAcH,SAOP,SAAAuG,EAAArB,GAoBP,MAAAtF,OAASA,GAASsF,EAClBgF,EApBA,WACA,MAAAlD,EAAA9B,EAAAG,QAAA,YACAK,EAAAR,EAAAG,QAAA,gBACA,GAAAK,EAAA,CACA,MAAAuB,EAAA/B,EAAAG,QAAA,QACA,WAAiBM,EAAI,CAAE/F,SAAA8B,OAAA,CAAkBsF,SAAAtB,OAAAuB,aAEzCD,GAAA9B,EAAAI,MAAA,gCAaA6E,IAVA,WACA,MAAAnD,EAAA9B,EAAAG,QAAA,gBACAK,EAAAR,EAAAG,QAAA,kBACA,GAAAK,EACA,WAAiBC,EAAI,CAAE/F,SAAA8B,OAAA,CAAkBsF,SAAAtB,UAEzCsB,GAAA9B,EAAAI,MAAA,8BAIA8E,GACA,GAAAF,EAAA,OAAAA,EACA,MAAAxE,EAAAR,EAAAG,QAAA,0BACA,OAAAK,EACA,IAAeC,EAAI,CAAE/F,SAAA8B,OAAA,CAAkBgE,eADvC,EAmBO,SAAA2C,EAAAnD,GACP,OAAA4C,EAAA5C,EAAA,CAA0B+C,OAASY,EAAQJ,MAAAN,SAAA,mBAOpC,SAAApC,EAAAb,EAAAM,GACP,MAAAoB,EAAmBgC,EAAkBH,MAAAvD,GACrCO,EAAcE,EAAI8C,MAAAvD,EAAAM,GAElB,OADAC,MAAAmB,YACAnB,EAOO,SAAAK,EAAAZ,EAAAM,GACP,MAAAiB,EAAcd,EAAI8C,MAAAvD,EAAAM,GAAA,eAClB,GAAAiB,EACA,OAAAA,EAEA,MAAA4D,EAAAnF,EAAAG,QAAA,QACA,GAAAgF,EAAA,CACA,MAAA5E,EAAA,IAAoBE,EAAI,CAAE/F,OAAAsF,EAAAtF,OAAA8B,OAAA,CAAoCgE,KAAA2E,KAE9D,OADA5E,EAAAzF,KAAA,cACAyF,GAOO,SAAA6E,EAAApF,GACP,MAAAqE,EAAArE,EAAAG,QAAA,eACA,GAAAkE,EAIA,OAHiBI,EAASlB,MAAAvD,EAAA,CAAmBqE,aACzCD,EAASb,MAAAvD,EAAA,CAAmBqE,aAChCrE,EAAAI,MAAA,4BMhKA,MAAMiF,UAAkBrC,EAIxBtF,aAAAsC,GACA,MAAAtG,EAAAsG,EAAAG,QAAA,UACA,GAAAzG,EACA,WAAiB2L,EAAS,CAAE3K,OAAAsF,EAAAtF,OAAA8B,OAAA,CAAoC9C,WAIhEoB,WACA,mBAEApB,YACA,OAAA0F,MAAA1F,MAAAuB,MAAA,OAIO,MAAMqK,UAAajG,EAI1B3B,aAAAsC,GACA,MAAAxD,EAAA,GAEA,GADAA,EAAAgE,KAAAR,EAAAG,QAAA,SACA3D,EAAAgE,KACA,OAEAhE,EAAAxD,KAAAgH,EAAAG,QAAA,eAAAH,EAAAI,MAAA,oBACA,MAAAG,EAAAP,EAAApF,QAAA,IAAwC0K,EAAI,CAAE5K,OAAAsF,EAAAtF,OAAA8B,WAe9C,OAdAA,EAAAkE,KAAAV,EAAAG,QAAA,MAAsCH,EAAAI,MAAA,iBACtCG,EAAAgF,OAAiB3C,EAAI5C,EAAA,CACrB+C,OAAcsC,EAAS9B,MACvBqB,cAAA,EACA3B,SAAA,gBAEAjD,EAAAb,MAAA,WACAa,EAAAI,MAAA,gCAEA5D,EAAA2E,MAAAnB,EAAAG,QAAA,MAAuCH,EAAAI,MAAA,4BACvCG,EAAAgF,OAAA3J,QACAoE,EAAAI,MAAA,oBAEA5D,EAAA+H,YAAAvE,EAAAG,QAAA,MAA6CH,EAAAI,MAAA,2BAC7CG,EAGAzF,WACA,aAEA9B,WACA,OAAW6I,EAAQxJ,KAAAmE,OAAAxD,KAAAU,QCrDZ,MAAM8L,UAAiBnG,EAI9B3B,aAAAsC,GACA,MAAAyF,EAAAzF,EAAAG,QAAA,cACA,IAAAsF,EACA,OAEA,MAAAjJ,EAAA,CAAoBiJ,UAEpB,GADAjJ,EAAA+B,SAAAyB,EAAAG,QAAA,YACA3D,EAAA+B,SAMA,OAFA/B,EAAAkJ,MAAA1F,EAAAG,QAAA,eAAAH,EAAAI,MAAA,iCACA5D,EAAA+H,YAAAvE,EAAAG,QAAA,MAA6CH,EAAAI,MAAA,2CAC7C,IAAeoF,EAAQ,CAAE9K,OAAAsF,EAAAtF,OAAA8B,WALzBwD,EAAAgE,UAAAyB,EAAA9I,OAQA7B,WACA,iBAEA2K,aACA,OAAW5D,EAAQxJ,KAAAmE,OAAAiJ,OAAA/L,OAEnB6E,eACA,OAAWsD,EAAQxJ,KAAAmE,OAAAkJ,MAAAhM,QC3BZ,MAAMiM,UAAgBtG,EAI7B3B,aAAAsC,GACA,MAAAxD,EAAA,GACA+D,EAAA,IAAoBoF,EAAO,CAAEjL,OAAAsF,EAAAtF,OAAA8B,WAE7B,GADAA,EAAAgE,KAAAR,EAAAG,QAAA,WACA3D,EAAAgE,KAOA,OAJAD,EAAAW,QAAkBL,EAA6Bb,EAAA,iBAAAA,EAAAI,MAAA,wBAC/C5D,EAAAxD,KAAAgH,EAAAG,QAAA,eAAAH,EAAAI,MAAA,wBACAJ,EAAApF,QAAA2F,EACA/D,EAAA+H,YAAAvE,EAAAG,QAAA,MAA6CH,EAAAI,MAAA,sCAC7CG,EAGAzF,WACA,gBAEA9B,WACA,OAAW6I,EAAQxJ,KAAAmE,OAAAxD,KAAAU,QCtBZ,MAAMkM,UAAyBvG,EAItC3B,aAAAsC,EAAAQ,GACA,MAAAhE,EAAA,CAAoBgE,QACpBD,EAAA,IAAoBqF,EAAgB,CAAElL,OAAAsF,EAAAtF,OAAA8B,WAStC,OARAA,EAAAxD,KAAAgH,EAAAG,QAAA,eAAAH,EAAAI,MAAA,yBACAJ,EAAApF,QAAA2F,EACA/D,EAAA0F,OAAAlC,EAAAG,QAAA,MAAAH,EAAAI,MAAA,gCACAG,EAAAW,QAAkBN,EAAWZ,MAAAI,MAAA,gCAC7B5D,EAAAkE,KAAAV,EAAAG,QAAA,MAAAH,EAAAI,MAAA,4CACAG,EAAAkD,UAAoBN,EAAanD,GACjCxD,EAAA2E,MAAAnB,EAAAG,QAAA,MAAAH,EAAAI,MAAA,yBACA5D,EAAA+H,YAAAvE,EAAAG,QAAA,MAA6CH,EAAAI,MAAA,uCAC7CG,EAGAzF,WACA,iBAEA9B,WACA,OAAW6I,EAAQxJ,KAAAmE,OAAAxD,KAAAU,QCTZ,MAAMmM,UAAkBxG,EAM/B3B,aAAAsC,EAAA8F,GAAAhL,KAAuCA,EAAAiL,cAAAC,mBACvC,MAAAxJ,OAAaA,GAASsJ,EAQtB,IAPAtJ,EAAAxD,KAAAgH,EAAAG,QAAA,eAAAH,EAAAI,yBAA0F0F,EAAAhL,QAC1FkF,EAAApF,QAAAkL,EACAC,GACA5M,OAAA+I,OAAA1F,EApBA,SAAAwD,GACA,MAAAiG,EAAAjG,EAAAG,QAAA,KACA,OAAA8F,EAIA,CAAUA,QAAAxG,YADVO,EAAAG,QAAA,eAAAH,EAAAI,MAAA,6BAFA,GAiBAX,CAAAO,IAEAxD,EAAAkE,KAAAV,EAAAG,QAAA,MAAwCH,EAAAI,kBAAkCtF,KAC1EgL,EAAAI,QAAA,KACA,CAEA,GADA1J,EAAA2E,MAAAnB,EAAAG,QAAA,KACA3D,EAAA2E,MAEA,OADA3E,EAAA+H,YAAAvE,EAAAG,QAAA,MAAmDH,EAAAI,iCAAiDtF,KACpGgL,EAEA,MAAAK,EAAmBzC,EAAkBH,MAAAvD,GACrC,IAAAoG,EACA,UAAArD,KAAAsD,KAAAL,EAEA,GADAI,EAAArD,EAAA/C,KAAAqG,GAEA,MAGAD,GACApG,EAAAI,MAAA,kBAEAgG,EAAA1E,SAAAyE,EACAL,EAAAI,QAAAxH,KAAA0H,IAIA9J,cACA,QAAAjE,KAAAmE,OAAAF,QAEAtD,WACA,OAAa6I,EAAQxJ,KAAAmE,OAAAxD,KAAAU,OAErB+F,kBACA,OAAApH,KAAAmE,OAAAiD,YAGaoC,EAAQxJ,KAAAmE,OAAAiD,YAAA/F,OAFrB,KAKAgE,UAAAwG,GACA,UAAAoC,KAAAjO,KAAA6N,QACAI,EAAAC,iBACAD,EAAAC,SAAArC,KCjEO,MAAMsC,UAAiBnH,EAI9B3B,aAAAsC,GACA,MAAAxD,EAAA,GAEA,GADAA,EAAAgE,KAAAR,EAAAG,QAAA,UACA3D,EAAAgE,KACA,OAEA,IAAAU,EAAkBG,EAAcrB,GAChC,IAAAkB,EAAA,CACA,MAAAV,EAAAR,EAAAG,QAAA,eAAAH,EAAAI,MAAA,sBACAc,EAAA,IAAoBT,EAAI,CAAE/F,OAAAsF,EAAAtF,OAAA8B,OAAA,CAAoCgE,UAE9DR,EAAAb,MAAA,MACAa,EAAAI,MAAA,qCAEAc,EAAApG,KAAA,aACA0B,EAAAxD,KAAAgH,EAAAG,QAAA,eAAAH,EAAAI,MAAA,sBACA5D,EAAA0F,OAAAlC,EAAAG,QAAA,MAAAH,EAAAI,MAAA,gCACA5D,EAAA9C,MAAmB0I,EAAWpC,MAAAI,MAAA,uBAC9B5D,EAAA+H,YAAAvE,EAAAG,QAAA,MAA6CH,EAAAI,MAAA,oCAC7C,MAAAG,EAAA,IAAoBiG,EAAQ,CAAE9L,OAAAsF,EAAAtF,OAAA8B,WAE9B,OADA+D,EAAAW,UACAX,EAGAzF,WACA,cAEA9B,WACA,OAAAyN,SAAApO,KAAAmE,OAAAxD,KAAAU,OAEAA,YACA,OAAW4I,EAAUjK,KAAAmE,OAAA9C,QCpCd,MAAMgN,UAAqBrH,EAIlC3B,aAAAsC,GACA,MAAA4D,EAAA5D,EAAArF,SACA6B,EAAA,GACA+D,EAAA,IAAoBmG,EAAY,CAAEhM,OAAAsF,EAAAtF,OAAA8B,WAKlC,GAJAA,EAAAmI,SAAA3E,EAAAG,QAAA,YACA3D,EAAAgE,KAAAhE,EAAAmI,SACA3E,EAAAG,QAAA,qBACAH,EAAAG,QAAA,iCACA3D,EAAAgE,KAEA,YADAR,EAAAgE,UAAAJ,GAIA,MAAA9I,KAAWA,GAAOyF,EAClBoG,EAAA,YAAA7L,EACA8L,EAAAD,GAAA,aAAA7L,EAEA0B,EAAAkE,KAAAV,EAAAG,QAAA,MAAAH,EAAAI,yCAA+FtF,iBAC/F,MAAA+J,EAAkBhE,EAA6Bb,MAAAI,oCAA6DtF,iBAa5G,OAZAyF,EAAAW,QAAA,CAAA2D,GACA+B,IACA/B,EAAArI,OAAAwE,UAAAhB,EAAAG,QAAA,KACA0E,EAAArI,OAAAwE,UACAT,EAAAW,QAAAxC,KAAyBmC,EAA6Bb,IAEtD2G,GACA3G,EAAAI,yCAAyDtF,kBAEzD0B,EAAA2E,MAAAnB,EAAAG,QAAA,MAAAH,EAAAI,4CAAmGtF,iBACnG0B,EAAA+H,YAAAvE,EAAAG,QAAA,MAA6CH,EAAAI,iCAAiDtF,iBAE9FyF,EAGAzF,WACA,OAAAzC,KAAAmE,OAAAgE,KAAA9G,MAEAiL,eACA,QAAAtM,KAAAmE,OAAAmI,UCjCA,SAAAkC,EAAA7G,GACA,MAAAqE,EAAArE,EAAAG,QAAA,UACA,GAAAkE,EAIA,OAHiBI,EAASlB,MAAAvD,EAAA,CAAmBqE,aACzCD,EAASb,MAAAvD,EAAA,CAAmBqE,aAChCrE,EAAAI,MAAA,4BAIO,MAAM0G,UAAkBjB,EAI/BnI,aAAAsC,EAAAQ,GAAAlE,QAAiCA,EAAA,MAAiB,IAClD,MAAAE,EAAA,CAAoBF,UAAAkE,QACpB,OAAWqF,EAAStC,MAAAvD,EAAA,IAAsB8G,EAAS,CAAEpM,OAAAsF,EAAAtF,OAAA8B,WAAmC,CACxF1B,KAAA,YACAiL,aAAAzJ,EACA0J,eAAA,CACA,CAASQ,EAAQjD,OACjB,CAAAsD,GACA,CAASzB,GACT,CAASsB,EAAYnD,OACrB,CAASkB,EAASlB,OAClB,CAASa,EAASb,UAKlBzI,WACA,kBAGA4C,UAAAwG,GACA,IAAA7L,KAAAiE,SAAAjE,KAAAqJ,SAAAqF,MAAAC,GAAA,YAAAA,EAAAhO,MAAA,CACA,MAAA6B,EAAA,gTAKY4B,EAAepE,KAAAqC,OAAArC,KAAAmE,OAAAxD,KAAAX,KAAAwC,SAE3BuE,MAAAmH,SAAArC,GACA7L,KAAAiE,gBCrDO,UAAA4H,EAAAzL,GACP,MAAAwO,EAAA,IAAAC,IAAAC,EAAA1O,GAAA+C,IAAA4L,KAAApO,OACAqO,EAAAnD,EAAAmD,SAAA/N,IAAAb,EAAAO,OAAA,GACAsO,EAAApD,EAAAqD,SAAAjO,IAAAb,EAAAO,OAAA,GACA,UAAAwO,IAAA,IAAAH,KAAAC,GAAA,CACA,MAAAG,EAAAN,EAAAK,SACAE,EAAAD,EAAAR,EAAAO,EAAA/O,GACA,UAAAkP,KAAAF,EACAR,EAAAW,IAAAD,EAAA3O,MAIA,SAAA0O,EAAAD,EAAAI,EAAAL,EAAAhH,GACA,UAAAmH,KAAAF,EAAA,CACA,MAAAzO,KAAaA,GAAO2O,EACpB,GAAA3O,GAAA6O,EAAAC,IAAA9O,GAAA,CACA,MAAA6B,oBAA0C7B,uDAA0DwH,EAAAxH,6CACtFyD,EAAe+K,EAAA9M,OAAAiN,EAAAnL,OAAAxD,KAAAwO,EAAA3M,KAK7B,SAAAsM,EAAA1O,GACA,OAAAA,EAAAyN,QACAlE,OAAA,EAAgBlH,UAAK,cAAAA,ID8BRiN,CAA+B7D,EAAA7L,QElDrC,MAAM2P,UAAcnC,EAI3BnI,aAAAsC,EAAAQ,GAAAlE,QAAiCA,GAAU,IAC3C,MAAAE,EAAA,CAAoBF,UAAAkE,QAEpB,GADAhE,EAAAkJ,MAAA1F,EAAAG,QAAA,SACA3D,EAAAkJ,MAGA,OAAWG,EAAStC,MAAAvD,EAAA,IAAsBgI,EAAK,CAAEtN,OAAAsF,EAAAtF,OAAA8B,WAAmC,CACpF1B,KAAA,kBACAkL,eAAA,CACA,CAASQ,EAAQjD,OACjB,CAAS6B,GACT,CAASX,EAASlB,MAAA,CAASmB,WAAA,IAC3B,CAASN,EAASb,MAAA,CAASe,SAAA,OAK3BxJ,WACA,yBCvBO,MAAMmN,UAAc5I,EAI3B3B,aAAAsC,GACA,MAAAxD,EAAA,GACA+D,EAAA,IAAoB0H,EAAK,CAAEvN,OAAAsF,EAAAtF,OAAA8B,WAQ3B,OAPA+D,EAAAmB,SAAmBgC,EAAkBH,MAAAvD,GACrCxD,EAAA0L,SAAAlI,EAAAG,QAAA,YACAI,EAAAW,QAAkBL,EAA6Bb,EAAA,oBAAAA,EAAAI,MAAA,kCAC/C5D,EAAAxD,KAAAgH,EAAAG,QAAA,eAAAH,EAAAI,MAAA,kCACAG,EAAAwD,QAAkB9B,EAAOsB,MAAAvD,GACzBxD,EAAA0L,UAAA3H,EAAAwD,SAAA/D,EAAAI,MAAA,2CACA5D,EAAA+H,YAAAvE,EAAAG,QAAA,MAA6CH,EAAAI,MAAA,gDAC7CG,EAGAzF,WACA,cAEA9B,WACA,OAAW6I,EAAQxJ,KAAAmE,OAAAxD,KAAAU,OAEnBwO,eACA,QAAA7P,KAAAmE,OAAA0L,UC1BO,MAAMC,UAAmBtC,EAIhCnI,aAAAsC,GAAA1D,QAA2BA,GAAU,IACrC,MAAAE,EAAA,CAAoBF,WAEpB,GADAE,EAAAgE,KAAAR,EAAAG,QAAA,cACA3D,EAAAgE,KAGA,OAAWqF,EAAStC,MAAAvD,EAAA,IAAsBmI,EAAU,CAAEzN,OAAAsF,EAAAtF,OAAA8B,WAAmC,CACzF1B,KAAA,aACAiL,aAAAzJ,EACA0J,eAAA,CACA,CAASiC,EAAK1E,UAKdzI,WACA,oBClBO,MAAMsN,UAAkBvC,EAI/BnI,aAAAsC,GAAA1D,QAA2BA,GAAU,IACrC,MAAAE,EAAA,CAAoBF,WAEpB,GADAE,EAAAgE,KAAAR,EAAAG,QAAA,aACA3D,EAAAgE,KAGA,OAAWqF,EAAStC,MAAAvD,EAAA,IAAsBoI,EAAS,CAAE1N,OAAAsF,EAAAtF,OAAA8B,WAAmC,CACxF1B,KAAA,YACAkL,eAAA,CACA,CAASvB,EAASlB,MAAA,CAASmB,WAAA,EAAAC,UAAA,IAC3B,CAASP,EAASb,MAAA,CAASe,SAAA,OAK3BxJ,WACA,kBAGA4C,UAAAwG,GACA,IAAA7L,KAAAiE,SAAAjE,KAAAqJ,SAAAqF,MAAAC,GAAA,YAAAA,EAAAhO,MAAA,CACA,MAAA6B,EAAA,gTAKY4B,EAAepE,KAAAqC,OAAArC,KAAAmE,OAAAxD,KAAAX,KAAAwC,SAE3BuE,MAAAmH,SAAArC,IChCO,MAAMmE,UAA0BxC,EAIvCnI,aAAAsC,EAAAsI,GAAAhM,QAAqCA,EAAA,MAAiB,IACtD,MAAAE,EAAA,CAAoB8L,YAEpB,GADA9L,EAAAgE,KAAAR,EAAAG,QAAA,aACA3D,EAAAgE,KAGA,OAAWqF,EAAStC,MAAAvD,EAAA,IAAsBqI,EAAiB,CAAE3N,OAAAsF,EAAAtF,OAAA8B,WAAmC,CAChG1B,KAAA,qBACAiL,aAAAzJ,EACA0J,eAAA,CACA,CAASQ,EAAQjD,OACjB,CAASa,EAASb,MAAA,CAASe,SAAA,OAK3BxJ,WACA,4BCPA,SAAAyN,EAAAvI,EAAAwI,GACA,MAAA9N,EAAAsF,EAAAtF,OAEA,SAAA0F,EAAAxC,GACAoC,EAAAI,MAAAxC,GAGA,SAAAuC,KAAAjB,GACA,OAAAc,EAAAG,WAAAjB,GAYA,SAAAuJ,EAAAC,GACA,MAAAlI,EAAAL,EAAA,aACA,GAAAK,EAIA,OAHgBwH,EAAKzE,MAAAvD,EAAAQ,EAAAkI,IACf5B,EAASvD,MAAAvD,EAAAQ,EAAAkI,IACftI,EAAA,gCAaA,SAAAuI,IACA,OA5BA,WACA,MAAAL,EAAAnI,EAAA,YACA,GAAAmI,EACA,OAAAtI,EAAAb,MAAA,aACakJ,EAAiB9E,MAAAvD,EAAAsI,GAEnB1C,EAAgBrC,MAAAvD,EAAAsI,GAsB3BA,IACAG,KAXA,WACA,MAAAnM,EAAA6D,EAAA,WACA,GAAA7D,EACA,OAAW6L,EAAU5E,MAAAvD,EAAA,CAAmB1D,aACxCmM,EAAA,CAAkBnM,aACZ8L,EAAS7E,MAAAvD,EAAA,CAAmB1D,aAClC8D,EAAA,qCAMA9D,IACM6L,EAAU5E,MAAAvD,IACVsF,EAAI/B,MAAAvD,IACJ2F,EAAOpC,MAAAvD,IACPwF,EAAQjC,MAAAvD,IACRoI,EAAS7E,MAAAvD,GAsBf,MAAA4I,EAnBA,WACA,IAAAlO,EAAAkB,OAAA,SACA,MAAAsI,EAAA,GACA,QACA,MAAAiC,EAAiBzC,EAAkBH,MAAAvD,GACnCmC,EAAAwG,IACA,IAAAxG,EAAA,CACAgE,EAAAvK,QAAAwE,EAAA,6BACA,MAEA+B,EAAAT,SAAAyE,EACAjC,EAAAxF,KAAAyD,GAEA,MAAA0G,EAAA1I,EAAA,OAIA,OAHAqI,EAAAM,UACA5E,EAAAxF,KAAAmK,GAEA3E,EAEA6E,GAEA,OADA/I,EAAArF,SAAAD,EAAAkB,QAAAwE,EAAA,uBACAwI,EAGO,SAAArF,EAAA3F,EAAA4K,EAAA,IAEP,OAAAD,EADA,IAAwB9K,EAASG,GACjC4K,GC5FA,SAAAQ,EAAAC,GACA,OAAAA,EAGA,MAAAC,EAAA,CACAC,KAAArE,KAAApJ,KAAA,IACAD,OAAAuN,EACAhQ,KAAAgQ,EACAI,UAAAJ,EACAlO,KAAAkO,EACA1H,QAAA0H,EACAvJ,YAAAuJ,EACAL,WAAAK,EACAK,kBAAAL,EACAM,2BAAAN,GAGO,SAAAO,GAAAC,GAAqBN,UAAAO,EAAAP,GAA4B,IAGxD,SAAAE,EAAAM,GAAAC,UAA2BA,EAAAC,YAI3B,OAHAD,IACAA,EAAAD,EAAAjL,WAAA,KAAAiL,EAAAzO,MAAA,GAAAyO,GAEAD,EAAAL,UAAAM,EAAAC,EAAAC,GAGA,SAAAlN,EAAA/C,EAAAkQ,EAAAb,KAAA3C,GACA,IAAA1M,EACA,SAEA,MAAAD,EAAAmQ,EAAAlQ,EAAAD,SAAA2M,GACA,OAAAoD,EAAAN,KAAA,CAAAM,EAAAhO,OAAA9B,EAAA8B,QAAA/B,IAGA,SAAAoQ,EAAAnQ,EAAAiQ,GACA,OAAAlN,EAAA/C,EAAAyP,EAAA,CAAgCQ,YAGhC,SAAAG,EAAApQ,EAAAsP,GACA,OAAAvM,EAAA/C,EAAA8P,EAAAzQ,KAAAiQ,GAGA,SAAAe,EAAAC,GACA,GAAAA,EAAArI,OAAAqI,EAAA3I,QACA,OAAAmI,EAAAN,KAAA,CACAzM,EAAAuN,EAAAzN,OAAAgE,KAAAiJ,EAAAnI,SACA5E,EAAAuN,EAAAzN,OAAAkE,SACAuJ,EAAAtJ,QAAAnF,IAAAV,GACA4B,EAAAuN,EAAAzN,OAAA2E,SAGA,MAAA+I,EAAAD,EAAAzN,OAAAsF,QAAAmI,EAAAzN,OAAAgE,KACAsB,EAAAmI,EAAAzN,OAAAsF,OAAA,CACAmI,EAAAzN,OAAAsF,OAAApI,MACA+P,EAAAhO,OAAAwO,EAAAzN,OAAAgE,KAAA/E,SACA,GACA0O,EAAAf,EAAAK,EAAAN,KAAA,IACArH,EACAmI,EAAAzN,OAAAgE,KAAA9G,MACAgD,EAAAuN,EAAAzN,OAAAuF,WACA,CAAS4H,UAAAM,EAAA/I,QAAA0I,QAAAK,IACT,OAAAR,EAAAN,KAAA,CAAAM,EAAAhO,OAAAyO,EAAAzO,QAAA0O,IAEA,SAAArP,EAAAmP,GACA,OAAAR,EAAAN,KAAA,CACAiB,EAAAH,EAAAvI,UACAsI,EAAAC,GACAvN,EAAAuN,EAAAzN,OAAA0D,UACAxD,EAAAuN,EAAAzN,OAAAwE,aAGA,SAAAqJ,EAAAlI,GACA,OAAAA,EAGAsH,EAAAN,KAAA,CACAzM,EAAAyF,EAAA3F,OAAA0F,WACAC,EAAAE,WAAA7G,IAAA7B,GAAA+C,EAAA/C,MAJA,GAOA,SAAA6K,EAAAyE,GACA,OAAAQ,EAAAN,KAAA,CACAiB,EAAAnB,EAAAvH,UACAhF,EAAAuM,EAAAzM,OAAAqH,UACA4F,EAAA3O,OAAAmO,EAAA/H,UACAxE,EAAAuM,EAAAzM,OAAAsH,UACAiG,EAAAd,EAAAzM,OAAAxD,KAAA,CAAmCsR,KAAArB,IACnCoB,EAAApB,EAAAlF,SACArH,EAAAuM,EAAAzM,OAAAwE,aASA,SAAAuJ,EAAAN,GACA,MAAApH,QAAWA,GAAUoH,EAAA3G,OACrB,OAAAmG,EAAAN,KAAA,CACAM,EAAAhO,OAAAwO,EAAAzN,OAAAxD,KAAAyC,QACAgO,EAAAJ,kBAAAI,EAAAN,KAAA,CACAM,EAAAH,2BAAAW,EAAAjR,MACA0D,EAAAuN,EAAA3G,OAAA9G,OAAA0F,QACA4H,EAAAG,EAAA3G,OAAA9G,OAAAmG,cAAAsH,GACAvN,EAAAuN,EAAA3G,OAAA9G,OAAAkE,SACAuJ,EAAA3G,OAAAV,KACAqH,EAAA3G,OAAAV,KAAApH,IACA,oBAAAqH,EAAA2H,IAjBA,SAAAA,EAAAZ,GACA,OAAAH,EAAAN,KAAA,CACAW,EAAAU,EAAAhO,OAAA9C,MAAAkQ,GACAlN,EAAA8N,EAAAhO,OAAAwE,cAcAjE,CAAAyN,EAAAP,GAAAzF,GAFA,GAIA9H,EAAAuN,EAAA3G,OAAA9G,OAAA2E,UAEAzE,EAAAuN,EAAAzN,OAAAwE,aAGA,SAAAoJ,EAAAK,GACA,OAAAA,EAAA7O,OACA6N,EAAAN,KAAA,CACAzM,EAAA+N,EAAAjO,OAAAkE,SACA+J,EAAAjP,IAAA+O,GACA7N,EAAA+N,EAAAjO,OAAA2E,SAJA,GA+CA,SAAAuJ,EAAAT,GACA,OAAAR,EAAAd,WAAAc,EAAAN,KAAA,CACAiB,EAAAH,EAAAvI,UACAhF,EAAAuN,EAAAzN,OAAA8L,UACA5L,EAAAuN,EAAAzN,OAAAF,SACAI,EAAAuN,EAAAzN,OAAAgE,MACA9D,EAAAuN,EAAAzN,OAAAkJ,OACAqE,EAAAE,EAAAzN,OAAAxD,KAAA,CAAkCsR,KAAAL,KAlBlCU,EAmBAV,EAlBAU,EAAAnO,OAAAiD,YAGAgK,EAAAN,KAAA,CACAzM,EAAAiO,EAAAnO,OAAAyJ,OACAwD,EAAAhO,OAAAkP,EAAAnO,OAAAiD,YAAAhE,QACAgO,EAAAhK,YAAA2J,EAAAuB,EAAAnO,OAAAiD,YAAA/F,MAAA,CAA8DkQ,QAAAe,OAL9D,IAkBAjO,EAAAuN,EAAAzN,OAAAkE,MACAkK,EAAAX,EAAA/D,QAAA+D,GACAvN,EAAAuN,EAAAzN,OAAA2E,OACAzE,EAAAuN,EAAAzN,OAAA+H,eACA,CAAS+F,KAAAL,IAxBT,IAAAU,EAoGA,SAAAE,EAAAZ,EAAAa,GACA,OAAArB,EAAAd,WAAAc,EAAAN,KAAA,CACAiB,EAAAH,EAAAvI,UACAhF,EAAAuN,EAAAzN,OAAAmI,UACAjI,EAAAuN,EAAAzN,OAAAgE,KAAAiJ,EAAAnI,SACA5E,EAAAuN,EAAAzN,OAAAkE,MACA+I,EAAAN,KAAAc,EAAA/I,QAAA1F,IAAAV,IACA4B,EAAAuN,EAAAzN,OAAA2E,OACAzE,EAAAuN,EAAAzN,OAAA+H,eACA,CAAS+F,KAAAL,EAAAa,WApPTrB,EAAAtQ,OAAA+I,OAAA,GAAuBgH,EAAAO,GA0PvB,MAAAsB,EAAA,CACAC,UAAAN,EACAO,kBAAAP,EACAQ,UAAAR,EACAS,UAnJA,SAAAlB,EAAAa,GACA,MAAAM,EAAAnB,EAAA/I,QAAA,CACAuI,EAAA3O,OAAAmP,EAAA/I,UACA6I,EAAAE,EAAAzN,OAAAxD,KAAA,CAAkCsR,KAAAL,EAAAa,WAClCpO,EAAAuN,EAAAzN,OAAAkE,MACA+I,EAAAN,KAAAc,EAAAxG,UAAAjI,IAAAgJ,IACA9H,EAAAuN,EAAAzN,OAAA2E,QACA,GACA,OAAAsI,EAAAd,WAAAc,EAAAN,KAAA,CACAiB,EAAAH,EAAAvI,UACAhF,EAAAuN,EAAAzN,OAAA6H,YACA+G,EACA1O,EAAAuN,EAAAzN,OAAA+H,eACA,CAAS+F,KAAAL,EAAAa,YAuITO,UApIA,SAAApB,EAAAa,GACA,OAAArB,EAAAd,WAAAc,EAAAN,KAAA,CACAiB,EAAAH,EAAAvI,UACAhF,EAAAuN,EAAAzN,OAAA6H,SACA3H,EAAAuN,EAAAzN,OAAAmI,UACAjI,EAAAuN,EAAAzN,OAAAgE,MACAiJ,EAAA3O,OAAAmP,EAAA/I,UACA6I,EAAAE,EAAAzN,OAAAxD,KAAA,CAAkCsR,KAAAL,EAAAa,WAClCpO,EAAAuN,EAAAzN,OAAA+H,eACA,CAAS+F,KAAAL,EAAAa,YA4HTQ,WAAAZ,EACAa,MA/FA,SAAAtB,EAAAa,GACA,OAAArB,EAAAd,WAAAc,EAAAN,KAAA,CACAiB,EAAAH,EAAAvI,UACAhF,EAAAuN,EAAAzN,OAAA0L,UACAuB,EAAA3O,OAAAmP,EAAA/I,UACA6I,EAAAE,EAAAzN,OAAAxD,KAAA,CAAkCsR,KAAAL,EAAAa,WAClCT,EAAAJ,EAAAlG,SACArH,EAAAuN,EAAAzN,OAAA+H,eACA,CAAS+F,KAAAL,EAAAa,YAwFTU,MAtFA,SAAAvB,EAAAa,GACA,OAAArB,EAAAd,WAAAc,EAAAN,KAAA,CACAiB,EAAAH,EAAAvI,UACAhF,EAAAuN,EAAAzN,OAAAgE,MACAiJ,EAAA3O,OAAAmP,EAAA/I,UACA6I,EAAAE,EAAAzN,OAAAxD,KAAA,CAAkCsR,KAAAL,EAAAa,WAClCpO,EAAAuN,EAAAzN,OAAA0F,QACAxF,EAAAuN,EAAAzN,OAAA9C,OACAgD,EAAAuN,EAAAzN,OAAA+H,eACA,CAAS+F,KAAAL,EAAAa,YA8ETW,QA5EA,SAAAxB,GACA,OAAAR,EAAAd,WAAAc,EAAAN,KAAA,CACAiB,EAAAH,EAAAvI,UACAhF,EAAAuN,EAAAzN,OAAAgE,MACAiJ,EAAA3O,OAAAmP,EAAA/I,UACA6I,EAAAE,EAAAzN,OAAAxD,KAAA,CAAkCsR,KAAAL,IAClCvN,EAAAuN,EAAAzN,OAAA+H,eACA,CAAS+F,KAAAL,KAsET1L,SApEA,SAAA0L,GACA,OAAAR,EAAAd,WAAAc,EAAAN,KAAA,CACAiB,EAAAH,EAAAvI,UACAoI,EAAAG,EAAAzN,OAAAiJ,OAAAwE,GACAvN,EAAAuN,EAAAzN,OAAA+B,UACAuL,EAAAG,EAAAzN,OAAAkJ,MAAAuE,GACAvN,EAAAuN,EAAAzN,OAAA+H,eACA,CAAS+F,KAAAL,KA8DT3B,SA5DA,SAAA2B,GACA,OAAAR,EAAAd,WAAAc,EAAAN,KAAA,CACAiB,EAAAH,EAAAvI,UACAhF,EAAAuN,EAAAzN,OAAAgE,MACAuJ,EAAAE,EAAAzN,OAAAxD,KAAA,CAAkCsR,KAAAL,IAClCvN,EAAAuN,EAAAzN,OAAA0F,QACAuH,EAAA3O,OAAAmP,EAAA/I,UACAxE,EAAAuN,EAAAzN,OAAAkE,SACAuJ,EAAAxG,UAAAjI,IAAAgJ,GACA9H,EAAAuN,EAAAzN,OAAA2E,OACAzE,EAAAuN,EAAAzN,OAAA+H,eACA,CAAS+F,KAAAL,KAkDTyB,KAhDA,SAAAzB,GACA,OAAAR,EAAAd,WAAAc,EAAAN,KAAA,CACAiB,EAAAH,EAAAvI,UACAhF,EAAAuN,EAAAzN,OAAAgE,MACAuJ,EAAAE,EAAAzN,OAAAxD,KAAA,CAAkCsR,KAAAL,IAClCvN,EAAAuN,EAAAzN,OAAAkE,MACAkK,EAAAX,EAAA1E,OAAA0E,GACAvN,EAAAuN,EAAAzN,OAAA2E,OACAzE,EAAAuN,EAAAzN,OAAA+H,eACA,CAAS+F,KAAAL,KAwCT0B,aAtCA,SAAAC,EAAAd,GACA,OAAArB,EAAAN,KAAA,CACAM,EAAAhO,OAAAmQ,EAAApP,OAAA9C,MAAA+B,QACAgO,EAAAd,WACAc,EAAAN,KAAA,KAAAM,EAAAzQ,KAAA4S,EAAAlS,MAAA,CAAwC4Q,KAAAsB,EAAAd,WAAkB,MAC1D,CAASR,KAAAsB,EAAAd,WAETpO,EAAAkP,EAAApP,OAAAwE,cAgCA6K,SAAAhB,EACAiB,eAAAjB,EACAkB,QAAAlB,EACAmB,QAAAnB,EACAoB,qBAAAvB,EACA7B,IAvBA,SAAAoB,GACA,OAAAR,EAAAhO,OAAAwO,EAAAxO,UA+BA,SAAAmP,EAAAsB,EAAApB,GACA,IAAAoB,EAAA,OACA,MAAAC,EAAAD,EAAA1Q,IAAA4Q,IATA,SAAAnC,EAAAa,GAEA,IADAC,EAAAd,EAAAnP,MAEA,UAAA6D,eAA+BsL,EAAAnP,wBAE/B,OAAAiQ,EAAAd,EAAAnP,MAAAmP,EAAAa,IAIAuB,CAAAD,EAAAtB,IACA,OAAArB,EAAAN,KAAAgD,GAEA,OAAAvB,EAAApB,GC3SA,SAAA8C,GAAAC,EAAApI,GACA,MAAA3I,EAAA,IAAAgR,IACAjO,EAAAgO,EAAAvK,OAAAG,GAAA,aAAAA,EAAArH,MACA,UAAA2R,KAAAlO,EAAA,CACA,MAAAmH,EAAAvB,EAAA7K,IAAAmT,EAAAlO,UACA,IAAAmH,EACA,SAEA,MAAAgH,EAAAlR,EAAAlC,IAAAmT,EAAAhH,QACAiH,EACAA,EAAAhO,KAAAgH,GAEAlK,EAAAmR,IAAAF,EAAAhH,OAAA,CAAAC,IAGA,OAAAlK,EA2CA,SAAAoR,GAAApD,GACA,MAAAtF,EAzCA,SAAAqI,GACA,MAAApI,EAAA,IAAAqI,IACAK,EAAA,IAAA3F,IACAG,EAAA,IAAAmF,IACA,UAAArK,KAAAoK,EACA,GAAApK,EAAA7F,QAAA,CACA,MAAAoQ,EAAArF,EAAA/N,IAAA6I,EAAAnJ,MACA0T,EACAA,EAAAhO,KAAAyD,GAEAkF,EAAAsF,IAAAxK,EAAAnJ,KAAA,CAAAmJ,SAIAA,EAAAnJ,OAGAmL,EAAA2D,IAAA3F,EAAAnJ,MAGA6T,EAAAjF,IAAAzF,GAFAgC,EAAAwI,IAAAxK,EAAAnJ,KAAAmJ,IAKA,OACAoK,MACApI,SACAkD,WACAwF,aACAtF,SAAA+E,GAAAC,EAAApI,IAaA2I,CAAAtD,GACA,UAAArH,KAAA+B,EAAAqI,IACApK,EAAAoE,iBACApE,EAAAoE,SAAArC,UAZA,WAAAC,OAAgCA,EAAA0I,eAChC,UAAAE,KAAAF,EAAA,CACA,MAAA7T,KAAWA,GAAO+T,EAClBlS,eAAiC7B,eAAkBmL,EAAA7K,IAAAN,GAAA8B,+BACzC2B,EAAKsQ,EAAArS,OAAAqS,EAAAvQ,OAAAxD,KAAA+T,EAAAlS,IAWfmS,CAAA9I,GAcO,SAAAqC,GAAAiD,GACP,UAAAoD,IAXAF,EAWAlD,EAVAkD,EAAAO,KACAP,EAAAO,OAEA,GAAA1P,UAAAmP,MAJA,IAAAA,ECzEAnU,EAAAQ,EAAAmU,EAAA,0BAAA3J,IAAAhL,EAAAQ,EAAAmU,EAAA,0BAAA3D,KAAAhR,EAAAQ,EAAAmU,EAAA,6BAAA3G","file":"webidl2.js","sourcesContent":["(function webpackUniversalModuleDefinition(root, factory) {\n\tif(typeof exports === 'object' && typeof module === 'object')\n\t\tmodule.exports = factory();\n\telse if(typeof define === 'function' && define.amd)\n\t\tdefine([], factory);\n\telse if(typeof exports === 'object')\n\t\texports[\"WebIDL2\"] = factory();\n\telse\n\t\troot[\"WebIDL2\"] = factory();\n})(this, function() {\nreturn "," \t// The module cache\n \tvar installedModules = {};\n\n \t// The require function\n \tfunction __webpack_require__(moduleId) {\n\n \t\t// Check if module is in cache\n \t\tif(installedModules[moduleId]) {\n \t\t\treturn installedModules[moduleId].exports;\n \t\t}\n \t\t// Create a new module (and put it into the cache)\n \t\tvar module = installedModules[moduleId] = {\n \t\t\ti: moduleId,\n \t\t\tl: false,\n \t\t\texports: {}\n \t\t};\n\n \t\t// Execute the module function\n \t\tmodules[moduleId].call(module.exports, module, module.exports, __webpack_require__);\n\n \t\t// Flag the module as loaded\n \t\tmodule.l = true;\n\n \t\t// Return the exports of the module\n \t\treturn module.exports;\n \t}\n\n\n \t// expose the modules object (__webpack_modules__)\n \t__webpack_require__.m = modules;\n\n \t// expose the module cache\n \t__webpack_require__.c = installedModules;\n\n \t// define getter function for harmony exports\n \t__webpack_require__.d = function(exports, name, getter) {\n \t\tif(!__webpack_require__.o(exports, name)) {\n \t\t\tObject.defineProperty(exports, name, { enumerable: true, get: getter });\n \t\t}\n \t};\n\n \t// define __esModule on exports\n \t__webpack_require__.r = function(exports) {\n \t\tif(typeof Symbol !== 'undefined' && Symbol.toStringTag) {\n \t\t\tObject.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });\n \t\t}\n \t\tObject.defineProperty(exports, '__esModule', { value: true });\n \t};\n\n \t// create a fake namespace object\n \t// mode & 1: value is a module id, require it\n \t// mode & 2: merge all properties of value into the ns\n \t// mode & 4: return value when already ns object\n \t// mode & 8|1: behave like require\n \t__webpack_require__.t = function(value, mode) {\n \t\tif(mode & 1) value = __webpack_require__(value);\n \t\tif(mode & 8) return value;\n \t\tif((mode & 4) && typeof value === 'object' && value && value.__esModule) return value;\n \t\tvar ns = Object.create(null);\n \t\t__webpack_require__.r(ns);\n \t\tObject.defineProperty(ns, 'default', { enumerable: true, value: value });\n \t\tif(mode & 2 && typeof value != 'string') for(var key in value) __webpack_require__.d(ns, key, function(key) { return value[key]; }.bind(null, key));\n \t\treturn ns;\n \t};\n\n \t// getDefaultExport function for compatibility with non-harmony modules\n \t__webpack_require__.n = function(module) {\n \t\tvar getter = module && module.__esModule ?\n \t\t\tfunction getDefault() { return module['default']; } :\n \t\t\tfunction getModuleExports() { return module; };\n \t\t__webpack_require__.d(getter, 'a', getter);\n \t\treturn getter;\n \t};\n\n \t// Object.prototype.hasOwnProperty.call\n \t__webpack_require__.o = function(object, property) { return Object.prototype.hasOwnProperty.call(object, property); };\n\n \t// __webpack_public_path__\n \t__webpack_require__.p = \"\";\n\n\n \t// Load entry module and return exports\n \treturn __webpack_require__(__webpack_require__.s = 0);\n","/**\n * @param {string} text\n */\nfunction lastLine(text) {\n  const splitted = text.split(\"\\n\");\n  return splitted[splitted.length - 1];\n}\n\n/**\n * @param {string} message error message\n * @param {\"Syntax\" | \"Validation\"} type error type\n */\nfunction error(source, position, current, message, type) {\n  /**\n   * @param {number} count\n   */\n  function sliceTokens(count) {\n    return count > 0 ?\n      source.slice(position, position + count) :\n      source.slice(Math.max(position + count, 0), position);\n  }\n\n  function tokensToText(inputs, { precedes } = {}) {\n    const text = inputs.map(t => t.trivia + t.value).join(\"\");\n    const nextToken = source[position];\n    if (nextToken.type === \"eof\") {\n      return text;\n    }\n    if (precedes) {\n      return text + nextToken.trivia;\n    }\n    return text.slice(nextToken.trivia.length);\n  }\n\n  const maxTokens = 5; // arbitrary but works well enough\n  const line =\n    source[position].type !== \"eof\" ? source[position].line :\n    source.length > 1 ? source[position - 1].line :\n    1;\n\n  const precedingLine = lastLine(\n    tokensToText(sliceTokens(-maxTokens), { precedes: true })\n  );\n\n  const subsequentTokens = sliceTokens(maxTokens);\n  const subsequentText = tokensToText(subsequentTokens);\n  const sobsequentLine = subsequentText.split(\"\\n\")[0];\n\n  const spaced = \" \".repeat(precedingLine.length) + \"^ \" + message;\n  const contextualMessage = precedingLine + sobsequentLine + \"\\n\" + spaced;\n\n  const contextType = type === \"Syntax\" ? \"since\" : \"inside\";\n  const grammaticalContext = current ? `, ${contextType} \\`${current.partial ? \"partial \" : \"\"}${current.type} ${current.name}\\`` : \"\";\n  return {\n    message: `${type} error at line ${line}${grammaticalContext}:\\n${contextualMessage}`,\n    line,\n    input: subsequentText,\n    tokens: subsequentTokens\n  };\n}\n\n/**\n * @param {string} message error message\n */\nexport function syntaxError(source, position, current, message) {\n  return error(source, position, current, message, \"Syntax\");\n}\n\n/**\n * @param {string} message error message\n */\nexport function validationError(source, token, current, message) {\n  return error(source, token.index, current, message, \"Validation\").message;\n}\n","import { syntaxError } from \"./error.js\";\n\n// These regular expressions use the sticky flag so they will only match at\n// the current location (ie. the offset of lastIndex).\nconst tokenRe = {\n  // This expression uses a lookahead assertion to catch false matches\n  // against integers early.\n  \"decimal\": /-?(?=[0-9]*\\.|[0-9]+[eE])(([0-9]+\\.[0-9]*|[0-9]*\\.[0-9]+)([Ee][-+]?[0-9]+)?|[0-9]+[Ee][-+]?[0-9]+)/y,\n  \"integer\": /-?(0([Xx][0-9A-Fa-f]+|[0-7]*)|[1-9][0-9]*)/y,\n  \"identifier\": /[_-]?[A-Za-z][0-9A-Z_a-z-]*/y,\n  \"string\": /\"[^\"]*\"/y,\n  \"whitespace\": /[\\t\\n\\r ]+/y,\n  \"comment\": /((\\/(\\/.*|\\*([^*]|\\*[^/])*\\*\\/)[\\t\\n\\r ]*)+)/y,\n  \"other\": /[^\\t\\n\\r 0-9A-Za-z]/y\n};\n\nexport const stringTypes = [\n  \"ByteString\",\n  \"DOMString\",\n  \"USVString\"\n];\n\nexport const argumentNameKeywords = [\n  \"attribute\",\n  \"callback\",\n  \"const\",\n  \"deleter\",\n  \"dictionary\",\n  \"enum\",\n  \"getter\",\n  \"includes\",\n  \"inherit\",\n  \"interface\",\n  \"iterable\",\n  \"maplike\",\n  \"namespace\",\n  \"partial\",\n  \"required\",\n  \"setlike\",\n  \"setter\",\n  \"static\",\n  \"stringifier\",\n  \"typedef\",\n  \"unrestricted\"\n];\n\nconst nonRegexTerminals = [\n  \"-Infinity\",\n  \"FrozenArray\",\n  \"Infinity\",\n  \"NaN\",\n  \"Promise\",\n  \"boolean\",\n  \"byte\",\n  \"double\",\n  \"false\",\n  \"float\",\n  \"implements\",\n  \"legacyiterable\",\n  \"long\",\n  \"mixin\",\n  \"null\",\n  \"octet\",\n  \"optional\",\n  \"or\",\n  \"readonly\",\n  \"record\",\n  \"sequence\",\n  \"short\",\n  \"true\",\n  \"unsigned\",\n  \"void\"\n].concat(argumentNameKeywords, stringTypes);\n\nconst punctuations = [\n  \"(\",\n  \")\",\n  \",\",\n  \"...\",\n  \":\",\n  \";\",\n  \"<\",\n  \"=\",\n  \">\",\n  \"?\",\n  \"[\",\n  \"]\",\n  \"{\",\n  \"}\"\n];\n\n/**\n * @param {string} str\n */\nfunction tokenise(str) {\n  const tokens = [];\n  let lastCharIndex = 0;\n  let trivia = \"\";\n  let line = 1;\n  let index = 0;\n  while (lastCharIndex < str.length) {\n    const nextChar = str.charAt(lastCharIndex);\n    let result = -1;\n\n    if (/[\\t\\n\\r ]/.test(nextChar)) {\n      result = attemptTokenMatch(\"whitespace\", { noFlushTrivia: true });\n    } else if (nextChar === '/') {\n      result = attemptTokenMatch(\"comment\", { noFlushTrivia: true });\n    }\n\n    if (result !== -1) {\n      const currentTrivia = tokens.pop().value;\n      line += (currentTrivia.match(/\\n/g) || []).length;\n      trivia += currentTrivia;\n      index -= 1;\n    } else if (/[-0-9.A-Z_a-z]/.test(nextChar)) {\n      result = attemptTokenMatch(\"decimal\");\n      if (result === -1) {\n        result = attemptTokenMatch(\"integer\");\n      }\n      if (result === -1) {\n        result = attemptTokenMatch(\"identifier\");\n        const token = tokens[tokens.length - 1];\n        if (result !== -1 && nonRegexTerminals.includes(token.value)) {\n          token.type = token.value;\n        }\n      }\n    } else if (nextChar === '\"') {\n      result = attemptTokenMatch(\"string\");\n    }\n\n    for (const punctuation of punctuations) {\n      if (str.startsWith(punctuation, lastCharIndex)) {\n        tokens.push({ type: punctuation, value: punctuation, trivia, line, index });\n        trivia = \"\";\n        lastCharIndex += punctuation.length;\n        result = lastCharIndex;\n        break;\n      }\n    }\n\n    // other as the last try\n    if (result === -1) {\n      result = attemptTokenMatch(\"other\");\n    }\n    if (result === -1) {\n      throw new Error(\"Token stream not progressing\");\n    }\n    lastCharIndex = result;\n    index += 1;\n  }\n\n  // remaining trivia as eof\n  tokens.push({\n    type: \"eof\",\n    value: \"\",\n    trivia\n  });\n\n  return tokens;\n\n  /**\n   * @param {keyof tokenRe} type\n   * @param {object} [options]\n   * @param {boolean} [options.noFlushTrivia]\n   */\n  function attemptTokenMatch(type, { noFlushTrivia } = {}) {\n    const re = tokenRe[type];\n    re.lastIndex = lastCharIndex;\n    const result = re.exec(str);\n    if (result) {\n      tokens.push({ type, value: result[0], trivia, line, index });\n      if (!noFlushTrivia) {\n        trivia = \"\";\n      }\n      return re.lastIndex;\n    }\n    return -1;\n  }\n}\n\nexport class Tokeniser {\n  /**\n   * @param {string} idl\n   */\n  constructor(idl) {\n    this.source = tokenise(idl);\n    this.position = 0;\n  }\n\n  /**\n   * @param {string} message\n   */\n  error(message) {\n    throw new WebIDLParseError(syntaxError(this.source, this.position, this.current, message));\n  }\n\n  /**\n   * @param {string} type\n   */\n  probe(type) {\n    return this.source.length > this.position && this.source[this.position].type === type;\n  }\n\n  /**\n   * @param  {...string} candidates\n   */\n  consume(...candidates) {\n    for (const type of candidates) {\n      if (!this.probe(type)) continue;\n      const token = this.source[this.position];\n      this.position++;\n      return token;\n    }\n  }\n\n  /**\n   * @param {number} position\n   */\n  unconsume(position) {\n    this.position = position;\n  }\n}\n\nclass WebIDLParseError extends Error {\n  constructor({ message, line, input, tokens }) {\n    super(message);\n    this.name = \"WebIDLParseError\"; // not to be mangled\n    this.line = line;\n    this.input = input;\n    this.tokens = tokens;\n  }\n}\n","export class Base {\r\n  constructor({ source, tokens }) {\r\n    Object.defineProperties(this, {\r\n      source: { value: source },\r\n      tokens: { value: tokens }\r\n    });\r\n  }\r\n\r\n  toJSON() {\r\n    const json = { type: undefined, name: undefined, inheritance: undefined };\r\n    let proto = this;\r\n    while (proto !== Object.prototype) {\r\n      const descMap = Object.getOwnPropertyDescriptors(proto);\r\n      for (const [key, value] of Object.entries(descMap)) {\r\n        if (value.enumerable || value.get) {\r\n          json[key] = this[key];\r\n        }\r\n      }\r\n      proto = Object.getPrototypeOf(proto);\r\n    }\r\n    return json;\r\n  }\r\n}\r\n","import { Base } from \"./base.js\";\nimport { unescape, type_with_extended_attributes, return_type, primitive_type } from \"./helpers.js\";\nimport { stringTypes } from \"../tokeniser.js\";\n\n/**\n * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n * @param {string} typeName\n */\nfunction generic_type(tokeniser, typeName) {\n  const base = tokeniser.consume(\"FrozenArray\", \"Promise\", \"sequence\", \"record\");\n  if (!base) {\n    return;\n  }\n  const ret = new Type({ source: tokeniser.source, tokens: { base } });\n  ret.tokens.open = tokeniser.consume(\"<\") || tokeniser.error(`No opening bracket after ${base.type}`);\n  switch (base.type) {\n    case \"Promise\": {\n      if (tokeniser.probe(\"[\")) tokeniser.error(\"Promise type cannot have extended attribute\");\n      const subtype = return_type(tokeniser, typeName) || tokeniser.error(\"Missing Promise subtype\");\n      ret.subtype.push(subtype);\n      break;\n    }\n    case \"sequence\":\n    case \"FrozenArray\": {\n      const subtype = type_with_extended_attributes(tokeniser, typeName) || tokeniser.error(`Missing ${base.type} subtype`);\n      ret.subtype.push(subtype);\n      break;\n    }\n    case \"record\": {\n      if (tokeniser.probe(\"[\")) tokeniser.error(\"Record key cannot have extended attribute\");\n      const keyType = tokeniser.consume(...stringTypes) || tokeniser.error(`Record key must be one of: ${stringTypes.join(\", \")}`);\n      const keyIdlType = new Type({ source: tokeniser.source, tokens: { base: keyType }});\n      keyIdlType.tokens.separator = tokeniser.consume(\",\") || tokeniser.error(\"Missing comma after record key type\");\n      keyIdlType.type = typeName;\n      const valueType = type_with_extended_attributes(tokeniser, typeName) || tokeniser.error(\"Error parsing generic type record\");\n      ret.subtype.push(keyIdlType, valueType);\n      break;\n    }\n  }\n  if (!ret.idlType) tokeniser.error(`Error parsing generic type ${base.type}`);\n  ret.tokens.close = tokeniser.consume(\">\") || tokeniser.error(`Missing closing bracket after ${base.type}`);\n  return ret;\n}\n\n/**\n * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n */\nfunction type_suffix(tokeniser, obj) {\n  const nullable = tokeniser.consume(\"?\");\n  if (nullable) {\n    obj.tokens.nullable = nullable;\n  }\n  if (tokeniser.probe(\"?\")) tokeniser.error(\"Can't nullable more than once\");\n}\n\n/**\n * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n * @param {string} typeName\n */\nfunction single_type(tokeniser, typeName) {\n  let ret = generic_type(tokeniser, typeName) || primitive_type(tokeniser);\n  if (!ret) {\n    const base = tokeniser.consume(\"identifier\", ...stringTypes);\n    if (!base) {\n      return;\n    }\n    ret = new Type({ source: tokeniser.source, tokens: { base } });\n    if (tokeniser.probe(\"<\")) tokeniser.error(`Unsupported generic type ${base.value}`);\n  }\n  if (ret.generic === \"Promise\" && tokeniser.probe(\"?\")) {\n    tokeniser.error(\"Promise type cannot be nullable\");\n  }\n  ret.type = typeName || null;\n  type_suffix(tokeniser, ret);\n  if (ret.nullable && ret.idlType === \"any\") tokeniser.error(\"Type `any` cannot be made nullable\");\n  return ret;\n}\n\n/**\n * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n * @param {string} type\n */\nfunction union_type(tokeniser, type) {\n  const tokens = {};\n  tokens.open = tokeniser.consume(\"(\");\n  if (!tokens.open) return;\n  const ret = new Type({ source: tokeniser.source, tokens });\n  ret.type = type || null;\n  while (true) {\n    const typ = type_with_extended_attributes(tokeniser) || tokeniser.error(\"No type after open parenthesis or 'or' in union type\");\n    if (typ.idlType === \"any\") tokeniser.error(\"Type `any` cannot be included in a union type\");\n    ret.subtype.push(typ);\n    const or = tokeniser.consume(\"or\");\n    if (or) {\n      typ.tokens.separator = or;\n    }\n    else break;\n  }\n  if (ret.idlType.length < 2) {\n    tokeniser.error(\"At least two types are expected in a union type but found less\");\n  }\n  tokens.close = tokeniser.consume(\")\") || tokeniser.error(\"Unterminated union type\");\n  type_suffix(tokeniser, ret);\n  return ret;\n}\n\nexport class Type extends Base {\n  /**\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n   * @param {string} typeName\n   */\n  static parse(tokeniser, typeName) {\n    return single_type(tokeniser, typeName) || union_type(tokeniser, typeName);\n  }\n\n  constructor({ source, tokens }) {\n    super({ source, tokens });\n    Object.defineProperty(this, \"subtype\", { value: [] });\n    this.extAttrs = [];\n  }\n\n  get generic() {\n    if (this.subtype.length && this.tokens.base) {\n      return this.tokens.base.value;\n    }\n    return \"\";\n  }\n  get nullable() {\n    return Boolean(this.tokens.nullable);\n  }\n  get union() {\n    return Boolean(this.subtype.length) && !this.tokens.base;\n  }\n  get idlType() {\n    if (this.subtype.length) {\n      return this.subtype;\n    }\n    // Adding prefixes/postfixes for \"unrestricted float\", etc.\n    const name = [\n      this.tokens.prefix,\n      this.tokens.base,\n      this.tokens.postfix\n    ].filter(t => t).map(t => t.value).join(\" \");\n    return unescape(name);\n  }\n}\n","import { Base } from \"./base.js\";\nimport { const_data, const_value } from \"./helpers.js\";\n\nexport class Default extends Base {\n  /**\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    const assign = tokeniser.consume(\"=\");\n    if (!assign) {\n      return null;\n    }\n    const def = const_value(tokeniser) || tokeniser.consume(\"string\", \"null\", \"[\", \"{\") || tokeniser.error(\"No value for default\");\n    const expression = [def];\n    if (def.type === \"[\") {\n      const close = tokeniser.consume(\"]\") || tokeniser.error(\"Default sequence value must be empty\");\n      expression.push(close);\n    } else if (def.type === \"{\") {\n      const close = tokeniser.consume(\"}\") || tokeniser.error(\"Default dictionary value must be empty\");\n      expression.push(close);\n    }\n    return new Default({ source: tokeniser.source, tokens: { assign }, expression });\n  }\n\n  constructor({ source, tokens, expression }) {\n    super({ source, tokens });\n    Object.defineProperty(this, \"expression\", { value: expression });\n  }\n\n  get type() {\n    return const_data(this.expression[0]).type;\n  }\n  get value() {\n    return const_data(this.expression[0]).value;\n  }\n  get negative() {\n    return const_data(this.expression[0]).negative;\n  }\n}\n","export class ArrayBase extends Array {\r\n  constructor({ source, tokens }) {\r\n    super();\r\n    Object.defineProperties(this, {\r\n      source: { value: source },\r\n      tokens: { value: tokens }\r\n    });\r\n  }\r\n}\r\n","import { Base } from \"./base.js\";\nimport { ArrayBase } from \"./array-base.js\";\nimport { list, identifiers, argument_list } from \"./helpers.js\";\n\nclass ExtendedAttributeParameters extends Base {\n  /**\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    const tokens = { assign: tokeniser.consume(\"=\") };\n    const ret = new ExtendedAttributeParameters({ source: tokeniser.source, tokens });\n    if (tokens.assign) {\n      tokens.secondaryName = tokeniser.consume(\"identifier\", \"decimal\", \"integer\", \"string\");\n    }\n    tokens.open = tokeniser.consume(\"(\");\n    if (tokens.open) {\n      ret.list = ret.rhsType === \"identifier-list\" ?\n        // [Exposed=(Window,Worker)]\n        identifiers(tokeniser) :\n        // [NamedConstructor=Audio(DOMString src)] or [Constructor(DOMString str)]\n        argument_list(tokeniser);\n      tokens.close = tokeniser.consume(\")\") || tokeniser.error(\"Unexpected token in extended attribute argument list\");\n    } else if (ret.hasRhs && !tokens.secondaryName) {\n      tokeniser.error(\"No right hand side to extended attribute assignment\");\n    }\n    return ret;\n  }\n\n  get rhsType() {\n    return !this.tokens.assign ? null :\n      !this.tokens.secondaryName ? \"identifier-list\" :\n        this.tokens.secondaryName.type;\n  }\n}\n\nclass SimpleExtendedAttribute extends Base {\n  /**\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    const name = tokeniser.consume(\"identifier\");\n    if (name) {\n      return new SimpleExtendedAttribute({\n        tokens: { name },\n        params: ExtendedAttributeParameters.parse(tokeniser)\n      });\n    }\n  }\n\n  constructor({ source, tokens, params }) {\n    super({ source, tokens });\n    Object.defineProperty(this, \"params\", { value: params });\n  }\n\n  get type() {\n    return \"extended-attribute\";\n  }\n  get name() {\n    return this.tokens.name.value;\n  }\n  get rhs() {\n    const { rhsType: type, tokens, list } = this.params;\n    if (!type) {\n      return null;\n    }\n    const value = type === \"identifier-list\" ? list : tokens.secondaryName.value;\n    return { type, value };\n  }\n  get arguments() {\n    const { rhsType, list } = this.params;\n    if (!list || rhsType === \"identifier-list\") {\n      return [];\n    }\n    return list;\n  }\n}\n\n// Note: we parse something simpler than the official syntax. It's all that ever\n// seems to be used\nexport class ExtendedAttributes extends ArrayBase {\n  /**\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    const tokens = {};\n    tokens.open = tokeniser.consume(\"[\");\n    if (!tokens.open) return [];\n    const ret = new ExtendedAttributes({ source: tokeniser.source, tokens });\n    ret.push(...list(tokeniser, {\n      parser: SimpleExtendedAttribute.parse,\n      listName: \"extended attribute\"\n    }));\n    tokens.close = tokeniser.consume(\"]\") || tokeniser.error(\"Unexpected closing token of extended attribute\");\n    if (!ret.length) {\n      tokeniser.error(\"Found an empty extended attribute\");\n    }\n    if (tokeniser.probe(\"[\")) {\n      tokeniser.error(\"Illegal double extended attribute lists, consider merging them\");\n    }\n    return ret;\n  }\n}\n","import { Type } from \"./type.js\";\nimport { Argument } from \"./argument.js\";\nimport { Token } from \"./token.js\";\nimport { ExtendedAttributes } from \"./extended-attributes.js\";\nimport { Operation } from \"./operation.js\";\nimport { Attribute } from \"./attribute.js\";\n\n/**\n * @param {string} identifier\n */\nexport function unescape(identifier) {\n  return identifier.startsWith('_') ? identifier.slice(1) : identifier;\n}\n\n/**\n * Parses comma-separated list\n * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n * @param {object} args\n * @param {Function} args.parser parser function for each item\n * @param {boolean} [args.allowDangler] whether to allow dangling comma\n * @param {string} [args.listName] the name to be shown on error messages\n */\nexport function list(tokeniser, { parser, allowDangler, listName = \"list\" }) {\n  const first = parser(tokeniser);\n  if (!first) {\n    return [];\n  }\n  first.tokens.separator = tokeniser.consume(\",\");\n  const items = [first];\n  while (first.tokens.separator) {\n    const item = parser(tokeniser);\n    if (!item) {\n      if (!allowDangler) {\n        tokeniser.error(`Trailing comma in ${listName}`);\n      }\n      break;\n    }\n    item.tokens.separator = tokeniser.consume(\",\");\n    items.push(item);\n    if (!item.tokens.separator) break;\n  }\n  return items;\n}\n\n/**\n * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n */\nexport function const_value(tokeniser) {\n  return tokeniser.consume(\"true\", \"false\", \"Infinity\", \"-Infinity\", \"NaN\", \"decimal\", \"integer\");\n}\n\n/**\n * @param {object} token\n * @param {string} token.type\n * @param {string} token.value\n */\nexport function const_data({ type, value }) {\n  switch (type) {\n    case \"true\":\n    case \"false\":\n      return { type: \"boolean\", value: type === \"true\" };\n    case \"Infinity\":\n    case \"-Infinity\":\n      return { type: \"Infinity\", negative: type.startsWith(\"-\") };\n    case \"[\":\n      return { type: \"sequence\", value: [] };\n    case \"{\":\n      return { type: \"dictionary\" };\n    case \"decimal\":\n    case \"integer\":\n      return { type: \"number\", value };\n    case \"string\":\n      return { type: \"string\", value: value.slice(1, -1) };\n    default:\n      return { type };\n  }\n}\n\n/**\n * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n */\nexport function primitive_type(tokeniser) {\n  function integer_type() {\n    const prefix = tokeniser.consume(\"unsigned\");\n    const base = tokeniser.consume(\"short\", \"long\");\n    if (base) {\n      const postfix = tokeniser.consume(\"long\");\n      return new Type({ source, tokens: { prefix, base, postfix } });\n    }\n    if (prefix) tokeniser.error(\"Failed to parse integer type\");\n  }\n\n  function decimal_type() {\n    const prefix = tokeniser.consume(\"unrestricted\");\n    const base = tokeniser.consume(\"float\", \"double\");\n    if (base) {\n      return new Type({ source, tokens: { prefix, base } });\n    }\n    if (prefix) tokeniser.error(\"Failed to parse float type\");\n  }\n\n  const { source } = tokeniser;\n  const num_type = integer_type(tokeniser) || decimal_type(tokeniser);\n  if (num_type) return num_type;\n  const base = tokeniser.consume(\"boolean\", \"byte\", \"octet\");\n  if (base) {\n    return new Type({ source, tokens: { base } });\n  }\n}\n\n/**\n * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n */\nexport function identifiers(tokeniser) {\n  const ids = list(tokeniser, { parser: Token.parser(tokeniser, \"identifier\"), listName: \"identifier list\" });\n  if (!ids.length) {\n    tokeniser.error(\"Expected identifiers but none found\");\n  }\n  return ids;\n}\n\n/**\n * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n */\nexport function argument_list(tokeniser) {\n  return list(tokeniser, { parser: Argument.parse, listName: \"arguments list\" });\n}\n\n/**\n * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n * @param {string} typeName\n */\nexport function type_with_extended_attributes(tokeniser, typeName) {\n  const extAttrs = ExtendedAttributes.parse(tokeniser);\n  const ret = Type.parse(tokeniser, typeName);\n  if (ret) ret.extAttrs = extAttrs;\n  return ret;\n}\n\n/**\n * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n * @param {string} typeName\n */\nexport function return_type(tokeniser, typeName) {\n  const typ = Type.parse(tokeniser, typeName || \"return-type\");\n  if (typ) {\n    return typ;\n  }\n  const voidToken = tokeniser.consume(\"void\");\n  if (voidToken) {\n    const ret = new Type({ source: tokeniser.source, tokens: { base: voidToken } });\n    ret.type = \"return-type\";\n    return ret;\n  }\n}\n\n/**\n * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n */\nexport function stringifier(tokeniser) {\n  const special = tokeniser.consume(\"stringifier\");\n  if (!special) return;\n  const member = Attribute.parse(tokeniser, { special }) ||\n    Operation.parse(tokeniser, { special }) ||\n    tokeniser.error(\"Unterminated stringifier\");\n  return member;\n}\n","import { Base } from \"./base.js\";\nimport { Default } from \"./default.js\";\nimport { ExtendedAttributes } from \"./extended-attributes.js\";\nimport { unescape, type_with_extended_attributes } from \"./helpers.js\";\nimport { argumentNameKeywords } from \"../tokeniser.js\";\n\nexport class Argument extends Base {\n  /**\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    const start_position = tokeniser.position;\n    const tokens = {};\n    const ret = new Argument({ source: tokeniser.source, tokens });\n    ret.extAttrs = ExtendedAttributes.parse(tokeniser);\n    tokens.optional = tokeniser.consume(\"optional\");\n    ret.idlType = type_with_extended_attributes(tokeniser, \"argument-type\");\n    if (!ret.idlType) {\n      return tokeniser.unconsume(start_position);\n    }\n    if (!tokens.optional) {\n      tokens.variadic = tokeniser.consume(\"...\");\n    }\n    tokens.name = tokeniser.consume(\"identifier\", ...argumentNameKeywords);\n    if (!tokens.name) {\n      return tokeniser.unconsume(start_position);\n    }\n    ret.default = tokens.optional ? Default.parse(tokeniser) : null;\n    return ret;\n  }\n\n  get optional() {\n    return !!this.tokens.optional;\n  }\n  get variadic() {\n    return !!this.tokens.variadic;\n  }\n  get name() {\n    return unescape(this.tokens.name.value);\n  }\n}\n","import { Base } from \"./base.js\";\r\n\r\nexport class Token extends Base {\r\n  /**\r\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\r\n   * @param {string} type\r\n   */\r\n  static parser(tokeniser, type) {\r\n    return () => {\r\n      const value = tokeniser.consume(type);\r\n      if (value) {\r\n        return new Token({ source: tokeniser.source, tokens: { value } });\r\n      }\r\n    };\r\n  }\r\n\r\n  get value() {\r\n    return this.tokens.value.value;\r\n  }\r\n}\r\n","export function idlTypeIncludesDictionary(idlType, defs) {\n  if (!idlType.union) {\n    const def = defs.unique.get(idlType.idlType);\n    if (!def) {\n      return false;\n    }\n    if (def.type === \"typedef\") {\n      return idlTypeIncludesDictionary(def.idlType, defs);\n    }\n    return def.type === \"dictionary\";\n  }\n  for (const subtype of idlType.subtype) {\n    if (idlTypeIncludesDictionary(subtype, defs)) {\n      return true;\n    }\n  }\n  return false;\n}\n","import { Base } from \"./base.js\";\nimport { return_type, argument_list, unescape } from \"./helpers.js\";\nimport { validationError } from \"../error.js\";\nimport { idlTypeIncludesDictionary } from \"../validators/helpers.js\";\n\nexport class Operation extends Base {\n  /**\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser, { special, regular } = {}) {\n    const tokens = { special };\n    const ret = new Operation({ source: tokeniser.source, tokens });\n    if (special && special.value === \"stringifier\") {\n      tokens.termination = tokeniser.consume(\";\");\n      if (tokens.termination) {\n        ret.arguments = [];\n        return ret;\n      }\n    }\n    if (!special && !regular) {\n      tokens.special = tokeniser.consume(\"getter\", \"setter\", \"deleter\");\n    }\n    ret.idlType = return_type(tokeniser) || tokeniser.error(\"Missing return type\");\n    tokens.name = tokeniser.consume(\"identifier\");\n    tokens.open = tokeniser.consume(\"(\") || tokeniser.error(\"Invalid operation\");\n    ret.arguments = argument_list(tokeniser);\n    tokens.close = tokeniser.consume(\")\") || tokeniser.error(\"Unterminated operation\");\n    tokens.termination = tokeniser.consume(\";\") || tokeniser.error(\"Unterminated operation, expected `;`\");\n    return ret;\n  }\n\n  get type() {\n    return \"operation\";\n  }\n  get name() {\n    const { name } = this.tokens;\n    if (!name) {\n      return \"\";\n    }\n    return unescape(name.value);\n  }\n  get special() {\n    if (!this.tokens.special) {\n      return \"\";\n    }\n    return this.tokens.special.value;\n  }\n\n  *validate(defs) {\n    for (const argument of this.arguments) {\n      if (idlTypeIncludesDictionary(argument.idlType, defs)) {\n        if (!argument.default) {\n          const message = `Optional dictionary arguments must have a default value of \\`{}\\`.`;\n          yield validationError(this.source, argument.tokens.name, this, message);\n        }\n      }\n    }\n  }\n}\n","import { Base } from \"./base.js\";\nimport { type_with_extended_attributes, unescape } from \"./helpers.js\";\n\nexport class Attribute extends Base {\n  /**\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser, { special, noInherit = false, readonly = false } = {}) {\n    const start_position = tokeniser.position;\n    const tokens = { special };\n    const ret = new Attribute({ source: tokeniser.source, tokens });\n    if (!special && !noInherit) {\n      tokens.special = tokeniser.consume(\"inherit\");\n    }\n    if (ret.special === \"inherit\" && tokeniser.probe(\"readonly\")) {\n      tokeniser.error(\"Inherited attributes cannot be read-only\");\n    }\n    tokens.readonly = tokeniser.consume(\"readonly\");\n    if (readonly && !tokens.readonly && tokeniser.probe(\"attribute\")) {\n      tokeniser.error(\"Attributes must be readonly in this context\");\n    }\n    tokens.base = tokeniser.consume(\"attribute\");\n    if (!tokens.base) {\n      tokeniser.unconsume(start_position);\n      return;\n    }\n    ret.idlType = type_with_extended_attributes(tokeniser, \"attribute-type\") || tokeniser.error(\"Attribute lacks a type\");\n    switch (ret.idlType.generic) {\n      case \"sequence\":\n      case \"record\": tokeniser.error(`Attributes cannot accept ${ret.idlType.generic} types`);\n    }\n    tokens.name = tokeniser.consume(\"identifier\", \"required\") || tokeniser.error(\"Attribute lacks a name\");\n    tokens.termination = tokeniser.consume(\";\") || tokeniser.error(\"Unterminated attribute, expected `;`\");\n    return ret;\n  }\n\n  get type() {\n    return \"attribute\";\n  }\n  get special() {\n    if (!this.tokens.special) {\n      return \"\";\n    }\n    return this.tokens.special.value;\n  }\n  get readonly() {\n    return !!this.tokens.readonly;\n  }\n  get name() {\n    return unescape(this.tokens.name.value);\n  }\n}\n","import { list, unescape } from \"./helpers.js\";\r\nimport { Token } from \"./token.js\";\r\nimport { Base } from \"./base.js\";\r\n\r\nclass EnumValue extends Token {\r\n  /**\r\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\r\n   */\r\n  static parse(tokeniser) {\r\n    const value = tokeniser.consume(\"string\");\r\n    if (value) {\r\n      return new EnumValue({ source: tokeniser.source, tokens: { value } });\r\n    }\r\n  }\r\n\r\n  get type() {\r\n    return \"enum-value\";\r\n  }\r\n  get value() {\r\n    return super.value.slice(1, -1);\r\n  }\r\n}\r\n\r\nexport class Enum extends Base {\r\n  /**\r\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\r\n   */\r\n  static parse(tokeniser) {\r\n    const tokens = {};\r\n    tokens.base = tokeniser.consume(\"enum\");\r\n    if (!tokens.base) {\r\n      return;\r\n    }\r\n    tokens.name = tokeniser.consume(\"identifier\") || tokeniser.error(\"No name for enum\");\r\n    const ret = tokeniser.current = new Enum({ source: tokeniser.source, tokens });\r\n    tokens.open = tokeniser.consume(\"{\") || tokeniser.error(\"Bodyless enum\");\r\n    ret.values = list(tokeniser, {\r\n      parser: EnumValue.parse,\r\n      allowDangler: true,\r\n      listName: \"enumeration\"\r\n    });\r\n    if (tokeniser.probe(\"string\")) {\r\n      tokeniser.error(\"No comma between enum values\");\r\n    }\r\n    tokens.close = tokeniser.consume(\"}\") || tokeniser.error(\"Unexpected value in enum\");\r\n    if (!ret.values.length) {\r\n      tokeniser.error(\"No value in enum\");\r\n    }\r\n    tokens.termination = tokeniser.consume(\";\") || tokeniser.error(\"No semicolon after enum\");\r\n    return ret;\r\n  }\r\n\r\n  get type() {\r\n    return \"enum\";\r\n  }\r\n  get name() {\r\n    return unescape(this.tokens.name.value);\r\n  }\r\n}\r\n","import { Base } from \"./base.js\";\r\nimport { unescape } from \"./helpers.js\";\r\n\r\nexport class Includes extends Base {\r\n  /**\r\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\r\n   */\r\n  static parse(tokeniser) {\r\n    const target = tokeniser.consume(\"identifier\");\r\n    if (!target) {\r\n      return;\r\n    }\r\n    const tokens = { target };\r\n    tokens.includes = tokeniser.consume(\"includes\");\r\n    if (!tokens.includes) {\r\n      tokeniser.unconsume(target.index);\r\n      return;\r\n    }\r\n    tokens.mixin = tokeniser.consume(\"identifier\") || tokeniser.error(\"Incomplete includes statement\");\r\n    tokens.termination = tokeniser.consume(\";\") || tokeniser.error(\"No terminating ; for includes statement\");\r\n    return new Includes({ source: tokeniser.source, tokens });\r\n  }\r\n\r\n  get type() {\r\n    return \"includes\";\r\n  }\r\n  get target() {\r\n    return unescape(this.tokens.target.value);\r\n  }\r\n  get includes() {\r\n    return unescape(this.tokens.mixin.value);\r\n  }\r\n}\r\n","import { Base } from \"./base\";\nimport { type_with_extended_attributes, unescape } from \"./helpers\";\n\nexport class Typedef extends Base {\n  /**\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    const tokens = {};\n    const ret = new Typedef({ source: tokeniser.source, tokens });\n    tokens.base = tokeniser.consume(\"typedef\");\n    if (!tokens.base) {\n      return;\n    }\n    ret.idlType = type_with_extended_attributes(tokeniser, \"typedef-type\") || tokeniser.error(\"Typedef lacks a type\");\n    tokens.name = tokeniser.consume(\"identifier\") || tokeniser.error(\"Typedef lacks a name\");\n    tokeniser.current = ret;\n    tokens.termination = tokeniser.consume(\";\") || tokeniser.error(\"Unterminated typedef, expected `;`\");\n    return ret;\n  }\n\n  get type() {\n    return \"typedef\";\n  }\n  get name() {\n    return unescape(this.tokens.name.value);\n  }\n}\n","import { Base } from \"./base\";\nimport { return_type, argument_list, unescape } from \"./helpers\";\n\nexport class CallbackFunction extends Base {\n  /**\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser, base) {\n    const tokens = { base };\n    const ret = new CallbackFunction({ source: tokeniser.source, tokens });\n    tokens.name = tokeniser.consume(\"identifier\") || tokeniser.error(\"Callback lacks a name\");\n    tokeniser.current = ret;\n    tokens.assign = tokeniser.consume(\"=\") || tokeniser.error(\"Callback lacks an assignment\");\n    ret.idlType = return_type(tokeniser) || tokeniser.error(\"Callback lacks a return type\");\n    tokens.open = tokeniser.consume(\"(\") || tokeniser.error(\"Callback lacks parentheses for arguments\");\n    ret.arguments = argument_list(tokeniser);\n    tokens.close = tokeniser.consume(\")\") || tokeniser.error(\"Unterminated callback\");\n    tokens.termination = tokeniser.consume(\";\") || tokeniser.error(\"Unterminated callback, expected `;`\");\n    return ret;\n  }\n\n  get type() {\n    return \"callback\";\n  }\n  get name() {\n    return unescape(this.tokens.name.value);\n  }\n}\n","import { Base } from \"./base.js\";\nimport { ExtendedAttributes } from \"./extended-attributes.js\";\nimport { unescape } from \"./helpers.js\";\n\n/**\n * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n */\nfunction inheritance(tokeniser) {\n  const colon = tokeniser.consume(\":\");\n  if (!colon) {\n    return {};\n  }\n  const inheritance = tokeniser.consume(\"identifier\") || tokeniser.error(\"Inheritance lacks a type\");\n  return { colon, inheritance };\n}\n\nexport class Container extends Base {\n    /**\n     * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n     * @param {*} instance\n     * @param {*} args\n     */\n    static parse(tokeniser, instance, { type, inheritable, allowedMembers }) {\n      const { tokens } = instance;\n      tokens.name = tokeniser.consume(\"identifier\") || tokeniser.error(`Missing name in ${instance.type}`);\n      tokeniser.current = instance;\n      if (inheritable) {\n        Object.assign(tokens, inheritance(tokeniser));\n      }\n      tokens.open = tokeniser.consume(\"{\") || tokeniser.error(`Bodyless ${type}`);\n      instance.members = [];\n      while (true) {\n        tokens.close = tokeniser.consume(\"}\");\n        if (tokens.close) {\n          tokens.termination = tokeniser.consume(\";\") || tokeniser.error(`Missing semicolon after ${type}`);\n          return instance;\n        }\n        const ea = ExtendedAttributes.parse(tokeniser);\n        let mem;\n        for (const [parser, ...args] of allowedMembers) {\n          mem = parser(tokeniser, ...args);\n          if (mem) {\n            break;\n          }\n        }\n        if (!mem) {\n          tokeniser.error(\"Unknown member\");\n        }\n        mem.extAttrs = ea;\n        instance.members.push(mem);\n      }\n    }\n\n    get partial() {\n      return !!this.tokens.partial;\n    }\n    get name() {\n      return unescape(this.tokens.name.value);\n    }\n    get inheritance() {\n      if (!this.tokens.inheritance) {\n        return null;\n      }\n      return unescape(this.tokens.inheritance.value);\n    }\n\n    *validate(defs) {\n      for (const member of this.members) {\n        if (member.validate) {\n          yield* member.validate(defs);\n        }\n      }\n    }\n  }\n","import { Base } from \"./base.js\";\nimport { Type } from \"./type.js\";\nimport { const_data, const_value, primitive_type } from \"./helpers.js\";\n\nexport class Constant extends Base {\n  /**\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    const tokens = {};\n    tokens.base = tokeniser.consume(\"const\");\n    if (!tokens.base) {\n      return;\n    }\n    let idlType = primitive_type(tokeniser);\n    if (!idlType) {\n      const base = tokeniser.consume(\"identifier\") || tokeniser.error(\"Const lacks a type\");\n      idlType = new Type({ source: tokeniser.source, tokens: { base } });\n    }\n    if (tokeniser.probe(\"?\")) {\n      tokeniser.error(\"Unexpected nullable constant type\");\n    }\n    idlType.type = \"const-type\";\n    tokens.name = tokeniser.consume(\"identifier\") || tokeniser.error(\"Const lacks a name\");\n    tokens.assign = tokeniser.consume(\"=\") || tokeniser.error(\"Const lacks value assignment\");\n    tokens.value = const_value(tokeniser) || tokeniser.error(\"Const lacks a value\");\n    tokens.termination = tokeniser.consume(\";\") || tokeniser.error(\"Unterminated const, expected `;`\");\n    const ret = new Constant({ source: tokeniser.source, tokens });\n    ret.idlType = idlType;\n    return ret;\n  }\n\n  get type() {\n    return \"const\";\n  }\n  get name() {\n    return unescape(this.tokens.name.value);\n  }\n  get value() {\n    return const_data(this.tokens.value);\n  }\n}\n","import { Base } from \"./base\";\nimport { type_with_extended_attributes } from \"./helpers\";\n\nexport class IterableLike extends Base {\n  /**\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    const start_position = tokeniser.position;\n    const tokens = {};\n    const ret = new IterableLike({ source: tokeniser.source, tokens });\n    tokens.readonly = tokeniser.consume(\"readonly\");\n    tokens.base = tokens.readonly ?\n      tokeniser.consume(\"maplike\", \"setlike\") :\n      tokeniser.consume(\"iterable\", \"maplike\", \"setlike\");\n    if (!tokens.base) {\n      tokeniser.unconsume(start_position);\n      return;\n    }\n\n    const { type } = ret;\n    const secondTypeRequired = type === \"maplike\";\n    const secondTypeAllowed = secondTypeRequired || type === \"iterable\";\n\n    tokens.open = tokeniser.consume(\"<\") || tokeniser.error(`Missing less-than sign \\`<\\` in ${type} declaration`);\n    const first = type_with_extended_attributes(tokeniser) || tokeniser.error(`Missing a type argument in ${type} declaration`);\n    ret.idlType = [first];\n    if (secondTypeAllowed) {\n      first.tokens.separator = tokeniser.consume(\",\");\n      if (first.tokens.separator) {\n        ret.idlType.push(type_with_extended_attributes(tokeniser));\n      }\n      else if (secondTypeRequired)\n      tokeniser.error(`Missing second type argument in ${type} declaration`);\n    }\n    tokens.close = tokeniser.consume(\">\") || tokeniser.error(`Missing greater-than sign \\`>\\` in ${type} declaration`);\n    tokens.termination = tokeniser.consume(\";\") || tokeniser.error(`Missing semicolon after ${type} declaration`);\n\n    return ret;\n  }\n\n  get type() {\n    return this.tokens.base.value;\n  }\n  get readonly() {\n    return !!this.tokens.readonly;\n  }\n}\n","import { Container } from \"./container.js\";\nimport { Attribute } from \"./attribute.js\";\nimport { Operation } from \"./operation.js\";\nimport { Constant } from \"./constant.js\";\nimport { IterableLike } from \"./iterable.js\";\nimport { stringifier } from \"./helpers.js\";\nimport { validationError } from \"../error.js\";\nimport { checkInterfaceMemberDuplication } from \"../validators/interface.js\";\n\n/**\n * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n */\nfunction static_member(tokeniser) {\n  const special = tokeniser.consume(\"static\");\n  if (!special) return;\n  const member = Attribute.parse(tokeniser, { special }) ||\n    Operation.parse(tokeniser, { special }) ||\n    tokeniser.error(\"No body in static member\");\n  return member;\n}\n\nexport class Interface extends Container {\n  /**\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser, base, { partial = null } = {}) {\n    const tokens = { partial, base };\n    return Container.parse(tokeniser, new Interface({ source: tokeniser.source, tokens }), {\n      type: \"interface\",\n      inheritable: !partial,\n      allowedMembers: [\n        [Constant.parse],\n        [static_member],\n        [stringifier],\n        [IterableLike.parse],\n        [Attribute.parse],\n        [Operation.parse]\n      ]\n    });\n  }\n\n  get type() {\n    return \"interface\";\n  }\n\n  *validate(defs) {\n    if (!this.partial && this.extAttrs.every(extAttr => extAttr.name !== \"Exposed\")) {\n      const message = `Interfaces must have [Exposed] extended attribute. \\\nTo fix, add, for example, [Exposed=Window]. Please also consider carefully \\\nif your interface should also be exposed in a Worker scope. Refer to the \\\n[WebIDL spec section on Exposed](https://heycam.github.io/webidl/#Exposed) \\\nfor more information.`;\n      yield validationError(this.source, this.tokens.name, this, message);\n    }\n    yield* super.validate(defs);\n    if (!this.partial) {\n      yield* checkInterfaceMemberDuplication(defs, this);\n    }\n  }\n}\n","import { validationError } from \"../error.js\";\n\nexport function* checkInterfaceMemberDuplication(defs, i) {\n  const opNames = new Set(getOperations(i).map(op => op.name));\n  const partials = defs.partials.get(i.name) || [];\n  const mixins = defs.mixinMap.get(i.name) || [];\n  for (const ext of [...partials, ...mixins]) {\n    const additions = getOperations(ext);\n    yield* forEachExtension(additions, opNames, ext, i);\n    for (const addition of additions) {\n      opNames.add(addition.name);\n    }\n  }\n\n  function* forEachExtension(additions, existings, ext, base) {\n    for (const addition of additions) {\n      const { name } = addition;\n      if (name && existings.has(name)) {\n        const message = `The operation \"${name}\" has already been defined for the base interface \"${base.name}\" either in itself or in a mixin`;\n        yield validationError(ext.source, addition.tokens.name, ext, message);\n      }\n    }\n  }\n\n  function getOperations(i) {\n    return i.members\n      .filter(({type}) => type === \"operation\");\n  }\n}\n","import { Container } from \"./container.js\";\nimport { Constant } from \"./constant.js\";\nimport { Attribute } from \"./attribute.js\";\nimport { Operation } from \"./operation.js\";\nimport { stringifier } from \"./helpers.js\";\n\nexport class Mixin extends Container {\n  /**\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser, base, { partial } = {}) {\n    const tokens = { partial, base };\n    tokens.mixin = tokeniser.consume(\"mixin\");\n    if (!tokens.mixin) {\n      return;\n    }\n    return Container.parse(tokeniser, new Mixin({ source: tokeniser.source, tokens }), {\n      type: \"interface mixin\",\n      allowedMembers: [\n        [Constant.parse],\n        [stringifier],\n        [Attribute.parse, { noInherit: true }],\n        [Operation.parse, { regular: true }]\n      ]\n    });\n  }\n\n  get type() {\n    return \"interface mixin\";\n  }\n}\n","import { Base } from \"./base.js\";\nimport { unescape, type_with_extended_attributes } from \"./helpers.js\";\nimport { ExtendedAttributes } from \"./extended-attributes.js\";\nimport { Default } from \"./default.js\";\n\nexport class Field extends Base {\n  /**\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    const tokens = {};\n    const ret = new Field({ source: tokeniser.source, tokens });\n    ret.extAttrs = ExtendedAttributes.parse(tokeniser);\n    tokens.required = tokeniser.consume(\"required\");\n    ret.idlType = type_with_extended_attributes(tokeniser, \"dictionary-type\") || tokeniser.error(\"Dictionary member lacks a type\");\n    tokens.name = tokeniser.consume(\"identifier\") || tokeniser.error(\"Dictionary member lacks a name\");\n    ret.default = Default.parse(tokeniser);\n    if (tokens.required && ret.default) tokeniser.error(\"Required member must not have a default\");\n    tokens.termination = tokeniser.consume(\";\") || tokeniser.error(\"Unterminated dictionary member, expected `;`\");\n    return ret;\n  }\n\n  get type() {\n    return \"field\";\n  }\n  get name() {\n    return unescape(this.tokens.name.value);\n  }\n  get required() {\n    return !!this.tokens.required;\n  }\n}\n","import { Container } from \"./container.js\";\nimport { Field } from \"./field.js\";\n\nexport class Dictionary extends Container {\n  /**\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser, { partial } = {}) {\n    const tokens = { partial };\n    tokens.base = tokeniser.consume(\"dictionary\");\n    if (!tokens.base) {\n      return;\n    }\n    return Container.parse(tokeniser, new Dictionary({ source: tokeniser.source, tokens }), {\n      type: \"dictionary\",\n      inheritable: !partial,\n      allowedMembers: [\n        [Field.parse],\n      ]\n    });\n  }\n\n  get type() {\n    return \"dictionary\";\n  }\n}\n","import { Container } from \"./container.js\";\nimport { Attribute } from \"./attribute.js\";\nimport { Operation } from \"./operation.js\";\nimport { validationError } from \"../error.js\";\n\nexport class Namespace extends Container {\n  /**\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser, { partial } = {}) {\n    const tokens = { partial };\n    tokens.base = tokeniser.consume(\"namespace\");\n    if (!tokens.base) {\n      return;\n    }\n    return Container.parse(tokeniser, new Namespace({ source: tokeniser.source, tokens }), {\n      type: \"namespace\",\n      allowedMembers: [\n        [Attribute.parse, { noInherit: true, readonly: true }],\n        [Operation.parse, { regular: true }]\n      ]\n    });\n  }\n\n  get type() {\n    return \"namespace\";\n  }\n\n  *validate(defs) {\n    if (!this.partial && this.extAttrs.every(extAttr => extAttr.name !== \"Exposed\")) {\n      const message = `Namespaces must have [Exposed] extended attribute. \\\nTo fix, add, for example, [Exposed=Window]. Please also consider carefully \\\nif your namespace should also be exposed in a Worker scope. Refer to the \\\n[WebIDL spec section on Exposed](https://heycam.github.io/webidl/#Exposed) \\\nfor more information.`;\n      yield validationError(this.source, this.tokens.name, this, message);\n    }\n    yield* super.validate(defs);\n  }\n}\n","import { Container } from \"./container.js\";\nimport { Operation } from \"./operation.js\";\nimport { Constant } from \"./constant.js\";\n\n\nexport class CallbackInterface extends Container {\n  /**\n   * @param {import(\"../tokeniser\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser, callback, { partial = null } = {}) {\n    const tokens = { callback };\n    tokens.base = tokeniser.consume(\"interface\");\n    if (!tokens.base) {\n      return;\n    }\n    return Container.parse(tokeniser, new CallbackInterface({ source: tokeniser.source, tokens }), {\n      type: \"callback interface\",\n      inheritable: !partial,\n      allowedMembers: [\n        [Constant.parse],\n        [Operation.parse, { regular: true }]\n      ]\n    });\n  }\n\n  get type() {\n    return \"callback interface\";\n  }\n}\n","\"use strict\";\n\nimport { Tokeniser } from \"./tokeniser.js\";\nimport { Enum } from \"./productions/enum.js\";\nimport { Includes } from \"./productions/includes.js\";\nimport { ExtendedAttributes } from \"./productions/extended-attributes.js\";\nimport { Typedef } from \"./productions/typedef.js\";\nimport { CallbackFunction } from \"./productions/callback.js\";\nimport { Interface } from \"./productions/interface.js\";\nimport { Mixin } from \"./productions/mixin.js\";\nimport { Dictionary } from \"./productions/dictionary.js\";\nimport { Namespace } from \"./productions/namespace.js\";\nimport { CallbackInterface } from \"./productions/callback-interface.js\";\n\n/**\n * @param {Tokeniser} tokeniser\n * @param {object} options\n * @param {boolean} [options.concrete]\n */\nfunction parseByTokens(tokeniser, options) {\n  const source = tokeniser.source;\n\n  function error(str) {\n    tokeniser.error(str);\n  }\n\n  function consume(...candidates) {\n    return tokeniser.consume(...candidates);\n  }\n\n  function callback() {\n    const callback = consume(\"callback\");\n    if (!callback) return;\n    if (tokeniser.probe(\"interface\")) {\n      return CallbackInterface.parse(tokeniser, callback);\n    }\n    return CallbackFunction.parse(tokeniser, callback);\n  }\n\n  function interface_(opts) {\n    const base = consume(\"interface\");\n    if (!base) return;\n    const ret = Mixin.parse(tokeniser, base, opts) ||\n      Interface.parse(tokeniser, base, opts) ||\n      error(\"Interface has no proper body\");\n    return ret;\n  }\n\n  function partial() {\n    const partial = consume(\"partial\");\n    if (!partial) return;\n    return Dictionary.parse(tokeniser, { partial }) ||\n      interface_({ partial }) ||\n      Namespace.parse(tokeniser, { partial }) ||\n      error(\"Partial doesn't apply to anything\");\n  }\n\n  function definition() {\n    return callback() ||\n      interface_() ||\n      partial() ||\n      Dictionary.parse(tokeniser) ||\n      Enum.parse(tokeniser) ||\n      Typedef.parse(tokeniser) ||\n      Includes.parse(tokeniser) ||\n      Namespace.parse(tokeniser);\n  }\n\n  function definitions() {\n    if (!source.length) return [];\n    const defs = [];\n    while (true) {\n      const ea = ExtendedAttributes.parse(tokeniser);\n      const def = definition();\n      if (!def) {\n        if (ea.length) error(\"Stray extended attributes\");\n        break;\n      }\n      def.extAttrs = ea;\n      defs.push(def);\n    }\n    const eof = consume(\"eof\");\n    if (options.concrete) {\n      defs.push(eof);\n    }\n    return defs;\n  }\n  const res = definitions();\n  if (tokeniser.position < source.length) error(\"Unrecognised tokens\");\n  return res;\n}\n\nexport function parse(str, options = {}) {\n  const tokeniser = new Tokeniser(str);\n  return parseByTokens(tokeniser, options);\n}\n","\"use strict\";\r\n\r\nfunction noop(arg) {\r\n  return arg;\r\n}\r\n\r\nconst templates = {\r\n  wrap: items => items.join(\"\"),\r\n  trivia: noop,\r\n  name: noop,\r\n  reference: noop,\r\n  type: noop,\r\n  generic: noop,\r\n  inheritance: noop,\r\n  definition: noop,\r\n  extendedAttribute: noop,\r\n  extendedAttributeReference: noop\r\n};\r\n\r\nexport function write(ast, { templates: ts = templates } = {}) {\r\n  ts = Object.assign({}, templates, ts);\r\n\r\n  function reference(raw, { unescaped, context }) {\r\n    if (!unescaped) {\r\n      unescaped = raw.startsWith(\"_\") ? raw.slice(1) : raw;\r\n    }\r\n    return ts.reference(raw, unescaped, context);\r\n  }\r\n\r\n  function token(t, wrapper = noop, ...args) {\r\n    if (!t) {\r\n      return \"\";\r\n    }\r\n    const value = wrapper(t.value, ...args);\r\n    return ts.wrap([ts.trivia(t.trivia), value]);\r\n  }\r\n\r\n  function reference_token(t, context) {\r\n    return token(t, reference, { context });\r\n  }\r\n\r\n  function name_token(t, arg) {\r\n    return token(t, ts.name, arg);\r\n  }\r\n\r\n  function type_body(it) {\r\n    if (it.union || it.generic) {\r\n      return ts.wrap([\r\n        token(it.tokens.base, ts.generic),\r\n        token(it.tokens.open),\r\n        ...it.subtype.map(type),\r\n        token(it.tokens.close)\r\n      ]);\r\n    }\r\n    const firstToken = it.tokens.prefix || it.tokens.base;\r\n    const prefix = it.tokens.prefix ? [\r\n      it.tokens.prefix.value,\r\n      ts.trivia(it.tokens.base.trivia)\r\n    ] : [];\r\n    const ref = reference(ts.wrap([\r\n      ...prefix,\r\n      it.tokens.base.value,\r\n      token(it.tokens.postfix)\r\n    ]), { unescaped: it.idlType, context: it });\r\n    return ts.wrap([ts.trivia(firstToken.trivia), ref]);\r\n  }\r\n  function type(it) {\r\n    return ts.wrap([\r\n      extended_attributes(it.extAttrs),\r\n      type_body(it),\r\n      token(it.tokens.nullable),\r\n      token(it.tokens.separator)\r\n    ]);\r\n  }\r\n  function default_(def) {\r\n    if (!def) {\r\n      return \"\";\r\n    }\r\n    return ts.wrap([\r\n      token(def.tokens.assign),\r\n      ...def.expression.map(t => token(t))\r\n    ]);\r\n  }\r\n  function argument(arg) {\r\n    return ts.wrap([\r\n      extended_attributes(arg.extAttrs),\r\n      token(arg.tokens.optional),\r\n      ts.type(type(arg.idlType)),\r\n      token(arg.tokens.variadic),\r\n      name_token(arg.tokens.name, { data: arg }),\r\n      default_(arg.default),\r\n      token(arg.tokens.separator)\r\n    ]);\r\n  }\r\n  function identifier(id, context) {\r\n    return ts.wrap([\r\n      reference_token(id.tokens.value, context),\r\n      token(id.tokens.separator)\r\n    ]);\r\n  }\r\n  function make_ext_at(it) {\r\n    const { rhsType } = it.params;\r\n    return ts.wrap([\r\n      ts.trivia(it.tokens.name.trivia),\r\n      ts.extendedAttribute(ts.wrap([\r\n        ts.extendedAttributeReference(it.name),\r\n        token(it.params.tokens.assign),\r\n        reference_token(it.params.tokens.secondaryName, it),\r\n        token(it.params.tokens.open),\r\n        ...!it.params.list ? [] :\r\n          it.params.list.map(\r\n            rhsType === \"identifier-list\" ? id => identifier(id, it) : argument\r\n          ),\r\n        token(it.params.tokens.close)\r\n      ])),\r\n      token(it.tokens.separator)\r\n    ]);\r\n  }\r\n  function extended_attributes(eats) {\r\n    if (!eats.length) return \"\";\r\n    return ts.wrap([\r\n      token(eats.tokens.open),\r\n      ...eats.map(make_ext_at),\r\n      token(eats.tokens.close)\r\n    ]);\r\n  }\r\n\r\n  function operation(it, parent) {\r\n    const body = it.idlType ? [\r\n      ts.type(type(it.idlType)),\r\n      name_token(it.tokens.name, { data: it, parent }),\r\n      token(it.tokens.open),\r\n      ts.wrap(it.arguments.map(argument)),\r\n      token(it.tokens.close),\r\n    ] : [];\r\n    return ts.definition(ts.wrap([\r\n      extended_attributes(it.extAttrs),\r\n      token(it.tokens.special),\r\n      ...body,\r\n      token(it.tokens.termination)\r\n    ]), { data: it, parent });\r\n  }\r\n\r\n  function attribute(it, parent) {\r\n    return ts.definition(ts.wrap([\r\n      extended_attributes(it.extAttrs),\r\n      token(it.tokens.special),\r\n      token(it.tokens.readonly),\r\n      token(it.tokens.base),\r\n      ts.type(type(it.idlType)),\r\n      name_token(it.tokens.name, { data: it, parent }),\r\n      token(it.tokens.termination)\r\n    ]), { data: it, parent });\r\n  }\r\n\r\n  function inheritance(inh) {\r\n    if (!inh.tokens.inheritance) {\r\n      return \"\";\r\n    }\r\n    return ts.wrap([\r\n      token(inh.tokens.colon),\r\n      ts.trivia(inh.tokens.inheritance.trivia),\r\n      ts.inheritance(reference(inh.tokens.inheritance.value, { context: inh }))\r\n    ]);\r\n  }\r\n\r\n  function container(it) {\r\n    return ts.definition(ts.wrap([\r\n      extended_attributes(it.extAttrs),\r\n      token(it.tokens.callback),\r\n      token(it.tokens.partial),\r\n      token(it.tokens.base),\r\n      token(it.tokens.mixin),\r\n      name_token(it.tokens.name, { data: it }),\r\n      inheritance(it),\r\n      token(it.tokens.open),\r\n      iterate(it.members, it),\r\n      token(it.tokens.close),\r\n      token(it.tokens.termination)\r\n    ]), { data: it });\r\n  }\r\n\r\n  function field(it, parent) {\r\n    return ts.definition(ts.wrap([\r\n      extended_attributes(it.extAttrs),\r\n      token(it.tokens.required),\r\n      ts.type(type(it.idlType)),\r\n      name_token(it.tokens.name, { data: it, parent }),\r\n      default_(it.default),\r\n      token(it.tokens.termination)\r\n    ]), { data: it, parent });\r\n  }\r\n  function const_(it, parent) {\r\n    return ts.definition(ts.wrap([\r\n      extended_attributes(it.extAttrs),\r\n      token(it.tokens.base),\r\n      ts.type(type(it.idlType)),\r\n      name_token(it.tokens.name, { data: it, parent }),\r\n      token(it.tokens.assign),\r\n      token(it.tokens.value),\r\n      token(it.tokens.termination)\r\n    ]), { data: it, parent });\r\n  }\r\n  function typedef(it) {\r\n    return ts.definition(ts.wrap([\r\n      extended_attributes(it.extAttrs),\r\n      token(it.tokens.base),\r\n      ts.type(type(it.idlType)),\r\n      name_token(it.tokens.name, { data: it }),\r\n      token(it.tokens.termination)\r\n    ]), { data: it });\r\n  }\r\n  function includes(it) {\r\n    return ts.definition(ts.wrap([\r\n      extended_attributes(it.extAttrs),\r\n      reference_token(it.tokens.target, it),\r\n      token(it.tokens.includes),\r\n      reference_token(it.tokens.mixin, it),\r\n      token(it.tokens.termination)\r\n    ]), { data: it });\r\n  }\r\n  function callback(it) {\r\n    return ts.definition(ts.wrap([\r\n      extended_attributes(it.extAttrs),\r\n      token(it.tokens.base),\r\n      name_token(it.tokens.name, { data: it }),\r\n      token(it.tokens.assign),\r\n      ts.type(type(it.idlType)),\r\n      token(it.tokens.open),\r\n      ...it.arguments.map(argument),\r\n      token(it.tokens.close),\r\n      token(it.tokens.termination),\r\n    ]), { data: it });\r\n  }\r\n  function enum_(it) {\r\n    return ts.definition(ts.wrap([\r\n      extended_attributes(it.extAttrs),\r\n      token(it.tokens.base),\r\n      name_token(it.tokens.name, { data: it }),\r\n      token(it.tokens.open),\r\n      iterate(it.values, it),\r\n      token(it.tokens.close),\r\n      token(it.tokens.termination)\r\n    ]), { data: it });\r\n  }\r\n  function enum_value(v, parent) {\r\n    return ts.wrap([\r\n      ts.trivia(v.tokens.value.trivia),\r\n      ts.definition(\r\n        ts.wrap(['\"', ts.name(v.value, { data: v, parent }), '\"']),\r\n        { data: v, parent }\r\n      ),\r\n      token(v.tokens.separator)\r\n    ]);\r\n  }\r\n  function iterable_like(it, parent) {\r\n    return ts.definition(ts.wrap([\r\n      extended_attributes(it.extAttrs),\r\n      token(it.tokens.readonly),\r\n      token(it.tokens.base, ts.generic),\r\n      token(it.tokens.open),\r\n      ts.wrap(it.idlType.map(type)),\r\n      token(it.tokens.close),\r\n      token(it.tokens.termination)\r\n    ]), { data: it, parent });\r\n  }\r\n  function eof(it) {\r\n    return ts.trivia(it.trivia);\r\n  }\r\n\r\n  const table = {\r\n    interface: container,\r\n    \"interface mixin\": container,\r\n    namespace: container,\r\n    operation,\r\n    attribute,\r\n    dictionary: container,\r\n    field,\r\n    const: const_,\r\n    typedef,\r\n    includes,\r\n    callback,\r\n    enum: enum_,\r\n    \"enum-value\": enum_value,\r\n    iterable: iterable_like,\r\n    legacyiterable: iterable_like,\r\n    maplike: iterable_like,\r\n    setlike: iterable_like,\r\n    \"callback interface\": container,\r\n    eof\r\n  };\r\n  function dispatch(it, parent) {\r\n    const dispatcher = table[it.type];\r\n    if (!dispatcher) {\r\n      throw new Error(`Type \"${it.type}\" is unsupported`);\r\n    }\r\n    return table[it.type](it, parent);\r\n  }\r\n  function iterate(things, parent) {\r\n    if (!things) return;\r\n    const results = things.map(thing => dispatch(thing, parent));\r\n    return ts.wrap(results);\r\n  }\r\n  return iterate(ast);\r\n}\r\n","\"use strict\";\n\nimport { validationError as error } from \"./error.js\";\n\nfunction getMixinMap(all, unique) {\n  const map = new Map();\n  const includes = all.filter(def => def.type === \"includes\");\n  for (const include of includes) {\n    const mixin = unique.get(include.includes);\n    if (!mixin) {\n      continue;\n    }\n    const array = map.get(include.target);\n    if (array) {\n      array.push(mixin);\n    } else {\n      map.set(include.target, [mixin]);\n    }\n  }\n  return map;\n}\n\nfunction groupDefinitions(all) {\n  const unique = new Map();\n  const duplicates = new Set();\n  const partials = new Map();\n  for (const def of all) {\n    if (def.partial) {\n      const array = partials.get(def.name);\n      if (array) {\n        array.push(def);\n      } else {\n        partials.set(def.name, [def]);\n      }\n      continue;\n    }\n    if (!def.name) {\n      continue;\n    }\n    if (!unique.has(def.name)) {\n      unique.set(def.name, def);\n    } else {\n      duplicates.add(def);\n    }\n  }\n  return {\n    all,\n    unique,\n    partials,\n    duplicates,\n    mixinMap: getMixinMap(all, unique)\n  };\n}\n\nfunction* checkDuplicatedNames({ unique, duplicates }) {\n  for (const dup of duplicates) {\n    const { name } = dup;\n    const message = `The name \"${name}\" of type \"${unique.get(name).type}\" was already seen`;\n    yield error(dup.source, dup.tokens.name, dup, message);\n  }\n}\n\nfunction* validateIterable(ast) {\n  const defs = groupDefinitions(ast);\n  for (const def of defs.all) {\n    if (def.validate) {\n      yield* def.validate(defs);\n    }\n  }\n  yield* checkDuplicatedNames(defs);\n}\n\n// Remove this once all of our support targets expose `.flat()` by default\nfunction flatten(array) {\n  if (array.flat) {\n    return array.flat();\n  }\n  return [].concat(...array);\n}\n\n/**\n * @param {*} ast AST or array of ASTs\n */\nexport function validate(ast) {\n  return [...validateIterable(flatten(ast))];\n}\n","export { parse } from \"./lib/webidl2.js\";\r\nexport { write } from \"./lib/writer.js\";\r\nexport { validate } from \"./lib/validator.js\";\r\n"],"sourceRoot":""}